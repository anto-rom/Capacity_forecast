{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4d849af5",
      "metadata": {},
      "source": [
        "# Planet — Corporate Hybrid Capacity Forecast **v11**\n",
        "## Hierarchical Forecasting (Vertical → Department) + Dynamic Level Adjustment + Risk-based Staffing + DOW Daily Plan + Real Productivity FTE\n",
        "\n",
        "**Generated:** 2026-02-14 (Europe/Madrid)\n",
        "\n",
        "### Executive intent\n",
        "Increase planning accuracy (especially **Payments** and **Hospitality**) by shifting the signal to a more stable layer:\n",
        "1) Forecast **monthly volume at Vertical** level  \n",
        "2) Allocate down to **Department** using **rolling 3-month shares** within each vertical  \n",
        "3) Apply **dynamic level adjustment** to reduce systematic bias\n",
        "\n",
        "### What’s new vs v10\n",
        "- **Hierarchical monthly forecast** (Vertical → Dept)\n",
        "- **Dynamic level adjustment** (per dept forecast)\n",
        "- **ARIMA governance** (rejects non-converged fits) + **ARIMA disabled for critical verticals**\n",
        "- Daily plan remains **DOW-profile based** without changing monthly totals\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e76fb19",
      "metadata": {},
      "source": [
        "## 1) Setup (interactive BASE_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f5f7d28b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Setup OK\n",
            "BASE_DIR: C:\\Projects\\Capacity_forecast_2026\n",
            "OUTPUT_XLSX: C:\\Projects\\Capacity_forecast_2026\\outputs\\capacity_forecast_hybrid_v11.xlsx\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import tkinter as tk\n",
        "from tkinter import filedialog\n",
        "\n",
        "root = tk.Tk()\n",
        "root.withdraw()\n",
        "root.attributes(\"-topmost\", True)\n",
        "\n",
        "BASE_DIR = filedialog.askdirectory(title=\"Select BASE_DIR (CAPACITY folder)\")\n",
        "if not BASE_DIR:\n",
        "    raise SystemExit(\"No folder selected. Execution stopped.\")\n",
        "\n",
        "BASE_DIR = str(Path(BASE_DIR).expanduser().resolve())\n",
        "INPUT_DIR  = str(Path(BASE_DIR) / \"input_model\")\n",
        "OUTPUT_DIR = str(Path(BASE_DIR) / \"outputs\")\n",
        "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "INCOMING_SOURCE_PATH = str(Path(INPUT_DIR) / \"Incoming_new.xlsx\")\n",
        "INCOMING_SHEET = \"Main\"\n",
        "\n",
        "DEPT_MAP_PATH = str(Path(INPUT_DIR) / \"department.xlsx\")\n",
        "DEPT_MAP_SHEET = \"map\"\n",
        "\n",
        "PRODUCTIVITY_PATH = str(Path(INPUT_DIR) / \"productivity_agents.xlsx\")\n",
        "CASE_REASON_PATH = str(Path(INPUT_DIR) / \"case_reason.xlsx\")\n",
        "\n",
        "OUTPUT_XLSX = str(Path(OUTPUT_DIR) / \"capacity_forecast_hybrid_v11.xlsx\")\n",
        "\n",
        "# Horizons\n",
        "H_MONTHS = 12\n",
        "DAILY_HORIZON_DAYS = 90\n",
        "\n",
        "SUPPORTED_LANGS = [\"Spanish\",\"English\",\"Portuguese\",\"French\",\"German\",\"Italian\"]\n",
        "\n",
        "# Prediction intervals\n",
        "ENABLE_MONTHLY_PI = True\n",
        "PI_ALPHA = 0.05  # 95% PI\n",
        "\n",
        "# Backtesting config (monthly)\n",
        "ENABLE_ACCURACY_TABLE = True\n",
        "ACCURACY_BACKTEST_MONTHS = 9\n",
        "ACCURACY_MIN_TRAIN_MONTHS = 12\n",
        "ACCURACY_HORIZON_MONTHS = 1\n",
        "ACCURACY_MAX_SPLITS = 9\n",
        "\n",
        "# Stability thresholds\n",
        "STAB = {\n",
        "    \"low_data_min_months\": 18,\n",
        "    \"broken_z_thresh\": 2.5,\n",
        "    \"volatility_cv_thresh\": 0.65,\n",
        "    \"zero_share_thresh\": 0.25,\n",
        "}\n",
        "\n",
        "# Critical verticals\n",
        "CRITICAL_VERTICALS = {\"Payments\",\"Hospitality\"}\n",
        "\n",
        "# Staffing risk policy\n",
        "RISK_POLICY = {\n",
        "    \"critical_low_acc_use_p95\": 75.0,\n",
        "    \"critical_mid_acc_blend\": 88.0,\n",
        "    \"volatility_uplift_pct\": 10.0,\n",
        "    \"lowdata_uplift_pct\": 15.0,\n",
        "    \"broken_uplift_pct\": 20.0,\n",
        "}\n",
        "\n",
        "# Level adjustment (bias reduction)\n",
        "LEVEL_ADJ = {\n",
        "    \"enabled\": True,\n",
        "    \"lookback_months\": 3,\n",
        "    \"clip_min\": 0.70,\n",
        "    \"clip_max\": 1.30,\n",
        "}\n",
        "\n",
        "# DOW profile\n",
        "DOW_LOOKBACK_DAYS = 180\n",
        "DOW_MIN_OBS = 30\n",
        "\n",
        "required_files = [INCOMING_SOURCE_PATH, DEPT_MAP_PATH, PRODUCTIVITY_PATH, CASE_REASON_PATH]\n",
        "missing = [p for p in required_files if not Path(p).exists()]\n",
        "if missing:\n",
        "    raise FileNotFoundError(\"Missing required input files:\\n- \" + \"\\n- \".join(missing) + f\"\\n\\nSelected BASE_DIR:\\n{BASE_DIR}\")\n",
        "\n",
        "print(\"✅ Setup OK\")\n",
        "print(\"BASE_DIR:\", BASE_DIR)\n",
        "print(\"OUTPUT_XLSX:\", OUTPUT_XLSX)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5cb0ee8",
      "metadata": {},
      "source": [
        "## 2) Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "56068edc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Imports loaded. Prophet: True\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from typing import Optional, Dict, Tuple\n",
        "from datetime import date, timedelta\n",
        "\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
        "\n",
        "try:\n",
        "    from prophet import Prophet\n",
        "except Exception:\n",
        "    try:\n",
        "        from fbprophet import Prophet\n",
        "    except Exception:\n",
        "        Prophet = None\n",
        "\n",
        "print(\"Imports loaded. Prophet:\", bool(Prophet))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03cf3465",
      "metadata": {},
      "source": [
        "## 3) Core functions (v11)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9322bd5f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CORE FUNCTIONS — v11\n",
        "# ============================================================\n",
        "\n",
        "def smape(y_true, y_pred) -> float:\n",
        "    y_true = np.array(y_true, dtype=float)\n",
        "    y_pred = np.array(y_pred, dtype=float)\n",
        "    denom = (np.abs(y_true) + np.abs(y_pred))\n",
        "    denom[denom == 0] = 1.0\n",
        "    return float(np.mean(2.0 * np.abs(y_pred - y_true) / denom) * 100.0)\n",
        "\n",
        "def accuracy_pct(y_true, y_pred) -> float:\n",
        "    y_true = np.array(y_true, dtype=float)\n",
        "    y_pred = np.array(y_pred, dtype=float)\n",
        "    denom = np.where(y_true > 0, y_true, np.nan)\n",
        "    return float(np.nanmean((1.0 - (np.abs(y_pred - y_true) / denom)) * 100.0))\n",
        "\n",
        "def bias_pct(y_true, y_pred) -> float:\n",
        "    y_true = np.array(y_true, dtype=float)\n",
        "    y_pred = np.array(y_pred, dtype=float)\n",
        "    denom = np.where(y_true > 0, y_true, np.nan)\n",
        "    return float(np.nanmean(((y_pred - y_true) / denom) * 100.0))\n",
        "\n",
        "def clamp_nonneg(a):\n",
        "    a = np.array(a, dtype=float)\n",
        "    a[~np.isfinite(a)] = 0.0\n",
        "    return np.clip(a, 0.0, None)\n",
        "\n",
        "def _z_from_alpha(alpha: float) -> float:\n",
        "    if abs(alpha - 0.10) < 1e-9: return 1.6448536269514722\n",
        "    if abs(alpha - 0.05) < 1e-9: return 1.959963984540054\n",
        "    if abs(alpha - 0.01) < 1e-9: return 2.5758293035489004\n",
        "    return 1.959963984540054\n",
        "\n",
        "def expm1_safe(x, cap_original: Optional[float] = None):\n",
        "    a = np.array(x, dtype=float)\n",
        "    a[~np.isfinite(a)] = -50.0\n",
        "    a = np.maximum(a, -50.0)\n",
        "    if cap_original and np.isfinite(cap_original) and cap_original > 0:\n",
        "        log_cap = np.log1p(cap_original)\n",
        "        a = np.minimum(a, log_cap)\n",
        "    y = np.expm1(a)\n",
        "    if cap_original and np.isfinite(cap_original) and cap_original > 0:\n",
        "        y = np.minimum(y, cap_original)\n",
        "    return np.clip(y, 0, None)\n",
        "\n",
        "def compute_dynamic_cap(ts_m: pd.Series) -> float:\n",
        "    if ts_m.empty or (ts_m.max() <= 0):\n",
        "        return np.inf\n",
        "    m12 = float(ts_m.tail(12).mean()) if len(ts_m) >= 3 else float(ts_m.mean())\n",
        "    med, mx = float(ts_m.median()), float(ts_m.max())\n",
        "    base = max(1.0, m12, med, 1.1 * mx)\n",
        "    return base * 6.0\n",
        "\n",
        "def compute_pi_from_log_mean_std(mean_log: np.ndarray, std_log: np.ndarray, cap: float, alpha: float = 0.05):\n",
        "    z = _z_from_alpha(alpha)\n",
        "    std_log = np.clip(np.array(std_log, dtype=float), 1e-8, None)\n",
        "    lo_log = mean_log - z * std_log\n",
        "    hi_log = mean_log + z * std_log\n",
        "    mean = expm1_safe(mean_log, cap_original=cap)\n",
        "    lo   = expm1_safe(lo_log,   cap_original=cap)\n",
        "    hi   = expm1_safe(hi_log,   cap_original=cap)\n",
        "    mean = clamp_nonneg(mean); lo = clamp_nonneg(lo); hi = clamp_nonneg(hi)\n",
        "    return mean, np.minimum(lo, hi), np.maximum(lo, hi)\n",
        "\n",
        "def coverage_95(actual: np.ndarray, lo: np.ndarray, hi: np.ndarray) -> float:\n",
        "    a = np.array(actual, dtype=float)\n",
        "    l = np.array(lo, dtype=float)\n",
        "    h = np.array(hi, dtype=float)\n",
        "    m = np.isfinite(a) & np.isfinite(l) & np.isfinite(h)\n",
        "    if m.sum() == 0:\n",
        "        return np.nan\n",
        "    ok = (a[m] >= l[m]) & (a[m] <= h[m])\n",
        "    return float(ok.mean() * 100.0)\n",
        "\n",
        "def clean_outliers_daily(g: pd.DataFrame) -> pd.DataFrame:\n",
        "    g = g.copy()\n",
        "    s = g.sort_values(\"Date\")[\"ticket_total\"].astype(float)\n",
        "    ql = s.quantile(0.01)\n",
        "    qh = s.quantile(0.99)\n",
        "    g[\"ticket_total\"] = s.clip(ql, qh).values\n",
        "    return g\n",
        "\n",
        "def winsorize_monthly(ts_m: pd.Series, lower_q=0.01, upper_q=0.99) -> pd.Series:\n",
        "    if ts_m.empty:\n",
        "        return ts_m\n",
        "    return ts_m.clip(lower=ts_m.quantile(lower_q), upper=ts_m.quantile(upper_q))\n",
        "\n",
        "# ---------- Loaders ----------\n",
        "def load_incoming(path: str, sheet_name: str) -> pd.DataFrame:\n",
        "    df = pd.read_excel(path, sheet_name=sheet_name, engine=\"openpyxl\")\n",
        "    required = {\"Date\", \"department_id\"}\n",
        "    missing = required - set(df.columns)\n",
        "    if missing:\n",
        "        raise ValueError(f\"Incoming missing columns: {sorted(list(missing))}. Found: {list(df.columns)}\")\n",
        "    if \"ticket_total\" not in df.columns:\n",
        "        candidates = [c for c in [\"total_incoming\",\"incoming_total\",\"Incoming\"] if c in df.columns]\n",
        "        if candidates:\n",
        "            df[\"ticket_total\"] = pd.to_numeric(df[candidates[0]], errors=\"coerce\").fillna(0.0)\n",
        "        elif {\"incoming_from_customers\",\"incoming_from_transfers\"}.issubset(df.columns):\n",
        "            df[\"ticket_total\"] = (\n",
        "                pd.to_numeric(df[\"incoming_from_customers\"], errors=\"coerce\").fillna(0.0)\n",
        "                + pd.to_numeric(df[\"incoming_from_transfers\"], errors=\"coerce\").fillna(0.0)\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\"Incoming must contain ticket_total or known alternative columns.\")\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[\"Date\"])\n",
        "    df[\"department_id\"] = df[\"department_id\"].astype(str).str.strip()\n",
        "    df[\"ticket_total\"] = pd.to_numeric(df[\"ticket_total\"], errors=\"coerce\").fillna(0.0).astype(float)\n",
        "    if \"language\" not in df.columns:\n",
        "        df[\"language\"] = \"English\"\n",
        "    df[\"language\"] = df[\"language\"].astype(str).str.strip()\n",
        "    df[\"language\"] = np.where(df[\"language\"].isin(SUPPORTED_LANGS), df[\"language\"], \"English\")\n",
        "    if \"department_name\" not in df.columns:\n",
        "        df[\"department_name\"] = None\n",
        "    if \"vertical\" not in df.columns:\n",
        "        df[\"vertical\"] = None\n",
        "    return df\n",
        "\n",
        "def load_dept_map(path: str, sheet: str) -> pd.DataFrame:\n",
        "    mp = pd.read_excel(path, sheet_name=sheet, engine=\"openpyxl\")\n",
        "    rename_map = {\"dept_id\":\"department_id\",\"dept_name\":\"department_name\",\"name\":\"department_name\",\n",
        "                  \"segment\":\"vertical\",\"vertical_name\":\"vertical\"}\n",
        "    mp = mp.rename(columns={k:v for k,v in rename_map.items() if k in mp.columns})\n",
        "    if \"department_id\" not in mp.columns:\n",
        "        raise ValueError(f\"Department map must contain department_id. Found: {list(mp.columns)}\")\n",
        "    mp[\"department_id\"] = mp[\"department_id\"].astype(str).str.strip()\n",
        "    if \"department_name\" in mp.columns:\n",
        "        mp[\"department_name\"] = mp[\"department_name\"].astype(str).str.strip()\n",
        "    if \"vertical\" in mp.columns:\n",
        "        mp[\"vertical\"] = mp[\"vertical\"].astype(str).str.strip()\n",
        "    cols = [c for c in [\"department_id\",\"department_name\",\"vertical\"] if c in mp.columns]\n",
        "    return mp[cols].drop_duplicates(\"department_id\")\n",
        "\n",
        "def apply_mapping(incoming: pd.DataFrame, mapping: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = incoming.merge(mapping, on=\"department_id\", how=\"left\", suffixes=(\"\", \"_map\"))\n",
        "    if \"department_name_map\" in df.columns:\n",
        "        df[\"department_name\"] = df[\"department_name\"].fillna(df[\"department_name_map\"])\n",
        "    if \"vertical_map\" in df.columns:\n",
        "        df[\"vertical\"] = df[\"vertical\"].fillna(df[\"vertical_map\"])\n",
        "    df[\"department_name\"] = df[\"department_name\"].fillna(\"Unknown\")\n",
        "    df[\"vertical\"] = df[\"vertical\"].fillna(\"Unmapped\")\n",
        "    df.drop(columns=[c for c in df.columns if c.endswith(\"_map\")], inplace=True, errors=\"ignore\")\n",
        "    return df\n",
        "\n",
        "def load_productivity(path: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    df = pd.read_excel(path, engine=\"openpyxl\")\n",
        "    req = {\"Date\",\"agent_id\",\"department_id\",\"prod_total_model\"}\n",
        "    missing = req - set(df.columns)\n",
        "    if missing:\n",
        "        raise ValueError(f\"productivity missing columns: {sorted(list(missing))}. Found: {list(df.columns)}\")\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[\"Date\"])\n",
        "    df[\"department_id\"] = df[\"department_id\"].astype(str).str.strip()\n",
        "    df[\"prod_total_model\"] = pd.to_numeric(df[\"prod_total_model\"], errors=\"coerce\")\n",
        "    df = df[np.isfinite(df[\"prod_total_model\"]) & (df[\"prod_total_model\"] >= 0)].copy()\n",
        "    df[\"dow\"] = df[\"Date\"].dt.weekday.astype(int)\n",
        "    dept_prod = (df.groupby(\"department_id\", as_index=False)[\"prod_total_model\"]\n",
        "                   .mean()\n",
        "                   .rename(columns={\"prod_total_model\":\"avg_tickets_per_agent_day\"}))\n",
        "    dept_dow_prod = (df.groupby([\"department_id\",\"dow\"], as_index=False)[\"prod_total_model\"]\n",
        "                       .mean()\n",
        "                       .rename(columns={\"prod_total_model\":\"avg_tickets_per_agent_day_dow\"}))\n",
        "    return dept_prod, dept_dow_prod\n",
        "\n",
        "# ---------- Exogenous calendar ----------\n",
        "def _easter_sunday(year: int) -> date:\n",
        "    a = year % 19\n",
        "    b = year // 100\n",
        "    c = year % 100\n",
        "    d = b // 4\n",
        "    e = b % 4\n",
        "    f = (b + 8) // 25\n",
        "    g = (b - f + 1) // 3\n",
        "    h = (19*a + b - d - g + 15) % 30\n",
        "    i = c // 4\n",
        "    k = c % 4\n",
        "    L = (32 + 2*e + 2*i - h - k) % 7\n",
        "    m = (a + 11*h + 22*L) // 451\n",
        "    month = (h + L - 7*m + 114) // 31\n",
        "    day = ((h + L - 7*m + 114) % 31) + 1\n",
        "    return date(year, month, day)\n",
        "\n",
        "def _last_friday_of_november(year: int) -> date:\n",
        "    d = date(year, 11, 30)\n",
        "    while d.weekday() != 4:\n",
        "        d = d.replace(day=d.day - 1)\n",
        "    return d\n",
        "\n",
        "def build_eu_core_holidays(start_date: pd.Timestamp, end_date: pd.Timestamp) -> pd.DataFrame:\n",
        "    years = list(range(start_date.year, end_date.year + 1))\n",
        "    rows = []\n",
        "    for y in years:\n",
        "        rows += [\n",
        "            {\"ds\": date(y,1,1), \"type\":\"holiday\", \"weight\":1.0},\n",
        "            {\"ds\": date(y,5,1), \"type\":\"holiday\", \"weight\":1.0},\n",
        "            {\"ds\": date(y,12,25), \"type\":\"holiday\", \"weight\":1.0},\n",
        "            {\"ds\": date(y,12,26), \"type\":\"holiday\", \"weight\":1.0},\n",
        "        ]\n",
        "        easter = _easter_sunday(y)\n",
        "        rows += [\n",
        "            {\"ds\": easter - timedelta(days=2), \"type\":\"holiday\", \"weight\":1.0},\n",
        "            {\"ds\": easter + timedelta(days=1), \"type\":\"holiday\", \"weight\":1.0},\n",
        "        ]\n",
        "        bf = _last_friday_of_november(y)\n",
        "        rows += [\n",
        "            {\"ds\": bf, \"type\":\"event\", \"weight\":0.6},\n",
        "            {\"ds\": bf + timedelta(days=4), \"type\":\"event\", \"weight\":0.6},\n",
        "        ]\n",
        "    exo = pd.DataFrame(rows)\n",
        "    exo[\"ds\"] = pd.to_datetime(exo[\"ds\"])\n",
        "    return exo[(exo[\"ds\"] >= start_date) & (exo[\"ds\"] <= end_date)].reset_index(drop=True)\n",
        "\n",
        "def build_exogenous_calendar(incoming: pd.DataFrame, horizon_days: int):\n",
        "    start = incoming[\"Date\"].min() - pd.Timedelta(days=365)\n",
        "    end = incoming[\"Date\"].max() + pd.Timedelta(days=horizon_days)\n",
        "    exo = build_eu_core_holidays(start, end)\n",
        "    cal = pd.DataFrame({\"ds\": pd.date_range(start, end, freq=\"D\")})\n",
        "    exo_daily = cal.merge(exo, on=\"ds\", how=\"left\")\n",
        "    exo_daily[\"is_holiday\"] = (exo_daily[\"type\"] == \"holiday\").astype(int)\n",
        "    exo_daily[\"is_event\"] = (exo_daily[\"type\"] == \"event\").astype(int)\n",
        "    exo_daily[\"weight_h\"] = np.where(exo_daily[\"is_holiday\"] == 1, exo_daily[\"weight\"].fillna(1.0), 0.0)\n",
        "    exo_daily[\"weight_e\"] = np.where(exo_daily[\"is_event\"] == 1, exo_daily[\"weight\"].fillna(1.0), 0.0)\n",
        "    exo_daily = exo_daily[[\"ds\",\"is_holiday\",\"is_event\",\"weight_h\",\"weight_e\"]]\n",
        "    exo_daily[\"month\"] = exo_daily[\"ds\"].dt.to_period(\"M\")\n",
        "    exo_monthly = (exo_daily.groupby(\"month\", as_index=False)\n",
        "                   .agg(hol_count=(\"is_holiday\",\"sum\"),\n",
        "                        evt_count=(\"is_event\",\"sum\"),\n",
        "                        hol_weight_sum=(\"weight_h\",\"sum\"),\n",
        "                        evt_weight_sum=(\"weight_e\",\"sum\")))\n",
        "    exo_monthly[\"month\"] = pd.PeriodIndex(exo_monthly[\"month\"], freq=\"M\")\n",
        "    return exo_daily, exo_monthly\n",
        "\n",
        "def prepare_monthly_exog(exo_monthly: pd.DataFrame, ts_index: pd.PeriodIndex) -> pd.DataFrame:\n",
        "    ex = exo_monthly.copy()\n",
        "    ex[\"month\"] = pd.PeriodIndex(ex[\"month\"], freq=\"M\")\n",
        "    ex = ex.set_index(\"month\").reindex(ts_index).fillna(0.0)\n",
        "    return ex[[\"hol_count\",\"evt_count\",\"hol_weight_sum\",\"evt_weight_sum\"]]\n",
        "\n",
        "# ---------- Stability ----------\n",
        "def detect_structural_break(ts_m: pd.Series, window: int = 6, min_total_months: int = 18, z_thresh: float = 2.5) -> Optional[pd.Period]:\n",
        "    ts_m = ts_m.dropna()\n",
        "    if len(ts_m) < max(min_total_months, 2*window + 1):\n",
        "        return None\n",
        "    last = ts_m.iloc[-window:]\n",
        "    prev = ts_m.iloc[-2*window:-window]\n",
        "    mu1, mu0 = float(last.mean()), float(prev.mean())\n",
        "    s1, s0 = float(last.std(ddof=1)), float(prev.std(ddof=1))\n",
        "    pooled = math.sqrt(max(1e-9, (s1*s1 + s0*s0) / 2.0))\n",
        "    z = abs(mu1 - mu0) / pooled if pooled > 0 else 0.0\n",
        "    return ts_m.index[-window] if z >= z_thresh else None\n",
        "\n",
        "def apply_break_window(ts_m: pd.Series, break_start: Optional[pd.Period], min_train_months: int = 12) -> pd.Series:\n",
        "    if break_start is None:\n",
        "        return ts_m\n",
        "    ts2 = ts_m.loc[break_start:]\n",
        "    return ts2 if len(ts2) >= min_train_months else ts_m\n",
        "\n",
        "def classify_series(ts_m: pd.Series) -> Dict[str, object]:\n",
        "    ts_m = ts_m.dropna()\n",
        "    n = int(len(ts_m))\n",
        "    zeros_share = float((ts_m <= 0).mean()) if n > 0 else 1.0\n",
        "    mean = float(ts_m.mean()) if n > 0 else 0.0\n",
        "    std = float(ts_m.std(ddof=1)) if n > 1 else 0.0\n",
        "    cv = float(std / mean) if mean > 0 else np.inf\n",
        "    brk = detect_structural_break(ts_m, window=6, min_total_months=18, z_thresh=STAB[\"broken_z_thresh\"])\n",
        "    label = \"Stable\"\n",
        "    reasons=[]\n",
        "    if n < STAB[\"low_data_min_months\"]:\n",
        "        label=\"LowData\"; reasons.append(\"low_history\")\n",
        "    else:\n",
        "        if zeros_share >= STAB[\"zero_share_thresh\"]:\n",
        "            label=\"Volatile\"; reasons.append(\"many_zeros\")\n",
        "        if cv >= STAB[\"volatility_cv_thresh\"]:\n",
        "            label=\"Volatile\"; reasons.append(\"high_cv\")\n",
        "        if brk is not None:\n",
        "            label=\"Broken\"; reasons.append(f\"break@{brk}\")\n",
        "    return {\"label\": label, \"months\": n, \"zeros_share\": zeros_share, \"cv\": cv,\n",
        "            \"break_start\": str(brk) if brk is not None else None,\n",
        "            \"reasons\": \";\".join(reasons) if reasons else None}\n",
        "\n",
        "# ---------- Models ----------\n",
        "def fit_prophet_monthly_log_with_pi(ts_m: pd.Series, exo_m: Optional[pd.DataFrame] = None, changepoint_prior_scale: float = 0.25):\n",
        "    if Prophet is None or len(ts_m) < 6:\n",
        "        return None, None, None\n",
        "    cap = compute_dynamic_cap(ts_m)\n",
        "    y = np.log1p(ts_m.values)\n",
        "    dfp = pd.DataFrame({\"ds\": ts_m.index.to_timestamp(), \"y\": y})\n",
        "    if exo_m is not None:\n",
        "        dfp = dfp.join(exo_m, on=ts_m.index).reset_index(drop=True)\n",
        "    m = Prophet(weekly_seasonality=False, yearly_seasonality=True, daily_seasonality=False,\n",
        "                interval_width=(1.0 - PI_ALPHA), changepoint_prior_scale=changepoint_prior_scale)\n",
        "    if exo_m is not None:\n",
        "        for col in exo_m.columns:\n",
        "            m.add_regressor(col)\n",
        "    m.fit(dfp)\n",
        "\n",
        "    def _future(h_months: int):\n",
        "        future = m.make_future_dataframe(periods=h_months, freq=\"MS\")\n",
        "        if exo_m is not None:\n",
        "            idx_future = pd.period_range(ts_m.index[-1] + 1, periods=h_months, freq=\"M\")\n",
        "            pad = pd.concat([exo_m.iloc[[-1]]] * h_months, ignore_index=True)\n",
        "            pad.index = idx_future\n",
        "            ex_full = pd.concat([exo_m, pad], axis=0)\n",
        "            for col in exo_m.columns:\n",
        "                future[col] = ex_full[col].values\n",
        "        return future\n",
        "\n",
        "    def f_pi(h_months: int):\n",
        "        pred = m.predict(_future(h_months))\n",
        "        pred.index = pd.PeriodIndex(pred[\"ds\"], freq=\"M\")\n",
        "        idx = pred.index[-h_months:]\n",
        "        mu_log = pred[\"yhat\"].iloc[-h_months:].values\n",
        "        lo_log = pred[\"yhat_lower\"].iloc[-h_months:].values\n",
        "        hi_log = pred[\"yhat_upper\"].iloc[-h_months:].values\n",
        "        mean = clamp_nonneg(expm1_safe(mu_log, cap_original=cap))\n",
        "        lo = clamp_nonneg(expm1_safe(lo_log, cap_original=cap))\n",
        "        hi = clamp_nonneg(expm1_safe(hi_log, cap_original=cap))\n",
        "        return pd.Series(mean, index=idx), pd.Series(np.minimum(lo,hi), index=idx), pd.Series(np.maximum(lo,hi), index=idx)\n",
        "\n",
        "    def f_mean(h_months: int):\n",
        "        mean, _, _ = f_pi(h_months)\n",
        "        return mean\n",
        "\n",
        "    return m, f_mean, f_pi\n",
        "\n",
        "def fit_arima_monthly_log_with_pi(ts_m: pd.Series, exo_m: Optional[pd.DataFrame] = None):\n",
        "    cap = compute_dynamic_cap(ts_m)\n",
        "    y = np.log1p(ts_m)\n",
        "    best_aic, best_model, best_exog, best_order = np.inf, None, None, None\n",
        "    pqs=[0,1]\n",
        "    seasonal = len(ts_m) >= 12\n",
        "    PsQs=[0]\n",
        "    for p in pqs:\n",
        "        for d in ([1] if len(ts_m) < 24 else [0,1]):\n",
        "            for q in pqs:\n",
        "                for P in PsQs:\n",
        "                    for D in ([0,1] if seasonal else [0]):\n",
        "                        for Q in PsQs:\n",
        "                            try:\n",
        "                                model = SARIMAX(\n",
        "                                    y,\n",
        "                                    order=(p,d,q),\n",
        "                                    seasonal_order=(P,D,Q,12 if seasonal else 0),\n",
        "                                    exog=(exo_m.values if exo_m is not None else None),\n",
        "                                    enforce_stationarity=False,\n",
        "                                    enforce_invertibility=False\n",
        "                                ).fit(disp=False)\n",
        "                                if hasattr(model, \"mle_retvals\") and not bool(model.mle_retvals.get(\"converged\", True)):\n",
        "                                    continue\n",
        "                                if model.aic < best_aic:\n",
        "                                    best_aic, best_model, best_exog = model.aic, model, exo_m\n",
        "                                    best_order = ((p,d,q),(P,D,Q,12 if seasonal else 0))\n",
        "                            except Exception:\n",
        "                                continue\n",
        "\n",
        "    def _future_exog(h_months: int, idx: pd.PeriodIndex):\n",
        "        if best_exog is None: return None\n",
        "        pad = pd.concat([best_exog.iloc[[-1]]] * h_months, ignore_index=True)\n",
        "        pad.index = idx\n",
        "        return pad.values\n",
        "\n",
        "    def f_pi(h_months: int):\n",
        "        idx = pd.period_range(ts_m.index[-1] + 1, periods=h_months, freq=\"M\")\n",
        "        if best_model is None:\n",
        "            mu_log = np.full(h_months, float(np.nanmean(y.values)))\n",
        "            std_log = np.full(h_months, float(np.nanstd(y.values) if np.isfinite(np.nanstd(y.values)) else 0.5))\n",
        "            mean, lo, hi = compute_pi_from_log_mean_std(mu_log, std_log, cap=cap, alpha=PI_ALPHA)\n",
        "            return pd.Series(mean,index=idx), pd.Series(lo,index=idx), pd.Series(hi,index=idx)\n",
        "        fc = best_model.get_forecast(h_months, exog=_future_exog(h_months, idx))\n",
        "        mu_log = fc.predicted_mean.values\n",
        "        ci = fc.conf_int(alpha=PI_ALPHA)\n",
        "        lo_log = ci.iloc[:,0].values\n",
        "        hi_log = ci.iloc[:,1].values\n",
        "        mean = clamp_nonneg(expm1_safe(mu_log, cap_original=cap))\n",
        "        lo = clamp_nonneg(expm1_safe(lo_log, cap_original=cap))\n",
        "        hi = clamp_nonneg(expm1_safe(hi_log, cap_original=cap))\n",
        "        return pd.Series(mean,index=idx), pd.Series(np.minimum(lo,hi),index=idx), pd.Series(np.maximum(lo,hi),index=idx)\n",
        "\n",
        "    return best_model, f_pi, best_order\n",
        "\n",
        "def fit_ets_monthly_log_with_pi(ts_m: pd.Series):\n",
        "    cap = compute_dynamic_cap(ts_m)\n",
        "    y_log = np.log1p(ts_m)\n",
        "    seasonal = 12 if len(ts_m) >= 24 else None\n",
        "    model = ExponentialSmoothing(y_log, trend=\"add\", seasonal=(\"add\" if seasonal else None), seasonal_periods=seasonal).fit()\n",
        "    resid = np.array(y_log.values, dtype=float) - np.array(model.fittedvalues, dtype=float)\n",
        "    sigma = float(np.nanstd(resid)) if np.isfinite(np.nanstd(resid)) else 0.5\n",
        "\n",
        "    def f_pi(h_months: int):\n",
        "        vals_log = np.array(model.forecast(h_months), dtype=float)\n",
        "        idx = pd.period_range(ts_m.index[-1] + 1, periods=h_months, freq=\"M\")\n",
        "        mean, lo, hi = compute_pi_from_log_mean_std(vals_log, np.full(h_months, sigma), cap=cap, alpha=PI_ALPHA)\n",
        "        return pd.Series(mean,index=idx), pd.Series(lo,index=idx), pd.Series(hi,index=idx)\n",
        "\n",
        "    return model, f_pi\n",
        "\n",
        "def rolling_cv_monthly(ts_m: pd.Series, exo_m: Optional[pd.DataFrame]):\n",
        "    n = len(ts_m)\n",
        "    if n < 9: return None\n",
        "    h = 3 if n >= 15 else 1\n",
        "    min_train = max(12, n - (h + 2))\n",
        "    splits=[]\n",
        "    for start in range(min_train, n - h + 1):\n",
        "        train = ts_m.iloc[:start]\n",
        "        test  = ts_m.iloc[start:start+h]\n",
        "        ex_train = exo_m.iloc[:start] if exo_m is not None else None\n",
        "        m={}\n",
        "        mp, _, fp_pi = fit_prophet_monthly_log_with_pi(train, ex_train, changepoint_prior_scale=0.30)\n",
        "        if fp_pi is not None:\n",
        "            try:\n",
        "                yhat = fp_pi(h)[0].values[:h]\n",
        "                m[\"Prophet\"] = smape(test.values, yhat)\n",
        "            except Exception:\n",
        "                m[\"Prophet\"]=200.0\n",
        "        try:\n",
        "            _, fa_pi, _ = fit_arima_monthly_log_with_pi(train, ex_train)\n",
        "            yhat = fa_pi(h)[0].values[:h]\n",
        "            m[\"ARIMA\"] = smape(test.values, yhat)\n",
        "        except Exception:\n",
        "            m[\"ARIMA\"]=200.0\n",
        "        try:\n",
        "            _, fe_pi = fit_ets_monthly_log_with_pi(train)\n",
        "            yhat = fe_pi(h)[0].values[:h]\n",
        "            m[\"ETS\"] = smape(test.values, yhat)\n",
        "        except Exception:\n",
        "            m[\"ETS\"]=200.0\n",
        "        splits.append(m)\n",
        "    return pd.DataFrame(splits).mean().to_dict()\n",
        "\n",
        "def select_or_blend_with_pi(fc_pi: Dict[str, Tuple[pd.Series,pd.Series,pd.Series]], cv_scores: Dict[str,float], blend: bool):\n",
        "    keys = list(fc_pi.keys())\n",
        "    if not keys:\n",
        "        raise ValueError(\"No candidates.\")\n",
        "    scores = {k: float(v) for k,v in (cv_scores or {}).items() if k in keys and np.isfinite(v)}\n",
        "    if (not blend) or (len(scores)==0):\n",
        "        k = keys[0]\n",
        "        mean, lo, hi = fc_pi[k]\n",
        "        return mean, lo, hi, {\"winner\": k, \"weights\": {k:1.0}}\n",
        "\n",
        "    inv = {k: (1.0/v if v > 0 else 0.0) for k,v in scores.items()}\n",
        "    total = sum(inv.values())\n",
        "    if total <= 0:\n",
        "        k = min(scores, key=scores.get)\n",
        "        mean, lo, hi = fc_pi[k]\n",
        "        return mean, lo, hi, {\"winner\": k, \"weights\": {k:1.0}}\n",
        "    w = {k: inv[k]/total for k in inv}\n",
        "    idx = None\n",
        "    for k in keys:\n",
        "        idx = fc_pi[k][0].index if idx is None else idx.union(fc_pi[k][0].index)\n",
        "\n",
        "    mean_bl = sum(w.get(k,0.0) * fc_pi[k][0].reindex(idx).fillna(0.0) for k in keys)\n",
        "\n",
        "    z = _z_from_alpha(PI_ALPHA)\n",
        "    var_log = np.zeros(len(idx), dtype=float)\n",
        "    for k, wk in w.items():\n",
        "        mu, lo, hi = fc_pi[k]\n",
        "        lo_v = lo.reindex(idx).astype(float).values\n",
        "        hi_v = hi.reindex(idx).astype(float).values\n",
        "        lo_log = np.log1p(np.clip(lo_v,0,None))\n",
        "        hi_log = np.log1p(np.clip(hi_v,0,None))\n",
        "        sigma = (hi_log - lo_log) / (2.0*z)\n",
        "        sigma = np.clip(sigma, 1e-8, None)\n",
        "        var_log += (wk**2) * (sigma**2)\n",
        "    std_log = np.sqrt(np.clip(var_log, 1e-10, None))\n",
        "    mu_v = mean_bl.reindex(idx).astype(float).values\n",
        "    mu_log = np.log1p(np.clip(mu_v,0,None))\n",
        "    cap = float(np.nanmax(mu_v) * 6.0) if np.isfinite(np.nanmax(mu_v)) and np.nanmax(mu_v)>0 else np.inf\n",
        "    mean, lo, hi = compute_pi_from_log_mean_std(mu_log, std_log, cap=cap, alpha=PI_ALPHA)\n",
        "    return pd.Series(mean,index=idx), pd.Series(lo,index=idx), pd.Series(hi,index=idx), {\"winner\": min(scores, key=scores.get), \"weights\": w}\n",
        "\n",
        "# ---------- Hierarchical shares ----------\n",
        "def dept_share_roll3_within_vertical(incoming: pd.DataFrame) -> pd.DataFrame:\n",
        "    d = incoming.copy()\n",
        "    d[\"Date\"] = pd.to_datetime(d[\"Date\"])\n",
        "    d[\"month\"] = d[\"Date\"].dt.to_period(\"M\")\n",
        "    dept_m = d.groupby([\"vertical\",\"department_id\",\"month\"], as_index=False)[\"ticket_total\"].sum()\n",
        "    vert_m = d.groupby([\"vertical\",\"month\"], as_index=False)[\"ticket_total\"].sum().rename(columns={\"ticket_total\":\"vertical_total\"})\n",
        "    x = dept_m.merge(vert_m, on=[\"vertical\",\"month\"], how=\"left\")\n",
        "    x[\"share\"] = np.where(x[\"vertical_total\"]>0, x[\"ticket_total\"]/x[\"vertical_total\"], 0.0)\n",
        "    x = x.sort_values([\"vertical\",\"department_id\",\"month\"])\n",
        "    x[\"share_roll3\"] = (x.groupby([\"vertical\",\"department_id\"])[\"share\"]\n",
        "                          .rolling(3, min_periods=1).mean()\n",
        "                          .reset_index(level=[0,1], drop=True))\n",
        "    return x[[\"vertical\",\"department_id\",\"month\",\"share_roll3\"]]\n",
        "\n",
        "def normalize_shares_for_month(shares_vd: pd.DataFrame, vertical: str, month: pd.Period, dept_ids: np.ndarray) -> pd.DataFrame:\n",
        "    sub = shares_vd[(shares_vd[\"vertical\"]==vertical) & (shares_vd[\"month\"]==month)].copy()\n",
        "    grid = pd.DataFrame({\"department_id\": [str(x) for x in dept_ids]})\n",
        "    sub = grid.merge(sub[[\"department_id\",\"share_roll3\"]], on=\"department_id\", how=\"left\").fillna(0.0)\n",
        "    s = float(sub[\"share_roll3\"].sum())\n",
        "    if s <= 0:\n",
        "        sub[\"share_norm\"] = 1.0 / max(1, len(sub))\n",
        "    else:\n",
        "        sub[\"share_norm\"] = sub[\"share_roll3\"] / s\n",
        "    return sub[[\"department_id\",\"share_norm\"]]\n",
        "\n",
        "# ---------- Level adjustment on dept forecasts ----------\n",
        "def apply_level_adjustment(fc_dept: pd.DataFrame, incoming: pd.DataFrame) -> pd.DataFrame:\n",
        "    out = fc_dept.copy()\n",
        "    if not LEVEL_ADJ[\"enabled\"] or out.empty:\n",
        "        out[\"level_adj_factor\"] = 1.0\n",
        "        return out\n",
        "    d = incoming.copy()\n",
        "    d[\"month\"] = pd.to_datetime(d[\"Date\"]).dt.to_period(\"M\")\n",
        "    hist = d.groupby([\"department_id\",\"month\"], as_index=False)[\"ticket_total\"].sum()\n",
        "    hist[\"month\"] = pd.PeriodIndex(hist[\"month\"], freq=\"M\")\n",
        "\n",
        "    k = int(LEVEL_ADJ[\"lookback_months\"])\n",
        "    clip_min = float(LEVEL_ADJ[\"clip_min\"]); clip_max = float(LEVEL_ADJ[\"clip_max\"])\n",
        "    factors={}\n",
        "    for dept, g in hist.groupby(\"department_id\"):\n",
        "        g = g.sort_values(\"month\")\n",
        "        if len(g) < k*2:\n",
        "            factors[str(dept)] = 1.0\n",
        "            continue\n",
        "        recent = float(g[\"ticket_total\"].tail(k).mean())\n",
        "        baseline = float(g[\"ticket_total\"].tail(k*2).head(k).mean())  # previous k months\n",
        "        if baseline <= 0:\n",
        "            factors[str(dept)] = 1.0\n",
        "        else:\n",
        "            factors[str(dept)] = float(np.clip(recent / baseline, clip_min, clip_max))\n",
        "\n",
        "    out[\"level_adj_factor\"] = out[\"department_id\"].astype(str).map(factors).fillna(1.0)\n",
        "    for col in [\"forecast_monthly_dept\",\"forecast_p05_dept\",\"forecast_p95_dept\"]:\n",
        "        out[col] = out[col].astype(float) * out[\"level_adj_factor\"].astype(float)\n",
        "    return out\n",
        "\n",
        "# ---------- Language shares (unchanged) ----------\n",
        "def compute_language_shares_rolling3(incoming: pd.DataFrame) -> pd.DataFrame:\n",
        "    d = incoming.copy()\n",
        "    d[\"Date\"] = pd.to_datetime(d[\"Date\"])\n",
        "    d[\"month\"] = d[\"Date\"].dt.to_period(\"M\")\n",
        "    d[\"language\"] = d[\"language\"].astype(str).str.strip()\n",
        "    d[\"language\"] = np.where(d[\"language\"].isin(SUPPORTED_LANGS), d[\"language\"], \"English\")\n",
        "    agg = (d.groupby([\"department_id\",\"month\",\"language\"], as_index=False)[\"ticket_total\"].sum()\n",
        "             .sort_values([\"department_id\",\"language\",\"month\"]))\n",
        "    totals = agg.groupby([\"department_id\",\"month\"], as_index=False)[\"ticket_total\"].sum().rename(columns={\"ticket_total\":\"dept_total\"})\n",
        "    agg = agg.merge(totals, on=[\"department_id\",\"month\"], how=\"left\")\n",
        "    agg[\"share\"] = np.where(agg[\"dept_total\"] > 0, agg[\"ticket_total\"]/agg[\"dept_total\"], 0.0)\n",
        "    agg[\"share_roll3\"] = (agg.groupby([\"department_id\",\"language\"])[\"share\"]\n",
        "                            .rolling(3, min_periods=1).mean()\n",
        "                            .reset_index(level=[0,1], drop=True))\n",
        "    agg = agg[agg[\"language\"].isin(SUPPORTED_LANGS)].copy()\n",
        "    return agg[[\"department_id\",\"month\",\"language\",\"share_roll3\"]]\n",
        "\n",
        "def apply_language_split(fc_dept: pd.DataFrame, shares: pd.DataFrame) -> pd.DataFrame:\n",
        "    f = fc_dept.copy()\n",
        "    f[\"month\"] = pd.PeriodIndex(f[\"month\"], freq=\"M\")\n",
        "    lang_df = pd.DataFrame({\"language\": SUPPORTED_LANGS})\n",
        "    f_exp = f.merge(lang_df, how=\"cross\")\n",
        "    sh = shares.copy()\n",
        "    sh[\"month\"] = pd.PeriodIndex(sh[\"month\"], freq=\"M\")\n",
        "    f_exp = f_exp.merge(sh, on=[\"department_id\",\"month\",\"language\"], how=\"left\")\n",
        "    f_exp[\"share_roll3\"] = f_exp[\"share_roll3\"].fillna(0.0)\n",
        "    sums = f_exp.groupby([\"department_id\",\"month\"])[\"share_roll3\"].transform(\"sum\")\n",
        "    f_exp[\"share_norm\"] = np.where(sums > 0, f_exp[\"share_roll3\"]/sums, 0.0)\n",
        "    f_exp[\"forecast_monthly\"] = f_exp[\"forecast_monthly_dept\"].astype(float) * f_exp[\"share_norm\"].astype(float)\n",
        "    f_exp[\"forecast_p05\"] = f_exp[\"forecast_p05_dept\"].astype(float) * f_exp[\"share_norm\"].astype(float)\n",
        "    f_exp[\"forecast_p95\"] = f_exp[\"forecast_p95_dept\"].astype(float) * f_exp[\"share_norm\"].astype(float)\n",
        "    keep = [\"department_id\",\"language\",\"month\",\"forecast_monthly\",\"forecast_p05\",\"forecast_p95\",\"share_norm\",\n",
        "            \"stability_label\",\"winner_model\",\"models_used\",\"level_adj_factor\"]\n",
        "    return f_exp[keep]\n",
        "\n",
        "# ---------- Staffing + daily plan utilities ----------\n",
        "def staffing_rule(accuracy: float, f: float, p95: float) -> float:\n",
        "    if not np.isfinite(accuracy):\n",
        "        return p95\n",
        "    if accuracy < RISK_POLICY[\"critical_low_acc_use_p95\"]:\n",
        "        return p95\n",
        "    if accuracy < RISK_POLICY[\"critical_mid_acc_blend\"]:\n",
        "        return f + 0.5*(p95 - f)\n",
        "    return f\n",
        "\n",
        "def uplift_from_label(label: str) -> float:\n",
        "    if label == \"Broken\": return RISK_POLICY[\"broken_uplift_pct\"]\n",
        "    if label == \"LowData\": return RISK_POLICY[\"lowdata_uplift_pct\"]\n",
        "    if label == \"Volatile\": return RISK_POLICY[\"volatility_uplift_pct\"]\n",
        "    return 0.0\n",
        "\n",
        "def get_weekend_service_policy(mapping: pd.DataFrame) -> pd.DataFrame:\n",
        "    mp = mapping.copy()\n",
        "    mp[\"vertical\"] = mp.get(\"vertical\", \"Unmapped\").astype(str).str.strip()\n",
        "    mp[\"department_name\"] = mp.get(\"department_name\", \"Unknown\").astype(str).str.strip()\n",
        "    payments_no_weekend_names = {\"CA_PYAC\",\"L2 Customer Support\",\"Datatrans L2 Customer Support\",\"Specialist - L2 Customer Support\"}\n",
        "    mp[\"weekend_service\"] = True\n",
        "    mp.loc[mp[\"vertical\"].str.lower() == \"partners\", \"weekend_service\"] = False\n",
        "    is_payments = mp[\"vertical\"].str.lower() == \"payments\"\n",
        "    mp.loc[is_payments & mp[\"department_name\"].isin(payments_no_weekend_names), \"weekend_service\"] = False\n",
        "    return mp[[\"department_id\",\"vertical\",\"department_name\",\"weekend_service\"]].drop_duplicates(\"department_id\")\n",
        "\n",
        "def compute_dept_dow_profile(incoming: pd.DataFrame, lookback_days: int = 180, min_obs: int = 30) -> pd.DataFrame:\n",
        "    d = incoming.copy()\n",
        "    d[\"Date\"] = pd.to_datetime(d[\"Date\"])\n",
        "    maxd = d[\"Date\"].max()\n",
        "    d = d[d[\"Date\"] >= (maxd - pd.Timedelta(days=lookback_days))].copy()\n",
        "    d[\"dow\"] = d[\"Date\"].dt.weekday.astype(int)\n",
        "    agg = d.groupby([\"department_id\",\"dow\"], as_index=False)[\"ticket_total\"].sum()\n",
        "    totals = agg.groupby(\"department_id\", as_index=False)[\"ticket_total\"].sum().rename(columns={\"ticket_total\":\"dept_total\"})\n",
        "    agg = agg.merge(totals, on=\"department_id\", how=\"left\")\n",
        "    agg[\"weight_raw\"] = np.where(agg[\"dept_total\"] > 0, agg[\"ticket_total\"]/agg[\"dept_total\"], 0.0)\n",
        "    depts = d[\"department_id\"].astype(str).unique().tolist()\n",
        "    grid = pd.MultiIndex.from_product([depts, list(range(7))], names=[\"department_id\",\"dow\"]).to_frame(index=False)\n",
        "    prof = grid.merge(agg[[\"department_id\",\"dow\",\"weight_raw\"]], on=[\"department_id\",\"dow\"], how=\"left\").fillna(0.0)\n",
        "    obs = d.groupby(\"department_id\")[\"Date\"].count().rename(\"n_obs\").reset_index()\n",
        "    prof = prof.merge(obs, on=\"department_id\", how=\"left\").fillna({\"n_obs\":0})\n",
        "    def _fallback():\n",
        "        w = np.zeros(7, dtype=float); w[:5] = 1.0/5.0\n",
        "        return w\n",
        "    out=[]\n",
        "    for dept, g in prof.groupby(\"department_id\"):\n",
        "        n = int(g[\"n_obs\"].iloc[0])\n",
        "        if n < min_obs or float(g[\"weight_raw\"].sum()) <= 0:\n",
        "            w = _fallback()\n",
        "        else:\n",
        "            w = g.sort_values(\"dow\")[\"weight_raw\"].values.astype(float)\n",
        "            s = float(w.sum()); w = w/s if s>0 else _fallback()\n",
        "        for dow, wv in enumerate(w):\n",
        "            out.append({\"department_id\": str(dept), \"dow\": int(dow), \"dow_weight\": float(wv)})\n",
        "    return pd.DataFrame(out)\n",
        "\n",
        "def build_daily_plan_from_monthly_staffing(staff_m: pd.DataFrame, incoming_hist: pd.DataFrame,\n",
        "                                           mapping_policy: pd.DataFrame, dept_dow_profile: pd.DataFrame,\n",
        "                                           horizon_days: int) -> pd.DataFrame:\n",
        "    last_date = incoming_hist[\"Date\"].max()\n",
        "    start = last_date + pd.Timedelta(days=1)\n",
        "    end = start + pd.Timedelta(days=horizon_days - 1)\n",
        "    wknd = dict(zip(mapping_policy[\"department_id\"].astype(str), mapping_policy[\"weekend_service\"].astype(bool)))\n",
        "    prof_key = {(r[\"department_id\"], int(r[\"dow\"])): float(r[\"dow_weight\"]) for _, r in dept_dow_profile.iterrows()}\n",
        "    rows=[]\n",
        "    for _, r in staff_m.iterrows():\n",
        "        dept = str(r[\"department_id\"])\n",
        "        m = r[\"month\"]; m = m if isinstance(m, pd.Period) else pd.Period(m, freq=\"M\")\n",
        "        month_days = pd.date_range(m.start_time, m.end_time, freq=\"D\")\n",
        "        month_days = month_days[(month_days >= start) & (month_days <= end)]\n",
        "        if len(month_days)==0: \n",
        "            continue\n",
        "        weekend_service = bool(wknd.get(dept, True))\n",
        "        w=[]\n",
        "        for dday in month_days:\n",
        "            dow = int(dday.weekday())\n",
        "            if (not weekend_service) and (dow>=5): w.append(0.0)\n",
        "            else: w.append(float(prof_key.get((dept, dow), 0.0)))\n",
        "        w=np.array(w, dtype=float); s=float(w.sum())\n",
        "        if s<=0:\n",
        "            allowed = np.array([(dday.weekday() < 5) if (not weekend_service) else True for dday in month_days], dtype=bool)\n",
        "            w=allowed.astype(float); s=float(w.sum()) if float(w.sum())>0 else 1.0\n",
        "        w=w/s\n",
        "        monthly_total=float(r[\"staffing_volume_monthly\"])\n",
        "        daily_alloc=monthly_total*w\n",
        "        for dday, val in zip(month_days, daily_alloc):\n",
        "            rows.append({\"Date\": dday, \"department_id\": dept,\n",
        "                         \"vertical\": r.get(\"vertical\"), \"department_name\": r.get(\"department_name\"),\n",
        "                         \"weekend_service\": weekend_service,\n",
        "                         \"Staffing_Daily_Tickets\": float(max(0.0, val))})\n",
        "    return pd.DataFrame(rows)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b522d09",
      "metadata": {},
      "source": [
        "## 4) Hierarchical monthly forecasting (Vertical → Dept) + Accuracy + Staffing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "48feec1d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------- Vertical forecast ----------\n",
        "def forecast_monthly_by_vertical(incoming: pd.DataFrame, exo_monthly: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    rows=[]\n",
        "    st_rows=[]\n",
        "    for vert, g in incoming.groupby(\"vertical\"):\n",
        "        g = clean_outliers_daily(g.sort_values(\"Date\"))\n",
        "        vm = g.assign(month=g[\"Date\"].dt.to_period(\"M\")).groupby(\"month\")[\"ticket_total\"].sum().sort_index()\n",
        "        vm.index = pd.PeriodIndex(vm.index, freq=\"M\")\n",
        "        if len(vm)==0:\n",
        "            continue\n",
        "        vm_full = winsorize_monthly(vm)\n",
        "        st = classify_series(vm_full)\n",
        "        st_rows.append({\"vertical\": vert, **st})\n",
        "\n",
        "        brk = detect_structural_break(vm_full, z_thresh=STAB[\"broken_z_thresh\"])\n",
        "        train = apply_break_window(vm_full, brk, min_train_months=ACCURACY_MIN_TRAIN_MONTHS)\n",
        "        ex_train = prepare_monthly_exog(exo_monthly, train.index)\n",
        "\n",
        "        label = st[\"label\"]\n",
        "        blend = (label == \"Stable\")\n",
        "        prophet_cp = 0.25 if label == \"Stable\" else (0.35 if label == \"Volatile\" else 0.45)\n",
        "        allow_arima = (label == \"Stable\") and (vert not in CRITICAL_VERTICALS)\n",
        "\n",
        "        fc_pi={}\n",
        "        used=[]\n",
        "        winner=None\n",
        "\n",
        "        # Prophet\n",
        "        mp, _, fp_pi = fit_prophet_monthly_log_with_pi(train, ex_train, changepoint_prior_scale=prophet_cp)\n",
        "        if fp_pi is not None:\n",
        "            fc_pi[\"Prophet\"] = fp_pi(H_MONTHS); used.append(\"Prophet\")\n",
        "        # ETS\n",
        "        try:\n",
        "            _, fe_pi = fit_ets_monthly_log_with_pi(train)\n",
        "            fc_pi[\"ETS\"] = fe_pi(H_MONTHS); used.append(\"ETS\")\n",
        "        except Exception:\n",
        "            pass\n",
        "        # ARIMA (optional)\n",
        "        if allow_arima:\n",
        "            try:\n",
        "                _, fa_pi, _ = fit_arima_monthly_log_with_pi(train, ex_train)\n",
        "                if fa_pi is not None:\n",
        "                    fc_pi[\"ARIMA\"] = fa_pi(H_MONTHS); used.append(\"ARIMA\")\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        if not fc_pi:\n",
        "            idx = pd.period_range(train.index[-1] + 1, periods=H_MONTHS, freq=\"M\")\n",
        "            val = max(0.0, float(train.mean()))\n",
        "            fc_pi[\"NaiveMean\"] = (pd.Series([val]*H_MONTHS,index=idx),\n",
        "                                 pd.Series([val]*H_MONTHS,index=idx),\n",
        "                                 pd.Series([val]*H_MONTHS,index=idx))\n",
        "            used.append(\"NaiveMean\")\n",
        "\n",
        "        cv = rolling_cv_monthly(train, ex_train) or {}\n",
        "        mean, p05, p95, meta = select_or_blend_with_pi(fc_pi, cv_scores=cv, blend=blend)\n",
        "        winner = meta.get(\"winner\")\n",
        "\n",
        "        for per in mean.index:\n",
        "            rows.append({\n",
        "                \"vertical\": vert,\n",
        "                \"month\": per,\n",
        "                \"forecast_monthly_vertical\": float(mean.loc[per]),\n",
        "                \"forecast_p05_vertical\": float(p05.loc[per]),\n",
        "                \"forecast_p95_vertical\": float(p95.loc[per]),\n",
        "                \"stability_label\": label,\n",
        "                \"models_used\": \",\".join(used),\n",
        "                \"winner_model\": winner,\n",
        "                \"prophet_cp\": float(prophet_cp),\n",
        "                \"break_start\": st.get(\"break_start\"),\n",
        "            })\n",
        "\n",
        "    fc_v = pd.DataFrame(rows)\n",
        "    if not fc_v.empty:\n",
        "        fc_v[\"month\"] = pd.PeriodIndex(fc_v[\"month\"], freq=\"M\")\n",
        "    st_v = pd.DataFrame(st_rows)\n",
        "    return fc_v, st_v\n",
        "\n",
        "\n",
        "# ---------- Allocation vertical -> dept ----------\n",
        "def allocate_vertical_to_dept(fc_vertical: pd.DataFrame, incoming: pd.DataFrame, shares_vd: pd.DataFrame) -> pd.DataFrame:\n",
        "    rows=[]\n",
        "    inc = incoming.copy()\n",
        "    inc[\"month\"] = pd.to_datetime(inc[\"Date\"]).dt.to_period(\"M\")\n",
        "    depts_by_vert = inc.groupby(\"vertical\")[\"department_id\"].apply(lambda s: sorted(set(s.astype(str)))).to_dict()\n",
        "\n",
        "    for _, r in fc_vertical.iterrows():\n",
        "        vert = r[\"vertical\"]; per = r[\"month\"]\n",
        "        dept_ids = np.array(depts_by_vert.get(vert, []), dtype=object)\n",
        "        if len(dept_ids)==0:\n",
        "            continue\n",
        "\n",
        "        # prefer shares for same forecast month; fallback to latest available historical shares\n",
        "        if (shares_vd[\"vertical\"].eq(vert) & shares_vd[\"month\"].eq(per)).any():\n",
        "            use_month = per\n",
        "        else:\n",
        "            hist_sub = shares_vd[shares_vd[\"vertical\"]==vert]\n",
        "            use_month = hist_sub[\"month\"].max() if not hist_sub.empty else per\n",
        "\n",
        "        sh_norm = normalize_shares_for_month(shares_vd, vert, use_month, dept_ids)\n",
        "\n",
        "        for _, sd in sh_norm.iterrows():\n",
        "            dept = str(sd[\"department_id\"])\n",
        "            share = float(sd[\"share_norm\"])\n",
        "            rows.append({\n",
        "                \"vertical\": vert,\n",
        "                \"department_id\": dept,\n",
        "                \"month\": per,\n",
        "                \"forecast_monthly_dept\": float(r[\"forecast_monthly_vertical\"] * share),\n",
        "                \"forecast_p05_dept\": float(r[\"forecast_p05_vertical\"] * share),\n",
        "                \"forecast_p95_dept\": float(r[\"forecast_p95_vertical\"] * share),\n",
        "                \"share_vert_to_dept\": share,\n",
        "                \"stability_label\": r.get(\"stability_label\"),\n",
        "                \"models_used\": r.get(\"models_used\"),\n",
        "                \"winner_model\": r.get(\"winner_model\"),\n",
        "                \"prophet_cp\": r.get(\"prophet_cp\"),\n",
        "            })\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    if not df.empty:\n",
        "        df[\"month\"] = pd.PeriodIndex(df[\"month\"], freq=\"M\")\n",
        "    return df\n",
        "\n",
        "\n",
        "# ---------- Accuracy backtest (hierarchical) ----------\n",
        "def backtest_accuracy_hierarchical(incoming: pd.DataFrame, exo_monthly: pd.DataFrame) -> pd.DataFrame:\n",
        "    inc = incoming.copy()\n",
        "    inc[\"Date\"] = pd.to_datetime(inc[\"Date\"])\n",
        "    inc[\"month\"] = inc[\"Date\"].dt.to_period(\"M\")\n",
        "\n",
        "    dept_m = inc.groupby([\"department_id\",\"vertical\",\"month\"], as_index=False)[\"ticket_total\"].sum()\n",
        "    dept_m[\"month\"] = pd.PeriodIndex(dept_m[\"month\"], freq=\"M\")\n",
        "    vert_m = inc.groupby([\"vertical\",\"month\"], as_index=False)[\"ticket_total\"].sum()\n",
        "    vert_m[\"month\"] = pd.PeriodIndex(vert_m[\"month\"], freq=\"M\")\n",
        "\n",
        "    shares_vd = dept_share_roll3_within_vertical(inc)\n",
        "\n",
        "    out=[]\n",
        "    for vert, vdf in vert_m.groupby(\"vertical\"):\n",
        "        vdf = vdf.sort_values(\"month\")\n",
        "        if len(vdf) < (ACCURACY_MIN_TRAIN_MONTHS + ACCURACY_BACKTEST_MONTHS):\n",
        "            continue\n",
        "\n",
        "        eval_months = vdf[\"month\"].iloc[-ACCURACY_BACKTEST_MONTHS:].tolist()\n",
        "        dept_ids = dept_m.loc[dept_m[\"vertical\"]==vert, \"department_id\"].astype(str).unique().tolist()\n",
        "        if not dept_ids:\n",
        "            continue\n",
        "\n",
        "        pred_map = {d: [] for d in dept_ids}\n",
        "        lo_map   = {d: [] for d in dept_ids}\n",
        "        hi_map   = {d: [] for d in dept_ids}\n",
        "        act_map  = {d: [] for d in dept_ids}\n",
        "\n",
        "        splits_done=0\n",
        "        for m in eval_months[::-1]:\n",
        "            train_end = m - 1\n",
        "            train_v = vdf[vdf[\"month\"] <= train_end].set_index(\"month\")[\"ticket_total\"]\n",
        "            train_v.index = pd.PeriodIndex(train_v.index, freq=\"M\")\n",
        "            if len(train_v) < ACCURACY_MIN_TRAIN_MONTHS:\n",
        "                continue\n",
        "\n",
        "            ex_train = prepare_monthly_exog(exo_monthly, train_v.index)\n",
        "            st = classify_series(train_v)\n",
        "            label = st[\"label\"]\n",
        "            blend = (label == \"Stable\")\n",
        "            prophet_cp = 0.30 if label == \"Stable\" else (0.40 if label == \"Volatile\" else 0.50)\n",
        "            allow_arima = (label == \"Stable\") and (vert not in CRITICAL_VERTICALS)\n",
        "\n",
        "            fc_pi={}\n",
        "            mp, _, fp_pi = fit_prophet_monthly_log_with_pi(train_v, ex_train, changepoint_prior_scale=prophet_cp)\n",
        "            if fp_pi is not None:\n",
        "                fc_pi[\"Prophet\"] = fp_pi(ACCURACY_HORIZON_MONTHS)\n",
        "            try:\n",
        "                _, fe_pi = fit_ets_monthly_log_with_pi(train_v)\n",
        "                fc_pi[\"ETS\"] = fe_pi(ACCURACY_HORIZON_MONTHS)\n",
        "            except Exception:\n",
        "                pass\n",
        "            if allow_arima:\n",
        "                try:\n",
        "                    _, fa_pi, _ = fit_arima_monthly_log_with_pi(train_v, ex_train)\n",
        "                    if fa_pi is not None:\n",
        "                        fc_pi[\"ARIMA\"] = fa_pi(ACCURACY_HORIZON_MONTHS)\n",
        "                except Exception:\n",
        "                    pass\n",
        "            if not fc_pi:\n",
        "                continue\n",
        "\n",
        "            cv = rolling_cv_monthly(train_v, ex_train) or {}\n",
        "            mean_v, p05_v, p95_v, _ = select_or_blend_with_pi(fc_pi, cv_scores=cv, blend=blend)\n",
        "            if m not in mean_v.index:\n",
        "                continue\n",
        "\n",
        "            v_fore = float(mean_v.loc[m]); v_lo=float(p05_v.loc[m]); v_hi=float(p95_v.loc[m])\n",
        "\n",
        "            sh_hist = shares_vd[(shares_vd[\"vertical\"]==vert) & (shares_vd[\"month\"]<=train_end)]\n",
        "            if sh_hist.empty:\n",
        "                sh_norm = pd.DataFrame({\"department_id\": dept_ids, \"share_norm\": [1/len(dept_ids)]*len(dept_ids)})\n",
        "            else:\n",
        "                use_month = sh_hist[\"month\"].max()\n",
        "                sh_norm = normalize_shares_for_month(shares_vd, vert, use_month, np.array(dept_ids, dtype=object))\n",
        "\n",
        "            act_sub = dept_m[(dept_m[\"vertical\"]==vert) & (dept_m[\"month\"]==m)][[\"department_id\",\"ticket_total\"]].copy()\n",
        "            act_sub[\"department_id\"] = act_sub[\"department_id\"].astype(str)\n",
        "            act_dict = dict(zip(act_sub[\"department_id\"], act_sub[\"ticket_total\"].astype(float)))\n",
        "\n",
        "            for _, sd in sh_norm.iterrows():\n",
        "                dpt = str(sd[\"department_id\"])\n",
        "                share = float(sd[\"share_norm\"])\n",
        "                pred_map[dpt].append(v_fore * share)\n",
        "                lo_map[dpt].append(v_lo * share)\n",
        "                hi_map[dpt].append(v_hi * share)\n",
        "                act_map[dpt].append(float(act_dict.get(dpt, 0.0)))\n",
        "\n",
        "            splits_done += 1\n",
        "            if splits_done >= ACCURACY_MAX_SPLITS:\n",
        "                break\n",
        "\n",
        "        for dpt in dept_ids:\n",
        "            y_true = np.array(act_map[dpt], dtype=float)\n",
        "            y_pred = np.array(pred_map[dpt], dtype=float)\n",
        "            if len(y_true)==0:\n",
        "                continue\n",
        "            out.append({\n",
        "                \"department_id\": str(dpt),\n",
        "                \"vertical\": vert,\n",
        "                \"Eval_Months\": int(len(y_true)),\n",
        "                \"sMAPE_%\": float(smape(y_true, y_pred)),\n",
        "                \"MAE\": float(np.mean(np.abs(y_pred - y_true))),\n",
        "                \"Bias_%\": float(bias_pct(y_true, y_pred)),\n",
        "                \"Accuracy_%\": float(accuracy_pct(y_true, y_pred)),\n",
        "                \"PI_Coverage_95_%\": float(coverage_95(y_true, np.array(lo_map[dpt],dtype=float), np.array(hi_map[dpt],dtype=float)))\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(out)\n",
        "\n",
        "\n",
        "# ---------- Staffing monthly ----------\n",
        "def compute_staffing_monthly(fc_dept: pd.DataFrame, acc: pd.DataFrame, mapping: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = fc_dept.merge(mapping, on=\"department_id\", how=\"left\", suffixes=(\"\", \"_map\"))\n",
        "    # Guarantee vertical/department_name columns exist after merge\n",
        "    if \"vertical\" not in df.columns:\n",
        "        df[\"vertical\"] = None\n",
        "    if \"vertical_map\" in df.columns:\n",
        "        df[\"vertical\"] = df[\"vertical\"].fillna(df[\"vertical_map\"])\n",
        "        df.drop(columns=[\"vertical_map\"], inplace=True, errors=\"ignore\")\n",
        "    if \"department_name\" not in df.columns:\n",
        "        df[\"department_name\"] = None\n",
        "    if \"department_name_map\" in df.columns:\n",
        "        df[\"department_name\"] = df[\"department_name\"].fillna(df[\"department_name_map\"])\n",
        "        df.drop(columns=[\"department_name_map\"], inplace=True, errors=\"ignore\")\n",
        "    df[\"vertical\"] = df[\"vertical\"].fillna(\"Unmapped\")\n",
        "    df[\"department_name\"] = df[\"department_name\"].fillna(\"Unknown\")\n",
        "    if acc is None or acc.empty:\n",
        "        df[\"Accuracy_%\"] = np.nan\n",
        "    else:\n",
        "        df = df.merge(acc[[\"department_id\",\"Accuracy_%\"]], on=\"department_id\", how=\"left\")\n",
        "\n",
        "    staff=[]\n",
        "    uplift=[]\n",
        "    for _, r in df.iterrows():\n",
        "        f = float(r[\"forecast_monthly_dept\"])\n",
        "        p95 = float(r[\"forecast_p95_dept\"]) if np.isfinite(r[\"forecast_p95_dept\"]) else f\n",
        "        accv = float(r[\"Accuracy_%\"]) if np.isfinite(r[\"Accuracy_%\"]) else np.nan\n",
        "        label = r.get(\"stability_label\",\"Stable\")\n",
        "        vert = r.get(\"vertical\", None)\n",
        "\n",
        "        base = staffing_rule(accv, f, p95) if vert in CRITICAL_VERTICALS else f\n",
        "        up = uplift_from_label(label)\n",
        "        staff.append(base * (1.0 + up/100.0))\n",
        "        uplift.append(up)\n",
        "\n",
        "    df[\"risk_uplift_pct\"] = uplift\n",
        "    df[\"staffing_volume_monthly\"] = staff\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de4500e2",
      "metadata": {},
      "source": [
        "## 5) Run + Export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4f479b0a",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "17:00:13 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:00:13 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:00:13 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:00:23 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:00:24 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:00:35 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:00:36 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:00:48 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:00:49 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:00:49 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:00:50 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:01:02 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:01:03 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:01:14 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:01:15 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:01:27 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:01:28 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:01:28 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:01:28 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:01:40 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:01:41 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:01:53 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:01:54 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:02:06 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:02:07 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:02:16 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:02:17 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:02:25 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:02:26 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:02:34 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:02:34 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:02:43 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:02:44 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:02:57 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:02:57 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:03:09 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:03:09 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:03:22 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:03:22 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:03:34 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:03:35 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:03:48 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:03:48 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:03:58 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:03:59 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:04:11 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:04:11 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:04:23 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:04:24 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:04:36 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:04:36 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:04:46 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:04:47 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:04:57 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:04:58 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:05:10 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:05:10 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:05:22 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:05:22 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:05:32 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:05:32 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:05:42 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:05:43 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:05:53 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:05:54 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:06:06 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:06:06 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:06:15 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:06:16 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:06:25 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:06:26 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:06:35 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:06:36 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:06:48 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:06:48 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:06:57 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:06:57 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:07:06 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:07:06 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:07:16 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:07:16 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:07:27 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:07:27 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:07:36 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:07:36 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:07:45 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:07:46 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:07:55 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:07:55 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:08:05 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:08:05 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:08:13 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:08:14 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:08:22 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:08:22 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:08:31 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:08:32 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:08:41 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:08:41 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:08:51 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:08:51 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:08:59 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:09:00 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:09:09 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:09:09 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:09:22 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:09:22 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:09:34 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:09:35 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:09:47 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:09:48 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:09:59 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:10:00 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:10:12 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:10:12 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:10:22 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:10:23 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:10:34 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:10:35 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:10:47 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:10:48 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:11:00 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:11:00 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:11:10 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:11:11 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:11:21 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:11:21 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:11:33 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:11:34 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:11:45 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:11:45 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:11:55 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:11:55 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:12:05 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:12:06 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:12:16 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:12:17 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:12:29 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:12:30 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:12:39 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:12:39 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:12:49 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:12:49 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:12:59 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:13:00 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:13:11 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:13:12 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:13:21 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:13:21 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:13:30 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:13:31 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:13:40 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:13:41 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:13:50 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:13:51 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:14:00 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:14:00 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:14:09 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:14:10 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:14:18 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:14:19 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:14:29 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:14:29 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:14:38 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:14:39 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:14:48 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:14:48 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:14:57 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:14:57 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:15:07 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:15:08 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:15:17 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:15:17 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:15:26 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:15:27 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:15:35 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:15:36 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:15:49 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:15:49 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:16:00 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:16:01 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:16:14 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:16:14 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:16:27 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:16:28 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:16:40 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:16:40 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:16:51 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:16:52 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:17:03 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:17:04 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:17:16 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:17:17 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:17:29 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:17:29 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:17:39 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:17:40 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:17:50 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:17:51 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:18:03 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:18:04 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:18:15 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:18:15 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:18:25 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:18:25 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:18:35 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:18:36 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:18:46 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:18:47 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:18:59 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:18:59 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:19:08 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:19:09 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:19:18 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:19:19 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:19:29 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:19:29 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:19:41 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:19:41 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:19:50 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:19:50 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:19:59 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:19:59 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:20:09 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:20:09 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:20:20 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:20:20 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:20:29 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:20:29 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:20:38 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:20:39 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:20:48 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:20:48 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:20:58 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:20:58 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:21:06 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:21:07 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:21:16 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:21:16 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:21:25 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:21:25 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:21:35 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:21:35 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:21:44 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:21:45 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:21:53 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:21:54 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:22:03 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:22:03 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:22:12 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:22:13 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:22:22 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:22:22 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:22:31 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:22:31 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:22:40 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:22:40 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:22:49 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:22:50 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:22:59 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:22:59 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:23:08 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:23:09 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:23:17 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:23:18 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:23:27 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:23:27 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:23:34 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:23:35 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:23:44 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:23:44 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:23:54 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:23:54 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:24:03 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:24:03 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:24:10 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:24:10 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:24:18 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:24:18 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:24:27 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:24:28 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:24:36 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:24:37 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:24:44 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:24:44 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:24:51 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:24:51 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:24:59 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:24:59 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:25:08 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:25:09 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:25:15 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:25:15 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:25:22 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:25:22 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:25:30 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:25:30 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:25:39 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:25:39 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:25:45 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:25:46 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:25:52 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:25:52 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:25:59 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:25:59 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:26:07 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:26:07 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:26:13 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:26:13 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:26:20 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:26:20 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:26:28 - cmdstanpy - INFO - Chain [1] done processing\n",
            "17:26:28 - cmdstanpy - INFO - Chain [1] start processing\n",
            "17:26:34 - cmdstanpy - INFO - Chain [1] done processing\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'vertical'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_10736\\40378378.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     65\u001b[39m     vertical_stability.sort_values([\u001b[33m\"vertical\"\u001b[39m]).to_excel(w, \u001b[33m\"vertical_stability\"\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     66\u001b[39m     fc_vertical.sort_values([\u001b[33m\"vertical\"\u001b[39m,\u001b[33m\"month\"\u001b[39m]).to_excel(w, \u001b[33m\"forecast_vertical_monthly\"\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     67\u001b[39m     shares_vd.sort_values([\u001b[33m\"vertical\"\u001b[39m,\u001b[33m\"department_id\"\u001b[39m,\u001b[33m\"month\"\u001b[39m]).to_excel(w, \u001b[33m\"dept_share_roll3_within_vertical\"\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     68\u001b[39m \n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     fc_dept_adj.merge(mapping, on=\u001b[33m\"department_id\"\u001b[39m, how=\u001b[33m\"left\"\u001b[39m).sort_values([\u001b[33m\"vertical\"\u001b[39m,\u001b[33m\"department_id\"\u001b[39m,\u001b[33m\"month\"\u001b[39m]).to_excel(w, \u001b[33m\"forecast_dept_monthly\"\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     70\u001b[39m     fc_dept_lang.sort_values([\u001b[33m\"vertical\"\u001b[39m,\u001b[33m\"department_id\"\u001b[39m,\u001b[33m\"language\"\u001b[39m,\u001b[33m\"month\"\u001b[39m]).to_excel(w, \u001b[33m\"forecast_dept_lang_monthly\"\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     71\u001b[39m \n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m acc_dept \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m acc_dept.empty:\n",
            "\u001b[32mc:\\Users\\anto-\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[39m\n\u001b[32m   7190\u001b[39m                 \u001b[33mf\"Length of ascending ({len(ascending)})\"\u001b[39m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m   7191\u001b[39m                 \u001b[33mf\" != length of by ({len(by)})\"\u001b[39m\n\u001b[32m   7192\u001b[39m             )\n\u001b[32m   7193\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m len(by) > \u001b[32m1\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m7194\u001b[39m             keys = [self._get_label_or_level_values(x, axis=axis) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;28;01min\u001b[39;00m by]\n\u001b[32m   7195\u001b[39m \n\u001b[32m   7196\u001b[39m             \u001b[38;5;66;03m# need to rewrap columns in Series to apply key function\u001b[39;00m\n\u001b[32m   7197\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[32mc:\\Users\\anto-\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m7194\u001b[39m         ...     key=\u001b[38;5;28;01mlambda\u001b[39;00m x: np.argsort(index_natsorted(df[\u001b[33m\"time\"\u001b[39m]))\n",
            "\u001b[32mc:\\Users\\anto-\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1910\u001b[39m             values = self.xs(key, axis=other_axes[\u001b[32m0\u001b[39m])._values\n\u001b[32m   1911\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1912\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1913\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1915\u001b[39m \n\u001b[32m   1916\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1917\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
            "\u001b[31mKeyError\u001b[39m: 'vertical'"
          ]
        }
      ],
      "source": [
        "incoming = load_incoming(INCOMING_SOURCE_PATH, INCOMING_SHEET)\n",
        "mapping = load_dept_map(DEPT_MAP_PATH, DEPT_MAP_SHEET)\n",
        "incoming = apply_mapping(incoming, mapping)\n",
        "\n",
        "dept_prod, dept_dow_prod = load_productivity(PRODUCTIVITY_PATH)\n",
        "\n",
        "exo_daily, exo_monthly = build_exogenous_calendar(incoming, horizon_days=DAILY_HORIZON_DAYS)\n",
        "\n",
        "shares_vd = dept_share_roll3_within_vertical(incoming)\n",
        "fc_vertical, vertical_stability = forecast_monthly_by_vertical(incoming, exo_monthly)\n",
        "fc_dept = allocate_vertical_to_dept(fc_vertical, incoming, shares_vd)\n",
        "\n",
        "# Level adjustment\n",
        "fc_dept_adj = apply_level_adjustment(fc_dept, incoming)\n",
        "\n",
        "# Accuracy (hierarchical)\n",
        "acc_dept = backtest_accuracy_hierarchical(incoming, exo_monthly) if ENABLE_ACCURACY_TABLE else pd.DataFrame()\n",
        "if acc_dept is not None and not acc_dept.empty:\n",
        "    acc_dept = acc_dept.merge(mapping, on=\"department_id\", how=\"left\", suffixes=(\"\", \"_map\"))\n",
        "    # Hard guarantee: vertical column exists (fallback to mapped or \"Unmapped\")\n",
        "    if \"vertical\" not in acc_dept.columns:\n",
        "        acc_dept[\"vertical\"] = None\n",
        "    if \"vertical_map\" in acc_dept.columns:\n",
        "        acc_dept[\"vertical\"] = acc_dept[\"vertical\"].fillna(acc_dept[\"vertical_map\"])\n",
        "        acc_dept.drop(columns=[\"vertical_map\"], inplace=True, errors=\"ignore\")\n",
        "    acc_dept[\"vertical\"] = acc_dept[\"vertical\"].fillna(\"Unmapped\")\n",
        "    # Sort only by available columns to avoid KeyError\n",
        "    sort_cols = [c for c in [\"vertical\",\"department_id\"] if c in acc_dept.columns]\n",
        "    acc_dept = acc_dept.sort_values(sort_cols)\n",
        "if \"vertical_map\" in acc_dept.columns:\n",
        "    acc_dept[\"vertical\"] = acc_dept.get(\"vertical\").fillna(acc_dept[\"vertical_map\"])\n",
        "    acc_dept.drop(columns=[\"vertical_map\"], inplace=True, errors=\"ignore\")\n",
        "if \"department_name_map\" in acc_dept.columns:\n",
        "    acc_dept[\"department_name\"] = acc_dept.get(\"department_name\").fillna(acc_dept[\"department_name_map\"])\n",
        "    acc_dept.drop(columns=[\"department_name_map\"], inplace=True, errors=\"ignore\")\n",
        "sort_cols = [c for c in [\"vertical\",\"department_id\"] if c in acc_dept.columns]\n",
        "acc_dept = acc_dept.sort_values(sort_cols)\n",
        "\n",
        "# Language split\n",
        "lang_shares = compute_language_shares_rolling3(incoming)\n",
        "fc_dept_lang = apply_language_split(fc_dept_adj, lang_shares).merge(mapping, on=\"department_id\", how=\"left\")\n",
        "\n",
        "# Staffing\n",
        "staff_m = compute_staffing_monthly(fc_dept_adj, acc_dept, mapping)\n",
        "sort_cols = [c for c in [\"vertical\",\"department_id\",\"month\"] if c in staff_m.columns]\n",
        "staff_m = staff_m.sort_values(sort_cols)\n",
        "\n",
        "# Daily plan\n",
        "dept_dow_profile = compute_dept_dow_profile(incoming, lookback_days=DOW_LOOKBACK_DAYS, min_obs=DOW_MIN_OBS)\n",
        "dept_policy = get_weekend_service_policy(mapping)\n",
        "daily_staff = build_daily_plan_from_monthly_staffing(staff_m, incoming, dept_policy, dept_dow_profile, horizon_days=DAILY_HORIZON_DAYS)\n",
        "\n",
        "# FTE conversion via productivity\n",
        "daily_staff[\"dow\"] = pd.to_datetime(daily_staff[\"Date\"]).dt.weekday.astype(int)\n",
        "daily_staff = daily_staff.merge(dept_dow_prod, on=[\"department_id\",\"dow\"], how=\"left\")\n",
        "daily_staff = daily_staff.merge(dept_prod, on=\"department_id\", how=\"left\")\n",
        "daily_staff[\"prod_used\"] = daily_staff[\"avg_tickets_per_agent_day_dow\"].fillna(daily_staff[\"avg_tickets_per_agent_day\"])\n",
        "daily_staff[\"Capacity_FTE_per_day\"] = np.where(\n",
        "    pd.to_numeric(daily_staff[\"prod_used\"], errors=\"coerce\").fillna(0.0) > 0,\n",
        "    daily_staff[\"Staffing_Daily_Tickets\"] / pd.to_numeric(daily_staff[\"prod_used\"], errors=\"coerce\"),\n",
        "    np.nan\n",
        ")\n",
        "\n",
        "with pd.ExcelWriter(OUTPUT_XLSX, engine=\"openpyxl\") as w:\n",
        "    vertical_stability.sort_values([\"vertical\"]).to_excel(w, \"vertical_stability\", index=False)\n",
        "    fc_vertical.sort_values([\"vertical\",\"month\"]).to_excel(w, \"forecast_vertical_monthly\", index=False)\n",
        "    shares_vd.sort_values([\"vertical\",\"department_id\",\"month\"]).to_excel(w, \"dept_share_roll3_within_vertical\", index=False)\n",
        "\n",
        "    fc_dept_adj.merge(mapping, on=\"department_id\", how=\"left\").sort_values([\"vertical\",\"department_id\",\"month\"]).to_excel(w, \"forecast_dept_monthly\", index=False)\n",
        "    fc_dept_lang.sort_values([\"vertical\",\"department_id\",\"language\",\"month\"]).to_excel(w, \"forecast_dept_lang_monthly\", index=False)\n",
        "\n",
        "    if acc_dept is not None and not acc_dept.empty:\n",
        "        acc_dept.to_excel(w, \"accuracy_dept_monthly\", index=False)\n",
        "\n",
        "    staff_m.to_excel(w, \"staffing_monthly_dept\", index=False)\n",
        "    dept_dow_profile.sort_values([\"department_id\",\"dow\"]).to_excel(w, \"dow_profile_dept\", index=False)\n",
        "    daily_staff.sort_values([\"vertical\",\"department_id\",\"Date\"]).to_excel(w, \"daily_capacity_plan_90d\", index=False)\n",
        "\n",
        "print(\"✅ v11 export complete:\", OUTPUT_XLSX)\n",
        "display(acc_dept.head(10) if acc_dept is not None else None)\n",
        "display(staff_m.head(10))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
