{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbaf159b",
   "metadata": {},
   "source": [
    "\n",
    "# corporate_hybrid_forecast_v17_3\n",
    "\n",
    "This notebook extends v17_2 to:\n",
    "\n",
    "1. **Capacity / Productivity from agents** (per month & department) using `input_model/productivity_agents.xlsx`:\n",
    "   - **Capacity** (per month) = *mean daily number of agents with* `item_target_day > 1` *×* *mean* `item_target_day` *(over those agents)*.\n",
    "   - **Productivity** (per month) = *sum of* `prod_total_model` *(all agents, with or without target)*.\n",
    "   - These values feed the **Board_[department]_2627** sheets.\n",
    "\n",
    "2. **New sheet `capacity_forecast`**: a long-format consolidation of all departments and months\n",
    "   (Jan-2026 → Feb-2027) with columns: *Month, department_name, KPI, Total* (no \"Comments\").\n",
    "\n",
    "It preserves the v17_2 pipeline (recommended model mapping → daily forecast → monthly (sum) → Einstein deduction → **bias-based calibration** → export boards).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9c2660",
   "metadata": {},
   "source": [
    "## 1) Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a724864a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT_XLSX → C:\\Users\\pt3canro\\Desktop\\CAPACITY\\outputs\\capacity_forecast_v17_3.xlsx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "BASE_DIR = r'C:\\Users\\pt3canro\\Desktop\\CAPACITY'\n",
    "BASE_DIR = str(Path(BASE_DIR).expanduser().resolve())\n",
    "\n",
    "INPUT_DIR  = str(Path(BASE_DIR) / 'input_model')\n",
    "OUTPUT_DIR = str(Path(BASE_DIR) / 'outputs')\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "INCOMING_SOURCE_PATH = os.path.join(INPUT_DIR, 'Incoming_new.xlsx')\n",
    "INCOMING_SHEET       = 'Main'\n",
    "DEPT_MAP_PATH  = os.path.join(INPUT_DIR, 'department.xlsx')\n",
    "DEPT_MAP_SHEET = 'map'\n",
    "AGENT_CAPACITY_PATH = os.path.join(INPUT_DIR, 'agent_language_n_target.xlsx')\n",
    "EINSTEIN_PATH       = os.path.join(INPUT_DIR, 'einstein.xlsx')\n",
    "INVENTORY_PATH      = os.path.join(INPUT_DIR, 'inventory_month.xlsx')\n",
    "PRODUCTIVITY_AGENTS_PATH = os.path.join(INPUT_DIR, 'productivity_agents.xlsx')\n",
    "\n",
    "OUTPUT_XLSX = os.path.join(OUTPUT_DIR, 'capacity_forecast_v17_3.xlsx')\n",
    "\n",
    "HORIZON_MONTHS = 12\n",
    "VERTICALS_TARGET = ['Payments','Partners','Hospitality']\n",
    "TARGET_TPH = 6.0\n",
    "EXCLUDE_DEPARTMENT_NAME_TOKENS = ['PROJ','DIST','KEY','PROXIMIS']\n",
    "\n",
    "print('OUTPUT_XLSX →', OUTPUT_XLSX)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e990414c",
   "metadata": {},
   "source": [
    "## 2) Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1970cbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from calendar import monthrange\n",
    "\n",
    "# Pick first present column name\n",
    "\n",
    "def pick_col(df, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    lower = {c.lower(): c for c in df.columns}\n",
    "    for c in candidates:\n",
    "        if c.lower() in lower:\n",
    "            return lower[c.lower()]\n",
    "    return None\n",
    "\n",
    "# Normalize headers\n",
    "\n",
    "def std_cols(df):\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "# Month start vectorized\n",
    "\n",
    "def to_month_start(dt_series):\n",
    "    s = pd.to_datetime(dt_series, errors='coerce')\n",
    "    return s.dt.to_period('M').dt.to_timestamp(how='start')\n",
    "\n",
    "# Working days in a month\n",
    "\n",
    "def working_days_in_month(year:int, month:int):\n",
    "    start = datetime(year, month, 1)\n",
    "    end = datetime(year+1,1,1)-timedelta(days=1) if month==12 else datetime(year,month+1,1)-timedelta(days=1)\n",
    "    days = pd.date_range(start, end, freq='D')\n",
    "    return int(np.sum(days.dayofweek < 5))\n",
    "\n",
    "# Quantile monotonicity guard\n",
    "\n",
    "def validate_quantiles(dfm):\n",
    "    viol = dfm[(dfm['forecast_p05_dept']>dfm['forecast_monthly_dept']) | (dfm['forecast_monthly_dept']>dfm['forecast_p95_dept'])]\n",
    "    if not viol.empty:\n",
    "        raise ValueError('Quantile order violation in monthly aggregation.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5006ca6e",
   "metadata": {},
   "source": [
    "## 3) Forecast engines (Baseline/STL + SARIMAX-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b08e5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Baseline (STL / SeasonalNaive fallback)\n",
    "\n",
    "def forecast_daily_baseline(y: pd.Series, horizon_days: int = 365):\n",
    "    y = y.asfreq('D').fillna(0)\n",
    "    if len(y) < 56:\n",
    "        idx = pd.date_range(y.index[-1] + pd.Timedelta(days=1), periods=horizon_days, freq='D')\n",
    "        last_week = y[-7:].to_numpy()\n",
    "        p50 = np.resize(last_week, horizon_days)\n",
    "        resid = (y[7:] - y.shift(7)[7:]).dropna()\n",
    "        std = resid.std() if len(resid)>0 else max(1.0, np.sqrt(max(y.mean(),1)))\n",
    "        p05 = np.clip(p50 - 1.645*std, 0, None)\n",
    "        p95 = p50 + 1.645*std\n",
    "        return pd.DataFrame({'date': idx, 'p50': p50, 'p05': p05, 'p95': p95})\n",
    "    try:\n",
    "        from statsmodels.tsa.seasonal import STL\n",
    "        y_box = np.log1p(y)\n",
    "        stl = STL(y_box, period=7, robust=True)\n",
    "        res = stl.fit()\n",
    "        trend, seas, resid = res.trend, res.seasonal, res.resid\n",
    "        last_trend = trend.iloc[-1]\n",
    "        std = resid.std() if resid.std()>0 else 0.5\n",
    "        idx = pd.date_range(y.index[-1] + pd.Timedelta(days=1), periods=horizon_days, freq='D')\n",
    "        seas_fut = np.resize(seas[-7:].to_numpy(), horizon_days)\n",
    "        mu_log = last_trend + seas_fut\n",
    "        p50 = np.expm1(mu_log); p50 = np.clip(p50, 0, None)\n",
    "        p05 = np.expm1(mu_log - 1.645*std); p05 = np.clip(p05, 0, None)\n",
    "        p95 = np.expm1(mu_log + 1.645*std)\n",
    "        return pd.DataFrame({'date': idx, 'p50': p50, 'p05': p05, 'p95': p95})\n",
    "    except Exception:\n",
    "        idx = pd.date_range(y.index[-1] + pd.Timedelta(days=1), periods=horizon_days, freq='D')\n",
    "        last_week = y[-7:].to_numpy()\n",
    "        p50 = np.resize(last_week, horizon_days)\n",
    "        resid = (y[7:] - y.shift(7)[7:]).dropna()\n",
    "        std = resid.std() if len(resid)>0 else max(1.0, np.sqrt(max(y.mean(),1)))\n",
    "        p05 = np.clip(p50 - 1.645*std, 0, None)\n",
    "        p95 = p50 + 1.645*std\n",
    "        return pd.DataFrame({'date': idx, 'p50': p50, 'p05': p05, 'p95': p95})\n",
    "\n",
    "# SARIMAX-7 (weekly)\n",
    "\n",
    "def forecast_daily_sarimax(y: pd.Series, horizon_days: int = 365):\n",
    "    try:\n",
    "        from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "    except Exception:\n",
    "        return None\n",
    "    y = y.asfreq('D').fillna(0)\n",
    "    y_log = np.log1p(y)\n",
    "    try:\n",
    "        model = SARIMAX(y_log, order=(1,0,1), seasonal_order=(1,0,1,7), enforce_stationarity=False, enforce_invertibility=False)\n",
    "        res = model.fit(disp=False)\n",
    "        pred = res.get_forecast(steps=horizon_days)\n",
    "        mean = np.expm1(pred.predicted_mean).clip(lower=0)\n",
    "        resid = (y[7:] - y.shift(7)[7:]).dropna()\n",
    "        std = resid.std() if len(resid)>0 else 1.0\n",
    "        p05 = np.clip(mean - 1.645*std, 0, None)\n",
    "        p95 = mean + 1.645*std\n",
    "        idx = pd.date_range(y.index[-1] + pd.Timedelta(days=1), periods=horizon_days, freq='D')\n",
    "        return pd.DataFrame({'date': idx, 'p50': mean.values, 'p05': p05.values, 'p95': p95.values})\n",
    "    except Exception:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfdd7ab",
   "metadata": {},
   "source": [
    "## 4) Load inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a894146f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded incoming rows = 458599\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Incoming\n",
    "incoming_raw = pd.read_excel(INCOMING_SOURCE_PATH, sheet_name=INCOMING_SHEET, engine='openpyxl')\n",
    "std_cols(incoming_raw)\n",
    "\n",
    "c_date = pick_col(incoming_raw, ['Date','date'])\n",
    "c_dept = pick_col(incoming_raw, ['department_id','dept_id','Department_ID'])\n",
    "c_cnt  = pick_col(incoming_raw, ['ticket_total','Ticket_Total','count','tickets','qty'])\n",
    "if c_date is None or c_dept is None:\n",
    "    raise KeyError('Incoming_new.xlsx must include Date and department_id columns.')\n",
    "\n",
    "incoming = incoming_raw[[c_date, c_dept] + ([c_cnt] if c_cnt else [])].copy()\n",
    "incoming.columns = ['date','department_id'] + (['ticket_total'] if c_cnt else [])\n",
    "if 'ticket_total' not in incoming.columns:\n",
    "    incoming['ticket_total'] = 1\n",
    "\n",
    "incoming['date']  = pd.to_datetime(incoming['date'], errors='coerce')\n",
    "incoming['month'] = to_month_start(incoming['date'])\n",
    "\n",
    "# Dept map\n",
    "dept_map = pd.read_excel(DEPT_MAP_PATH, sheet_name=DEPT_MAP_SHEET, engine='openpyxl')\n",
    "std_cols(dept_map)\n",
    "dm_id   = pick_col(dept_map, ['department_id','dept_id','Department_ID'])\n",
    "dm_name = pick_col(dept_map, ['department_name','Department','dept_name','Department_Name'])\n",
    "dm_vert = pick_col(dept_map, ['vertical','Vertical'])\n",
    "if dm_id is None or dm_name is None or dm_vert is None:\n",
    "    raise KeyError('department.xlsx must contain department_id, department_name and vertical (sheet map).')\n",
    "\n",
    "dept_map = dept_map[[dm_id, dm_name, dm_vert]].drop_duplicates()\n",
    "dept_map.columns = ['department_id','department_name','vertical']\n",
    "\n",
    "incoming['department_id'] = pd.to_numeric(incoming['department_id'], errors='coerce').astype('Int64')\n",
    "dept_map['department_id'] = pd.to_numeric(dept_map['department_id'], errors='coerce').astype('Int64')\n",
    "\n",
    "incoming = incoming.merge(dept_map, on='department_id', how='left')\n",
    "\n",
    "# Scope & exclusions\n",
    "incoming = incoming[incoming['vertical'].isin(VERTICALS_TARGET)].copy()\n",
    "mask_excl = pd.Series(False, index=incoming.index)\n",
    "for tok in EXCLUDE_DEPARTMENT_NAME_TOKENS:\n",
    "    mask_excl |= incoming['department_name'].astype(str).str.upper().str.contains(tok.upper(), na=False)\n",
    "\n",
    "incoming = incoming.loc[~mask_excl].copy()\n",
    "\n",
    "# Monthly actuals\n",
    "monthly_actuals = (incoming\n",
    "                   .groupby(['vertical','department_id','department_name','month'], as_index=False)\n",
    "                   .agg(actual_volume=('ticket_total','sum')))\n",
    "\n",
    "print('Loaded incoming rows =', len(incoming))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c03de321",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Einstein\n",
    "if Path(EINSTEIN_PATH).exists():\n",
    "    ein = pd.read_excel(EINSTEIN_PATH, engine='openpyxl'); std_cols(ein)\n",
    "    e_date = pick_col(ein, ['Date','date']); e_dept = pick_col(ein, ['department_id','dept_id','Department_ID'])\n",
    "    if e_date and e_dept:\n",
    "        ein_use = ein[[e_date, e_dept]].copy(); ein_use.columns=['date','department_id']\n",
    "        ein_use['date'] = pd.to_datetime(ein_use['date'], errors='coerce'); ein_use['month'] = to_month_start(ein_use['date'])\n",
    "        ein_month = ein_use.groupby(['department_id','month'], as_index=False).size().rename(columns={'size':'einstein_solved'})\n",
    "    else:\n",
    "        ein_month = pd.DataFrame(columns=['department_id','month','einstein_solved'])\n",
    "else:\n",
    "    ein_month = pd.DataFrame(columns=['department_id','month','einstein_solved'])\n",
    "\n",
    "# Capacity (historical optional)\n",
    "if Path(AGENT_CAPACITY_PATH).exists():\n",
    "    cap = pd.read_excel(AGENT_CAPACITY_PATH, engine='openpyxl'); std_cols(cap)\n",
    "    c_year  = pick_col(cap, ['Year','year'])\n",
    "    c_mnum  = pick_col(cap, ['Month_number','month_number','month'])\n",
    "    c_mstart= pick_col(cap, ['MonthStartDate','monthstartdate'])\n",
    "    c_deptc = pick_col(cap, ['department_id','dept_id','Department_ID'])\n",
    "    c_prod_total = pick_col(cap, ['productivity_total','prod_total_model'])\n",
    "    c_avgpd      = pick_col(cap, ['avg_per_day'])\n",
    "    c_hours      = pick_col(cap, ['productive'])\n",
    "    if c_mstart is None and (c_year is not None and c_mnum is not None):\n",
    "        cap['MonthStartDate'] = pd.to_datetime(cap[c_year].astype(str)+'-'+cap[c_mnum].astype(str)+'-01'); c_mstart='MonthStartDate'\n",
    "    if c_mstart is None or c_deptc is None:\n",
    "        monthly_capacity_hist = pd.DataFrame(columns=['department_id','month','capacity'])\n",
    "    else:\n",
    "        cols=[c_mstart,c_deptc]; ren={c_mstart:'month', c_deptc:'department_id'}\n",
    "        if c_prod_total: cols.append(c_prod_total); ren[c_prod_total]='tickets_capacity'\n",
    "        if c_avgpd:      cols.append(c_avgpd);      ren[c_avgpd]='avg_per_day'\n",
    "        if c_hours:      cols.append(c_hours);      ren[c_hours]='productive_hours'\n",
    "        cap_use = cap[cols].rename(columns=ren)\n",
    "        cap_use['month'] = pd.to_datetime(cap_use['month'], errors='coerce').dt.to_period('M').dt.to_timestamp(how='start')\n",
    "        if 'tickets_capacity' in cap_use.columns:\n",
    "            monthly_capacity_hist = cap_use.groupby(['department_id','month'], as_index=False).agg(capacity=('tickets_capacity','sum'))\n",
    "        elif 'avg_per_day' in cap_use.columns:\n",
    "            cap_use['wdays'] = cap_use['month'].apply(lambda d: working_days_in_month(d.year, d.month))\n",
    "            cap_use['capacity'] = cap_use['avg_per_day'] * cap_use['wdays']\n",
    "            monthly_capacity_hist = cap_use.groupby(['department_id','month'], as_index=False).agg(capacity=('capacity','sum'))\n",
    "        elif 'productive_hours' in cap_use.columns:\n",
    "            cap_use['capacity'] = cap_use['productive_hours'] * TARGET_TPH\n",
    "            monthly_capacity_hist = cap_use.groupby(['department_id','month'], as_index=False).agg(capacity=('capacity','sum'))\n",
    "        else:\n",
    "            monthly_capacity_hist = pd.DataFrame(columns=['department_id','month','capacity'])\n",
    "else:\n",
    "    monthly_capacity_hist = pd.DataFrame(columns=['department_id','month','capacity'])\n",
    "\n",
    "# Inventory (optional)\n",
    "if Path(INVENTORY_PATH).exists():\n",
    "    inv = pd.read_excel(INVENTORY_PATH, engine='openpyxl'); std_cols(inv)\n",
    "    i_date = pick_col(inv, ['Date','date']); i_dept = pick_col(inv, ['department_id','dept_id','Department_ID'])\n",
    "    if i_date and i_dept:\n",
    "        inv_use = inv[[i_date,i_dept]].copy(); inv_use.columns=['date','department_id']\n",
    "        inv_use['date']=pd.to_datetime(inv_use['date'], errors='coerce'); inv_use['month']=to_month_start(inv_use['date'])\n",
    "        inv_daily = inv_use.groupby(['department_id','date'], as_index=False).size().rename(columns={'size':'open_count'})\n",
    "        inv_daily['month']=to_month_start(inv_daily['date'])\n",
    "        monthly_inventory = inv_daily.groupby(['department_id','month'], as_index=False).agg(inventory_mean=('open_count','mean'))\n",
    "    else:\n",
    "        monthly_inventory = pd.DataFrame(columns=['department_id','month','inventory_mean'])\n",
    "else:\n",
    "    monthly_inventory = pd.DataFrame(columns=['department_id','month','inventory_mean'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166af1c6",
   "metadata": {},
   "source": [
    "## 5) Recommended model mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d093213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended models loaded for 0 departments (fallback Baseline for others).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "recommended = {}  # department_id -> engine label\n",
    "# Option 1: read from previous remediation report (v17)\n",
    "try:\n",
    "    prev = pd.read_excel(os.path.join(OUTPUT_DIR, 'capacity_forecast_v17.xlsx'), sheet_name='Remediation_Report')\n",
    "    if {'department_id','Recommended_Model'}.issubset(prev.columns):\n",
    "        for _,r in prev.iterrows():\n",
    "            if pd.notna(r['department_id']) and pd.notna(r['Recommended_Model']):\n",
    "                recommended[int(r['department_id'])] = str(r['Recommended_Model'])\n",
    "except Exception:\n",
    "    pass\n",
    "# Option 2: read manual CSV (department_id,model)\n",
    "try:\n",
    "    rm_csv = os.path.join(INPUT_DIR,'recommended_models.csv')\n",
    "    if Path(rm_csv).exists():\n",
    "        dfm = pd.read_csv(rm_csv)\n",
    "        if {'department_id','model'}.issubset(dfm.columns):\n",
    "            for _,r in dfm.iterrows():\n",
    "                recommended[int(r['department_id'])] = str(r['model'])\n",
    "except Exception:\n",
    "    pass\n",
    "print('Recommended models loaded for', len(recommended), 'departments (fallback Baseline for others).')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cfd8d0",
   "metadata": {},
   "source": [
    "## 6) Daily forecast using per-department engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daaaed3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily forecast rows: 5840\n"
     ]
    }
   ],
   "source": [
    "\n",
    "incoming_daily = (incoming.groupby(['department_id','date'], as_index=False)\n",
    "                  .agg(tickets=('ticket_total','sum')))\n",
    "\n",
    "engine_funcs = {\n",
    "    'Baseline(STL)': forecast_daily_baseline,\n",
    "    'STL': forecast_daily_baseline,\n",
    "    'SARIMAX-7': forecast_daily_sarimax\n",
    "}\n",
    "\n",
    "daily_forecasts = []\n",
    "HORIZON_DAYS = 365\n",
    "for dpt_id, grp in incoming_daily.groupby('department_id'):\n",
    "    y = grp.set_index('date')['tickets'].sort_index().asfreq('D').fillna(0)\n",
    "    engine = recommended.get(int(dpt_id), 'Baseline(STL)')\n",
    "    func = engine_funcs.get(engine, forecast_daily_baseline)\n",
    "    try:\n",
    "        fc = func(y, horizon_days=HORIZON_DAYS)\n",
    "        if fc is None:\n",
    "            raise RuntimeError('Engine returned None')\n",
    "    except Exception:\n",
    "        fc = forecast_daily_baseline(y, horizon_days=HORIZON_DAYS)\n",
    "        engine = 'Baseline(STL)'\n",
    "    fc.insert(0, 'department_id', dpt_id)\n",
    "    fc['engine_used'] = engine\n",
    "    daily_forecasts.append(fc)\n",
    "\n",
    "fc_daily_built = pd.concat(daily_forecasts, ignore_index=True) if daily_forecasts else pd.DataFrame()\n",
    "fc_daily_built = fc_daily_built.merge(dept_map, on='department_id', how='left')\n",
    "fc_daily_built = fc_daily_built[fc_daily_built['vertical'].isin(VERTICALS_TARGET)]\n",
    "for tok in EXCLUDE_DEPARTMENT_NAME_TOKENS:\n",
    "    fc_daily_built = fc_daily_built[~fc_daily_built['department_name'].astype(str).str.upper().str.contains(tok.upper(), na=False)]\n",
    "\n",
    "fc_daily_built.rename(columns={'p50':'forecast_daily_dept','p05':'p05_daily_dept','p95':'p95_daily_dept'}, inplace=True)\n",
    "print('Daily forecast rows:', len(fc_daily_built))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04449bbf",
   "metadata": {},
   "source": [
    "## 7) Monthly aggregation + Einstein deduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df24f64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monthly + Einstein done → rows: 208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pt3canro\\AppData\\Local\\Temp\\ipykernel_42256\\2183548159.py:16: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: g.set_index('month')['einstein_rate'].tail(3).mean())\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Monthly sum\n",
    "monthly_fc_raw = (fc_daily_built.assign(month=to_month_start(fc_daily_built['date']))\n",
    "                  .groupby(['vertical','department_id','department_name','month'], as_index=False)\n",
    "                  .agg(forecast_monthly_dept=('forecast_daily_dept','sum'),\n",
    "                       forecast_p05_dept=('p05_daily_dept','sum'),\n",
    "                       forecast_p95_dept=('p95_daily_dept','sum')))\n",
    "validate_quantiles(monthly_fc_raw)\n",
    "\n",
    "# Einstein deduction (using 3-month recent per department)\n",
    "if not ein_month.empty:\n",
    "    hist_rates = monthly_actuals.merge(ein_month, on=['department_id','month'], how='left')\n",
    "    hist_rates['einstein_solved'] = hist_rates['einstein_solved'].fillna(0)\n",
    "    hist_rates['einstein_rate'] = (hist_rates['einstein_solved'] / hist_rates['actual_volume']).replace([np.inf,-np.inf], 0).fillna(0)\n",
    "    recent = (hist_rates.sort_values('month')\n",
    "              .groupby('department_id')\n",
    "              .apply(lambda g: g.set_index('month')['einstein_rate'].tail(3).mean())\n",
    "              .rename('einstein_rate_recent').reset_index())\n",
    "else:\n",
    "    recent = pd.DataFrame(columns=['department_id','einstein_rate_recent'])\n",
    "\n",
    "monthly_adj = monthly_fc_raw.merge(recent, on='department_id', how='left')\n",
    "monthly_adj['einstein_rate_recent'] = monthly_adj['einstein_rate_recent'].fillna(0).clip(0, 0.9)\n",
    "for c in ['forecast_monthly_dept','forecast_p05_dept','forecast_p95_dept']:\n",
    "    monthly_adj[c + '_post_einstein'] = monthly_adj[c] * (1 - monthly_adj['einstein_rate_recent'])\n",
    "\n",
    "print('Monthly + Einstein done → rows:', len(monthly_adj))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d26575",
   "metadata": {},
   "source": [
    "## 8) Bias-based calibration (from Model_Used_and_Error table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08f20e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration applied. Share of rows with calib_factor != 1: 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build Model_Used_and_Error from user's table (can be replaced by a dynamic backtest later)\n",
    "model_used_error_df = pd.DataFrame([\n",
    "    {'vertical':'Hospitality','department_id':7,'department_name':'CS_PMSH_L1','model_used':'STL','backtest_months':12,'mape_pct':29.35,'wape_pct':26.00,'bias_pct':-0.40},\n",
    "    {'vertical':'Hospitality','department_id':8,'department_name':'CS_PMSP_CLOUD_L1','model_used':'STL','backtest_months':12,'mape_pct':43.80,'wape_pct':39.12,'bias_pct':12.64},\n",
    "    {'vertical':'Hospitality','department_id':11,'department_name':'CS_PMSP_CLOUD_L2','model_used':'STL','backtest_months':12,'mape_pct':27.76,'wape_pct':22.27,'bias_pct':-4.06},\n",
    "    {'vertical':'Hospitality','department_id':23,'department_name':'CS_PMSP_FRANCE','model_used':'STL','backtest_months':12,'mape_pct':31.44,'wape_pct':25.79,'bias_pct':-1.79},\n",
    "    {'vertical':'Hospitality','department_id':5,'department_name':'CS_PMSP_INTEG','model_used':'STL','backtest_months':12,'mape_pct':20.88,'wape_pct':17.68,'bias_pct':-2.70},\n",
    "    {'vertical':'Hospitality','department_id':9,'department_name':'CS_PMSP_PREM_L1','model_used':'STL','backtest_months':12,'mape_pct':17.29,'wape_pct':14.44,'bias_pct':-3.07},\n",
    "    {'vertical':'Hospitality','department_id':10,'department_name':'CS_PMSP_PREM_L2','model_used':'STL','backtest_months':12,'mape_pct':28.77,'wape_pct':23.23,'bias_pct':-1.12},\n",
    "    {'vertical':'Partners','department_id':12,'department_name':'CS_PART_APAC','model_used':'STL','backtest_months':12,'mape_pct':20.88,'wape_pct':20.99,'bias_pct':-5.14},\n",
    "    {'vertical':'Partners','department_id':13,'department_name':'CS_PART_EMEA','model_used':'STL','backtest_months':12,'mape_pct':27.76,'wape_pct':25.91,'bias_pct':7.36},\n",
    "    {'vertical':'Partners','department_id':14,'department_name':'CS_PART_LATAM','model_used':'STL','backtest_months':12,'mape_pct':26.60,'wape_pct':21.38,'bias_pct':5.29},\n",
    "    {'vertical':'Partners','department_id':15,'department_name':'CS_PART_US','model_used':'STL','backtest_months':12,'mape_pct':40.03,'wape_pct':30.97,'bias_pct':-9.47},\n",
    "    {'vertical':'Payments','department_id':3,'department_name':'CA_PYAC','model_used':'STL','backtest_months':12,'mape_pct':18.63,'wape_pct':18.70,'bias_pct':1.28},\n",
    "    {'vertical':'Payments','department_id':1,'department_name':'CS_GT3C_EU','model_used':'STL','backtest_months':12,'mape_pct':24.80,'wape_pct':22.19,'bias_pct':0.58},\n",
    "    {'vertical':'Payments','department_id':18,'department_name':'Datatrans L2 Customer Support','model_used':'STL','backtest_months':12,'mape_pct':54.89,'wape_pct':38.55,'bias_pct':-7.28},\n",
    "    {'vertical':'Payments','department_id':2,'department_name':'L2 Customer Support','model_used':'STL','backtest_months':12,'mape_pct':28.56,'wape_pct':24.46,'bias_pct':-1.36},\n",
    "    {'vertical':'Payments','department_id':21,'department_name':'Specialist - L2 Customer Support','model_used':'STL','backtest_months':12,'mape_pct':42.25,'wape_pct':41.25,'bias_pct':17.26},\n",
    "])\n",
    "\n",
    "calib_from_bias = model_used_error_df[['department_id','bias_pct']].copy()\n",
    "calib_from_bias['department_id'] = pd.to_numeric(calib_from_bias['department_id'], errors='coerce')\n",
    "calib_from_bias['calib_factor'] = (1 - calib_from_bias['bias_pct']/100.0).clip(0.70, 1.30)\n",
    "\n",
    "monthly_adj = monthly_adj.drop(columns=['calib_factor'], errors='ignore')\n",
    "monthly_adj = monthly_adj.merge(calib_from_bias[['department_id','calib_factor']], on='department_id', how='left')\n",
    "monthly_adj['calib_factor'] = monthly_adj['calib_factor'].fillna(1.0)\n",
    "\n",
    "for c in ['forecast_monthly_dept_post_einstein','forecast_p05_dept_post_einstein','forecast_p95_dept_post_einstein']:\n",
    "    monthly_adj[c + '_cal'] = monthly_adj[c] * monthly_adj['calib_factor']\n",
    "\n",
    "print('Calibration applied. Share of rows with calib_factor != 1:', (monthly_adj['calib_factor']!=1.0).mean().round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c882e2",
   "metadata": {},
   "source": [
    "## 9) Capacity & Productivity from productivity_agents.xlsx (agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5327a3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agents-based capacity/productivity rows: 379\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The file contains daily agent-level (or agent-day) information.\n",
    "# We compute per department-month:\n",
    "#   - agents_gt1_day: daily count of agents with item_target_day > 1 (unique agents if available, else row count)\n",
    "#   - target_mean_gt1_day: daily mean of item_target_day over rows with item_target_day > 1\n",
    "#   - prod_sum_day: daily sum of prod_total_model (all rows, no filter)\n",
    "# Then:\n",
    "#   - Capacity_month = mean(agents_gt1_day across the month) * mean(target_mean_gt1_day across the month)\n",
    "#   - Productivity_month = sum(prod_sum_day across the month)\n",
    "\n",
    "if Path(PRODUCTIVITY_AGENTS_PATH).exists():\n",
    "    pa = pd.read_excel(PRODUCTIVITY_AGENTS_PATH, engine='openpyxl')\n",
    "    std_cols(pa)\n",
    "    c_date = pick_col(pa, ['date','Date','work_date'])\n",
    "    c_dept = pick_col(pa, ['department_id','dept_id','Department_ID'])\n",
    "    c_agent= pick_col(pa, ['agent_id','Agent_ID','worker_id'])\n",
    "    c_target = pick_col(pa, ['item_target_day','target_day','item_target'])\n",
    "    c_prod   = pick_col(pa, ['prod_total_model','productivity','prod_model'])\n",
    "\n",
    "    if c_date is None or c_dept is None or c_target is None or c_prod is None:\n",
    "        print('WARNING: productivity_agents.xlsx missing required columns. Skipping agents-based capacity/productivity.')\n",
    "        cap_prod_month = pd.DataFrame(columns=['department_id','month','capacity_agents','productivity_agents'])\n",
    "    else:\n",
    "        pa = pa[[c_date, c_dept] + ([c_agent] if c_agent else []) + [c_target, c_prod]].copy()\n",
    "        pa.columns = ['date','department_id'] + (['agent_id'] if c_agent else []) + ['item_target_day','prod_total_model']\n",
    "        pa['date'] = pd.to_datetime(pa['date'], errors='coerce')\n",
    "        pa['month'] = to_month_start(pa['date'])\n",
    "        pa['department_id'] = pd.to_numeric(pa['department_id'], errors='coerce')\n",
    "        pa['item_target_day'] = pd.to_numeric(pa['item_target_day'], errors='coerce')\n",
    "        pa['prod_total_model'] = pd.to_numeric(pa['prod_total_model'], errors='coerce')\n",
    "\n",
    "        # Daily aggregates by department\n",
    "        if 'agent_id' in pa.columns:\n",
    "            # unique agents per day with target>1\n",
    "            daily_gt1 = (pa[pa['item_target_day']>1]\n",
    "                         .groupby(['department_id','date'])['agent_id'].nunique()\n",
    "                         .rename('agents_gt1_day').reset_index())\n",
    "        else:\n",
    "            # fallback: count rows as agent-day records\n",
    "            daily_gt1 = (pa[pa['item_target_day']>1]\n",
    "                         .groupby(['department_id','date'], as_index=False)\n",
    "                         .size().rename(columns={'size':'agents_gt1_day'}))\n",
    "\n",
    "        daily_target = (pa[pa['item_target_day']>1]\n",
    "                        .groupby(['department_id','date'], as_index=False)\n",
    "                        .agg(target_mean_gt1_day=('item_target_day','mean')))\n",
    "\n",
    "        daily_prod = (pa.groupby(['department_id','date'], as_index=False)\n",
    "                      .agg(prod_sum_day=('prod_total_model','sum')))\n",
    "\n",
    "        # Merge daily pieces\n",
    "        daily_all = (daily_gt1.merge(daily_target, on=['department_id','date'], how='outer')\n",
    "                               .merge(daily_prod, on=['department_id','date'], how='outer'))\n",
    "        daily_all['month'] = to_month_start(daily_all['date'])\n",
    "\n",
    "        # Monthly metrics\n",
    "        cap_prod_month = (daily_all.groupby(['department_id','month'], as_index=False)\n",
    "                          .agg(agents_gt1_month_mean = ('agents_gt1_day','mean'),\n",
    "                               target_gt1_month_mean = ('target_mean_gt1_day','mean'),\n",
    "                               productivity_agents   = ('prod_sum_day','sum')))\n",
    "\n",
    "        cap_prod_month['capacity_agents'] = cap_prod_month['agents_gt1_month_mean'] * cap_prod_month['target_gt1_month_mean']\n",
    "        cap_prod_month = cap_prod_month[['department_id','month','capacity_agents','productivity_agents']]\n",
    "else:\n",
    "    print('WARNING: productivity_agents.xlsx not found → capacity/productivity from agents will be empty.')\n",
    "    cap_prod_month = pd.DataFrame(columns=['department_id','month','capacity_agents','productivity_agents'])\n",
    "\n",
    "print('Agents-based capacity/productivity rows:', len(cap_prod_month))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643ecc85",
   "metadata": {},
   "source": [
    "## 10) Capacity assembly (hist + projection) and injection of agents KPIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4084d251",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build monthly_capacity_all (hist + projection for HORIZON_MONTHS)\n",
    "if 'monthly_capacity_hist' not in globals() or monthly_capacity_hist is None:\n",
    "    monthly_capacity_hist = pd.DataFrame(columns=['department_id','month','capacity'])\n",
    "\n",
    "if not monthly_capacity_hist.empty:\n",
    "    monthly_capacity_hist = monthly_capacity_hist.copy()\n",
    "    monthly_capacity_hist['department_id'] = pd.to_numeric(monthly_capacity_hist['department_id'], errors='coerce')\n",
    "    monthly_capacity_hist['month'] = pd.to_datetime(monthly_capacity_hist['month'], errors='coerce').dt.to_period('M').dt.to_timestamp(how='start')\n",
    "    monthly_capacity_hist['capacity'] = pd.to_numeric(monthly_capacity_hist['capacity'], errors='coerce')\n",
    "\n",
    "last_hist_candidates = []\n",
    "if not monthly_actuals.empty:\n",
    "    last_hist_candidates.append(pd.to_datetime(monthly_actuals['month'], errors='coerce').max())\n",
    "if not monthly_capacity_hist.empty:\n",
    "    last_hist_candidates.append(monthly_capacity_hist['month'].max())\n",
    "\n",
    "if len(last_hist_candidates)>0 and pd.notna(max(last_hist_candidates)):\n",
    "    last_hist_month = max(last_hist_candidates)\n",
    "else:\n",
    "    last_hist_month = pd.Timestamp.today().to_period('M').to_timestamp(how='start')\n",
    "\n",
    "first_future_month = last_hist_month + pd.offsets.MonthBegin(1)\n",
    "future_months = pd.date_range(first_future_month, periods=HORIZON_MONTHS, freq='MS')\n",
    "\n",
    "proj_rows = []\n",
    "if not monthly_capacity_hist.empty and len(future_months)>0:\n",
    "    for dpt_id, g in monthly_capacity_hist.groupby('department_id'):\n",
    "        last3_mean = pd.to_numeric(g.sort_values('month').tail(3)['capacity'], errors='coerce').mean()\n",
    "        for m in future_months:\n",
    "            proj_rows.append({'department_id': dpt_id, 'month': m, 'capacity': last3_mean})\n",
    "cap_proj = pd.DataFrame(proj_rows, columns=['department_id','month','capacity'])\n",
    "monthly_capacity_all = pd.concat([monthly_capacity_hist, cap_proj], ignore_index=True)\n",
    "\n",
    "# Inject agents KPIs to override capacity/productivity for boards\n",
    "# Merge cap_prod_month to monthly_adj keys (department_id + month)\n",
    "cap_prod_month['month'] = pd.to_datetime(cap_prod_month['month'], errors='coerce').dt.to_period('M').dt.to_timestamp(how='start')\n",
    "\n",
    "# We'll assemble long_dept from calibrated forecast and then attach agents capacity/productivity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645d7a0c",
   "metadata": {},
   "source": [
    "## 11) Build long_dept (calibrated forecast + actuals + agents KPIs + inventory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdb9e5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_dept rows: 208\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use calibrated forecast for boards\n",
    "fc_board = monthly_adj.rename(columns={'forecast_monthly_dept_post_einstein_cal': 'forecast_adj_cal'})\n",
    "fc_board['forecast_adj_cal'] = monthly_adj['forecast_monthly_dept_post_einstein_cal']\n",
    "\n",
    "# Base long_dept (bring actuals)\n",
    "long_dept = (fc_board[['vertical','department_id','department_name','month','forecast_adj_cal']]\n",
    "             .merge(monthly_actuals[['vertical','department_id','department_name','month','actual_volume']],\n",
    "                    on=['vertical','department_id','department_name','month'], how='left'))\n",
    "\n",
    "# Attach inventory\n",
    "long_dept = long_dept.merge(monthly_inventory, on=['department_id','month'], how='left')\n",
    "\n",
    "# Attach capacity (from agents) and productivity (from agents)\n",
    "long_dept = long_dept.merge(cap_prod_month, on=['department_id','month'], how='left')\n",
    "\n",
    "# Fallback: if no agents data, try historical capacity\n",
    "long_dept = long_dept.merge(monthly_capacity_all, on=['department_id','month'], how='left', suffixes=('', '_hist'))\n",
    "\n",
    "# Decide final capacity/productivity columns for the boards\n",
    "# - Capacity: prefer agents-based capacity when available; else use historical\n",
    "long_dept['capacity'] = np.where(long_dept['capacity_agents'].notna(), long_dept['capacity_agents'], long_dept['capacity'])\n",
    "\n",
    "# - Productivity: prefer agents-based productivity when available; else (if nothing) fallback to capacity\n",
    "long_dept['productivity'] = np.where(long_dept['productivity_agents'].notna(), long_dept['productivity_agents'], long_dept['capacity'])\n",
    "\n",
    "# Final forecast value\n",
    "long_dept['forecast'] = long_dept['forecast_adj_cal']\n",
    "\n",
    "# Clean numeric types\n",
    "for c in ['forecast','actual_volume','capacity','productivity','inventory_mean']:\n",
    "    if c in long_dept.columns:\n",
    "        long_dept[c] = pd.to_numeric(long_dept[c], errors='coerce')\n",
    "\n",
    "print('long_dept rows:', len(long_dept))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b6c861",
   "metadata": {},
   "source": [
    "## 12) Build Board_[department]_2627 with agents-based KPIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7701ea66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_month = pd.Timestamp('2026-01-01')\n",
    "end_month   = pd.Timestamp('2027-02-01')\n",
    "month_range = pd.date_range(start_month, end_month, freq='MS')\n",
    "month_map = {1:'J',2:'F',3:'M',4:'A',5:'M',6:'J',7:'J',8:'A',9:'S',10:'O',11:'N',12:'D'}\n",
    "labels = [month_map[m.month] + '-' + str(m.year % 100).zfill(2) for m in month_range]\n",
    "\n",
    "# Alignment helper\n",
    "\n",
    "def to_row_range(values: pd.Series, months: pd.Series, label_list: list) -> pd.Series:\n",
    "    lab = months.dt.month.map(month_map) + '-' + (months.dt.year % 100).astype(str).str.zfill(2)\n",
    "    s = pd.Series(values.to_numpy(), index=lab.to_numpy())\n",
    "    return s.reindex(label_list)\n",
    "\n",
    "# Build one board matrix\n",
    "\n",
    "def build_dept_matrix_range(long_dept_in: pd.DataFrame, department_name: str) -> pd.DataFrame:\n",
    "    base = long_dept_in.copy()\n",
    "    base = base[(base['department_name'] == department_name) & (base['month'] >= start_month) & (base['month'] <= end_month)]\n",
    "\n",
    "    agg_cols = {'forecast':'sum','actual_volume':'sum','capacity':'sum','productivity':'sum','inventory_mean':'mean'}\n",
    "    base = (base.groupby('month', as_index=False)\n",
    "                 .agg(**{k: pd.NamedAgg(column=k, aggfunc=v) for k,v in agg_cols.items()}))\n",
    "    base = base.set_index('month').reindex(month_range).reset_index().rename(columns={'index':'month'})\n",
    "\n",
    "    for c in ['forecast','actual_volume','capacity','productivity','inventory_mean']:\n",
    "        base[c] = pd.to_numeric(base[c], errors='coerce')\n",
    "\n",
    "    row_forecast = to_row_range(base['forecast'].round(0),      base['month'], labels)\n",
    "    row_actual   = to_row_range(base['actual_volume'].round(0), base['month'], labels)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        acc_vals = 100.0 - ((base['forecast'] - base['actual_volume']).abs() / base['actual_volume'] * 100.0)\n",
    "    row_acc = to_row_range(acc_vals.round(1), base['month'], labels)\n",
    "\n",
    "    row_capacity = to_row_range(base['capacity'],     base['month'], labels)\n",
    "    row_prod     = to_row_range(base['productivity'], base['month'], labels)\n",
    "\n",
    "    # % difference capacity vs productivity (positive if capacity>productivity)\n",
    "    row_capacity_num = pd.to_numeric(row_capacity, errors='coerce')\n",
    "    row_prod_num     = pd.to_numeric(row_prod, errors='coerce')\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        diff_cap_prod = ((row_capacity_num - row_prod_num) / row_capacity_num * 100.0)\n",
    "    row_diff_cap_prod = diff_cap_prod.round(0).fillna(0).astype(int).astype(str) + '%'\n",
    "\n",
    "    row_exp_vs_cap = to_row_range((base['forecast'] - base['capacity']).round(0), base['month'], labels)\n",
    "\n",
    "    # As requested earlier: % diff between Forecast and Actual Volume\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        avp_vals = ((base['forecast'] - base['actual_volume']) / base['actual_volume'] * 100.0)\n",
    "    row_act_vs_prod = to_row_range(avp_vals.round(1), base['month'], labels)\n",
    "\n",
    "    row_inventory = to_row_range(base['inventory_mean'].round(2), base['month'], labels)\n",
    "    row_comments  = pd.Series(['']*len(labels), index=labels)\n",
    "\n",
    "    mat = pd.DataFrame({\n",
    "        'Comments': row_comments,\n",
    "        'Forecast': row_forecast,\n",
    "        'Actual Volume': row_actual,\n",
    "        'Forecast Accuracy': row_acc,\n",
    "        'Capacity': row_capacity,\n",
    "        'Productivity': row_prod,\n",
    "        'Difference Capacity vs Productivity': row_diff_cap_prod,\n",
    "        'Expected Forecast vs Capacity': row_exp_vs_cap,\n",
    "        'Actual Volume vs Productivity': row_act_vs_prod,\n",
    "        'Inventory': row_inventory\n",
    "    }).T\n",
    "    return mat\n",
    "\n",
    "# Build & Export boards + long sheet later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570b3488",
   "metadata": {},
   "source": [
    "## 13) Build consolidated sheet `capacity_forecast` (no Comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15426b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capacity_forecast rows: 2016\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build a long-format dataset by stacking each department's matrix rows (excluding Comments)\n",
    "\n",
    "def flatten_board_to_long(dept_name: str, mat: pd.DataFrame, labels: list) -> pd.DataFrame:\n",
    "    # mat is row-indexed by KPI names, columns are month labels\n",
    "    mat2 = mat.loc[[k for k in mat.index if k != 'Comments']].copy()\n",
    "    records = []\n",
    "    for kpi in mat2.index:\n",
    "        for m in labels:\n",
    "            val = mat2.loc[kpi, m] if m in mat2.columns else np.nan\n",
    "            records.append({'Month': m, 'department_name': dept_name, 'KPI': kpi, 'Total': val})\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "capacity_forecast_long = []\n",
    "for dname in sorted(long_dept['department_name'].dropna().unique().tolist()):\n",
    "    mat = build_dept_matrix_range(long_dept, dname)\n",
    "    df_long = flatten_board_to_long(dname, mat, labels)\n",
    "    capacity_forecast_long.append(df_long)\n",
    "\n",
    "capacity_forecast = pd.concat(capacity_forecast_long, ignore_index=True) if capacity_forecast_long else pd.DataFrame(columns=['Month','department_name','KPI','Total'])\n",
    "print('capacity_forecast rows:', len(capacity_forecast))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666bdb0d",
   "metadata": {},
   "source": [
    "## 14) Export to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d906dc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\ '\n",
      "C:\\Users\\pt3canro\\AppData\\Local\\Temp\\ipykernel_42256\\4216527132.py:4: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  - remove invalid characters: : \\ / ? * [ ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export complete → C:\\Users\\pt3canro\\Desktop\\CAPACITY\\outputs\\capacity_forecast_v17_3.xlsx\n"
     ]
    }
   ],
   "source": [
    "def sanitize_sheet_name(name: str, suffix: str = \"\", max_len: int = 31) -> str:\n",
    "    \"\"\"\n",
    "    Make a string safe for Excel sheet names:\n",
    "      - remove invalid characters: : \\ / ? * [ ]\n",
    "      - replace control/unprintable chars with '_'\n",
    "      - trim to 31 char max (Excel limit)\n",
    "      - avoid empty name by defaulting to 'Sheet'\n",
    "    Optionally append a suffix (e.g., '_2627') and trim again to fit.\n",
    "    \"\"\"\n",
    "    if name is None:\n",
    "        name = \"Sheet\"\n",
    "\n",
    "    invalid_chars = set(':\\\\/?*[]')\n",
    "    cleaned = []\n",
    "    for ch in str(name):\n",
    "        if ch in invalid_chars:\n",
    "            cleaned.append('_')\n",
    "        else:\n",
    "            # Replace control chars and non-printables\n",
    "            cleaned.append(ch if ch.isprintable() else '_')\n",
    "    base = \"\".join(cleaned).strip() or \"Sheet\"\n",
    "\n",
    "    # Append suffix first, then trim\n",
    "    candidate = f\"{base}{suffix}\" if suffix else base\n",
    "    if len(candidate) > max_len:\n",
    "        candidate = candidate[:max_len]\n",
    "    return candidate\n",
    "\n",
    "# Compute file existence once\n",
    "excel_path = Path(OUTPUT_XLSX)\n",
    "file_exists = excel_path.exists()\n",
    "\n",
    "# Build writer kwargs correctly depending on append vs create\n",
    "writer_kwargs = dict(engine='openpyxl')\n",
    "if file_exists:\n",
    "    writer_kwargs.update(mode='a', if_sheet_exists='replace')\n",
    "else:\n",
    "    writer_kwargs.update(mode='w')  # do NOT include if_sheet_exists when creating\n",
    "\n",
    "with pd.ExcelWriter(OUTPUT_XLSX, **writer_kwargs) as writer:\n",
    "    # --- Base sheets (only when creating the file) ---\n",
    "    if not file_exists:\n",
    "        if 'monthly_actuals' in globals() and isinstance(monthly_actuals, pd.DataFrame) and not monthly_actuals.empty:\n",
    "            monthly_actuals.to_excel(writer, sheet_name='Monthly_Actuals', index=False)\n",
    "\n",
    "        if 'monthly_fc_raw' in globals() and isinstance(monthly_fc_raw, pd.DataFrame) and not monthly_fc_raw.empty:\n",
    "            monthly_fc_raw.to_excel(writer, sheet_name='Monthly_Forecast_RAW', index=False)\n",
    "\n",
    "        if 'monthly_adj' in globals() and isinstance(monthly_adj, pd.DataFrame) and not monthly_adj.empty:\n",
    "            monthly_adj.to_excel(writer, sheet_name='Monthly_Forecast_CAL', index=False)\n",
    "\n",
    "        if 'monthly_capacity_hist' in globals() and isinstance(monthly_capacity_hist, pd.DataFrame) and not monthly_capacity_hist.empty:\n",
    "            monthly_capacity_hist.to_excel(writer, sheet_name='Monthly_Capacity_Hist', index=False)\n",
    "\n",
    "        if 'monthly_capacity_all' in globals() and isinstance(monthly_capacity_all, pd.DataFrame) and not monthly_capacity_all.empty:\n",
    "            monthly_capacity_all.to_excel(writer, sheet_name='Monthly_Capacity_All', index=False)\n",
    "        else:\n",
    "            # Write an empty schema if not available\n",
    "            pd.DataFrame(columns=['department_id','month','capacity']).to_excel(\n",
    "                writer, sheet_name='Monthly_Capacity_All', index=False\n",
    "            )\n",
    "\n",
    "        if 'monthly_inventory' in globals() and isinstance(monthly_inventory, pd.DataFrame) and not monthly_inventory.empty:\n",
    "            monthly_inventory.to_excel(writer, sheet_name='Monthly_Inventory', index=False)\n",
    "\n",
    "    # --- Boards per department ---\n",
    "    if 'long_dept' in globals() and isinstance(long_dept, pd.DataFrame) and not long_dept.empty:\n",
    "        dept_names = (\n",
    "            long_dept['department_name']\n",
    "            .dropna()\n",
    "            .astype(str)\n",
    "            .unique()\n",
    "            .tolist()\n",
    "    )\n",
    "    for dname in sorted(dept_names):\n",
    "        # Construye la matriz\n",
    "        mat = build_dept_matrix_range(long_dept, dname)\n",
    "\n",
    "        # === ARREGLO 1: asegurar etiquetas de fila (índice) ===\n",
    "        # Si el DataFrame no tiene índice significativo, ponle nombres de fila\n",
    "        # Si ya trae índice correcto, esto no hace daño.\n",
    "        if mat.index.name is None:\n",
    "            mat.index.name = 'Fila'         # cabecera de la primera columna en Excel\n",
    "\n",
    "        # Si deseas forzar un conjunto concreto de nombres de fila, descomenta y ajusta:\n",
    "        # mat.index = [\n",
    "        #     'Tickets',            # 1ª fila\n",
    "        #     'Backlog inicial',    # 2ª\n",
    "        #     'RR (%)',             # 3ª\n",
    "        #     'Capacidad (FTE*h)',  # 4ª\n",
    "        #     'Capacidad ajustada', # 5ª\n",
    "        #     'Cumplimiento',       # 6ª\n",
    "        #     'Backlog final',      # 7ª\n",
    "        #     'Δ Backlog (%)'       # 8ª\n",
    "        # ]\n",
    "\n",
    "        # === ARREGLO 2 (ver sección B abajo): limpiar inf/-inf/NaN si hace falta ===\n",
    "        mat = (\n",
    "            mat.replace([np.inf, -np.inf], np.nan)\n",
    "               .fillna(0)  # o deja NaN si prefieres ver celdas vacías\n",
    "        )\n",
    "\n",
    "        # Sanitiza el nombre de pestaña y exporta con índice\n",
    "        sh = sanitize_sheet_name(dname, suffix=\"_2627\", max_len=31)\n",
    "        mat.to_excel(writer, sheet_name=sh, index=True)\n",
    "\n",
    "    # --- Model_Used_and_Error ---\n",
    "    if 'model_used_error_df' in globals() and isinstance(model_used_error_df, pd.DataFrame) and not model_used_error_df.empty:\n",
    "        model_used_error_df.to_excel(writer, sheet_name='Model_Used_and_Error', index=False)\n",
    "\n",
    "    # --- capacity_forecast (consolidated) ---\n",
    "    if 'capacity_forecast' in globals() and isinstance(capacity_forecast, pd.DataFrame) and not capacity_forecast.empty:\n",
    "        capacity_forecast.to_excel(writer, sheet_name='capacity_forecast', index=False)\n",
    "\n",
    "print('Export complete →', OUTPUT_XLSX)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
