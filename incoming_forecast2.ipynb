{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "accdca46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports & Setup\n",
    "\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "# Prophet import (supports both modern and legacy names)\n",
    "try:\n",
    "    from prophet import Prophet\n",
    "except Exception:\n",
    "    from fbprophet import Prophet  # fallback if older package is installed\n",
    "\n",
    "# Optional visuals (off by default)\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9c74efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>vertical</th>\n",
       "      <th>tickets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3155</th>\n",
       "      <td>2026-01-10</td>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3156</th>\n",
       "      <td>2026-01-10</td>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3157</th>\n",
       "      <td>2026-01-11</td>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3158</th>\n",
       "      <td>2026-01-11</td>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3159</th>\n",
       "      <td>2026-01-11</td>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date vertical  tickets\n",
       "3155 2026-01-10      nan        0\n",
       "3156 2026-01-10      nan        0\n",
       "3157 2026-01-11      nan        0\n",
       "3158 2026-01-11      nan        0\n",
       "3159 2026-01-11      nan        0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Cell EP-1: Load Einstein productivity and compute human-demand series\n",
    "\n",
    "# Canonical incoming path\n",
    "INCOMING_PATH = r\"C:\\Users\\pt3canro\\Desktop\\CAPACITY\\Incoming_new.xlsx\"\n",
    "\n",
    "def build_daily_from_df(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create a DAILY incoming series per vertical (tickets), filling missing days with zeros.\"\"\"\n",
    "    # Basic schema checks\n",
    "    expected = {\"Date\", \"vertical\", \"total_incoming\"}\n",
    "    missing = expected - set(df_in.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns in Incoming_new.xlsx: {missing}\")\n",
    "\n",
    "    g = (df_in\n",
    "         .assign(Date=pd.to_datetime(df_in[\"Date\"], errors=\"coerce\"))\n",
    "         .dropna(subset=[\"Date\"])\n",
    "         .groupby([\"vertical\", pd.Grouper(key=\"Date\", freq=\"D\")])[\"total_incoming\"]\n",
    "         .sum()\n",
    "         .rename(\"tickets\")\n",
    "         .reset_index())\n",
    "\n",
    "    # Fill missing days per vertical (safe, no 'vertical' clash)\n",
    "    g = (g.set_index(\"Date\")\n",
    "           .groupby(\"vertical\", group_keys=False)\n",
    "           .apply(lambda x: x.asfreq(\"D\").fillna({\"tickets\": 0}))\n",
    "           .reset_index())\n",
    "\n",
    "    # Clean types\n",
    "    g[\"tickets\"] = g[\"tickets\"].fillna(0).clip(lower=0).round().astype(int)\n",
    "    # Force vertical to string labels (avoid float/NaN issues later)\n",
    "    g[\"vertical\"] = g[\"vertical\"].astype(str).str.strip()\n",
    "    return g.sort_values([\"vertical\", \"Date\"]).reset_index(drop=True)\n",
    "\n",
    "# Rebuild 'df' if needed\n",
    "if \"df\" not in globals():\n",
    "    df = pd.read_excel(INCOMING_PATH)\n",
    "\n",
    "# Rebuild 'daily' if needed\n",
    "if \"daily\" not in globals():\n",
    "    daily = build_daily_from_df(df)\n",
    "\n",
    "# Quick sanity check\n",
    "assert {\"Date\",\"vertical\",\"tickets\"} <= set(daily.columns), \"'daily' lacks required columns\"\n",
    "display(daily.tail())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adee514f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Date'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pt3canro\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'Date'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m CALL_PATH = \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mC:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mUsers\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mpt3canro\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mDesktop\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mCAPACITY\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mcall_performance.xlsx\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m cp = pd.read_excel(CALL_PATH)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m cp[\u001b[33m\"\u001b[39m\u001b[33mDate\u001b[39m\u001b[33m\"\u001b[39m] = pd.to_datetime(\u001b[43mcp\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, errors=\u001b[33m\"\u001b[39m\u001b[33mcoerce\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m cp = cp.dropna(subset=[\u001b[33m\"\u001b[39m\u001b[33mDate\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Normalize column names\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pt3canro\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pt3canro\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'Date'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell CP-1: Load call performance and compute smoothed AHT per language\n",
    "\n",
    "CALL_PATH = r\"C:\\Users\\pt3canro\\Desktop\\CAPACITY\\call_performance.xlsx\"\n",
    "cp = pd.read_excel(CALL_PATH)\n",
    "\n",
    "cp[\"Date\"] = pd.to_datetime(cp[\"Date\"], errors=\"coerce\")\n",
    "cp = cp.dropna(subset=[\"Date\"])\n",
    "\n",
    "# Normalize column names\n",
    "rename_map = {\n",
    "    \"lang\": \"language\",\n",
    "    \"Language\": \"language\",\n",
    "    \"AHT_sec\": \"aht_seconds\",\n",
    "    \"aht\": \"aht_seconds\",\n",
    "}\n",
    "cp.rename(columns={k:v for k,v in rename_map.items() if k in cp.columns}, inplace=True)\n",
    "\n",
    "cp = cp[[\"Date\",\"language\",\"aht_seconds\"]].dropna()\n",
    "cp[\"aht_seconds\"] = cp[\"aht_seconds\"].clip(10, 3600)\n",
    "\n",
    "# Smooth AHT with 14-day rolling median\n",
    "cp = cp.sort_values([\"language\",\"Date\"])\n",
    "cp[\"aht_sm\"] = cp.groupby(\"language\")[\"aht_seconds\"]\\\n",
    "                 .transform(lambda s: s.rolling(14, min_periods=3).median())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbb77bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell CP-2: Convert call AHT to ticket AHT\n",
    "# Calls are usually shorter than tickets; use multiplier\n",
    "TICKET_OVER_CALL = 1.35\n",
    "\n",
    "call_aht = (cp.groupby(\"language\")[\"aht_sm\"]\n",
    "              .median()\n",
    "              .dropna()\n",
    "              .to_dict())\n",
    "\n",
    "ticket_aht_from_calls = {lang: float(aht) * TICKET_OVER_CALL \n",
    "                         for lang,aht in call_aht.items()}\n",
    "\n",
    "ticket_aht_from_calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee751c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell CAP-3: Integrate AHT from calls + Einstein for staffing\n",
    "\n",
    "# 1. Use daily_with_bot instead of daily\n",
    "df_staff = daily_with_bot.copy()\n",
    "\n",
    "# 2. Allocate languages as before\n",
    "# (reusing your Celda 8 logic)\n",
    "df_staff[\"tickets_total\"] = df_staff[\"tickets_human\"]\n",
    "\n",
    "alloc_rows = []\n",
    "for _, row in df_staff.iterrows():\n",
    "    alloc = allocate_by_language(int(row[\"tickets_total\"]), CONFIG[\"language_shares\"])\n",
    "    for lang, t in alloc.items():\n",
    "        alloc_rows.append({\n",
    "            \"Date\": row[\"Date\"],\n",
    "            \"vertical\": row[\"vertical\"],\n",
    "            \"language\": lang,\n",
    "            \"tickets\": int(t)\n",
    "        })\n",
    "df_lang_staff = pd.DataFrame(alloc_rows)\n",
    "\n",
    "# 3. Replace AHT by call‑based values\n",
    "df_lang_staff[\"aht_sec\"] = df_lang_staff[\"language\"].map(ticket_aht_from_calls).fillna(900)\n",
    "\n",
    "# 4. Compute agents\n",
    "WH = CONFIG[\"work_hours_effective\"]\n",
    "OCC = CONFIG[\"occupancy_target\"]\n",
    "SHR = CONFIG[\"shrinkage\"]\n",
    "\n",
    "df_lang_staff[\"agents\"] = df_lang_staff.apply(\n",
    "    lambda r: agents_needed(\n",
    "        tickets=r[\"tickets\"],\n",
    "        aht_sec=r[\"aht_sec\"],\n",
    "        work_hours_effective=WH,\n",
    "        occupancy=OCC,\n",
    "        shrinkage=SHR\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_lang_staff.head()\n",
    "``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edcf45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell DASH-1: Final daily-by-language dashboard (incoming forecast → human demand → staffing → SLA gap)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 0) Guards & parameters\n",
    "# ----------------------------\n",
    "REQUIRED_CONFIG_KEYS = [\"language_shares\", \"work_hours_effective\", \"occupancy_target\", \"shrinkage\", \"export_dir\"]\n",
    "missing_cfg = [k for k in REQUIRED_CONFIG_KEYS if k not in CONFIG]\n",
    "if missing_cfg:\n",
    "    raise RuntimeError(f\"CONFIG missing keys: {missing_cfg}. Please run your configuration cell first.\")\n",
    "\n",
    "LANG_SHARES = CONFIG[\"language_shares\"].copy()\n",
    "WORK_H = float(CONFIG[\"work_hours_effective\"])\n",
    "OCC   = float(CONFIG[\"occupancy_target\"])\n",
    "SHR   = float(CONFIG[\"shrinkage\"])\n",
    "OUTDIR = Path(CONFIG[\"export_dir\"])\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Helper: deterministic Hamilton allocation\n",
    "def allocate_by_language(total: int, shares: dict) -> dict:\n",
    "    \"\"\"Deterministic rounding (Hamilton method).\"\"\"\n",
    "    total = int(total) if pd.notna(total) else 0\n",
    "    if total <= 0:\n",
    "        return {k: 0 for k in shares}\n",
    "    ssum = float(sum(shares.values()))\n",
    "    shares = {k: (v / ssum) for k, v in shares.items()} if not (0.99 <= ssum <= 1.01) else shares\n",
    "    raw = {k: total * float(v) for k, v in shares.items()}\n",
    "    base = {k: int(np.floor(v)) for k, v in raw.items()}\n",
    "    remainder = int(total - sum(base.values()))\n",
    "    if remainder > 0:\n",
    "        fracs = sorted(((k, raw[k] - base[k]) for k in shares), key=lambda x: x[1], reverse=True)\n",
    "        for k, _ in fracs[:remainder]:\n",
    "            base[k] += 1\n",
    "    return base\n",
    "\n",
    "def agents_needed(tickets: int, aht_sec: int, work_h: float, occ: float, shrink: float) -> int:\n",
    "    \"\"\"Compute agents/day using ticket AHT, occupancy and shrinkage.\"\"\"\n",
    "    work_sec = work_h * 3600.0\n",
    "    base = (tickets * aht_sec) / (work_sec * occ) if work_sec > 0 and occ > 0 else 0\n",
    "    return int(np.ceil(base) * (1.0 + shrink))\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Build incoming forecast by language (allocate from forecast_daily if not already available)\n",
    "# ----------------------------\n",
    "if \"forecast_daily_lang\" in globals():\n",
    "    # Reuse precomputed language allocation\n",
    "    incoming_lang = (forecast_daily_lang\n",
    "                     .groupby([\"Date\",\"language\"], as_index=False)[\"tickets\"]\n",
    "                     .sum()\n",
    "                     .rename(columns={\"tickets\":\"incoming_forecast\"}))\n",
    "else:\n",
    "    if \"forecast_daily\" not in globals():\n",
    "        raise RuntimeError(\"Missing 'forecast_daily'. Please run your forecasting cells first.\")\n",
    "    # Allocate forecast per language deterministically\n",
    "    alloc_rows = []\n",
    "    tmp = forecast_daily[[\"Date\",\"vertical\",\"tickets_total\"]].copy()\n",
    "    tmp[\"tickets_total\"] = tmp[\"tickets_total\"].fillna(0).clip(lower=0).round().astype(int)\n",
    "    for _, row in tmp.iterrows():\n",
    "        alloc = allocate_by_language(int(row[\"tickets_total\"]), LANG_SHARES)\n",
    "        for lang, t in alloc.items():\n",
    "            alloc_rows.append({\"Date\": row[\"Date\"], \"language\": lang, \"incoming_forecast\": int(t)})\n",
    "    incoming_lang = (pd.DataFrame(alloc_rows)\n",
    "                     .groupby([\"Date\",\"language\"], as_index=False)[\"incoming_forecast\"].sum())\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Build Einstein baseline forecast by language\n",
    "#    - If ein_daily exists (with or without language), project to forecast horizon (dates in incoming_lang)\n",
    "#    - Otherwise, set to zero\n",
    "# ----------------------------\n",
    "# Collect forecast horizon dates\n",
    "future_dates = pd.to_datetime(incoming_lang[\"Date\"].unique())\n",
    "min_future, max_future = future_dates.min(), future_dates.max()\n",
    "\n",
    "def project_einstein_series(series: pd.Series, horizon_index: pd.DatetimeIndex) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Seasonal-naive (7-day) projection; fallback to rolling median if <7 obs; else zeros.\n",
    "    'series' must have DateTimeIndex at daily freq for a language.\n",
    "    \"\"\"\n",
    "    series = series.sort_index().asfreq(\"D\").fillna(0)\n",
    "    if len(series) >= 7:\n",
    "        last_week = series.iloc[-7:].values\n",
    "        reps = int(np.ceil(len(horizon_index)/7))\n",
    "        fc = np.tile(last_week, reps)[:len(horizon_index)]\n",
    "        return pd.Series(fc, index=horizon_index)\n",
    "    elif len(series) >= 3:\n",
    "        med = float(series.rolling(7, min_periods=3).median().iloc[-1])\n",
    "        return pd.Series(np.full(len(horizon_index), med), index=horizon_index)\n",
    "    else:\n",
    "        return pd.Series(np.zeros(len(horizon_index)), index=horizon_index)\n",
    "\n",
    "# Build einstein resolved forecast by language\n",
    "if \"ein_daily\" in globals():\n",
    "    # Try to see if ein_daily has language information\n",
    "    ein_df = ein_daily.copy()\n",
    "    if \"language\" in ein_df.columns:\n",
    "        # aggregate per Date x language\n",
    "        ein_hist = (ein_df.groupby([\"Date\",\"language\"]).size()\n",
    "                         .rename(\"einstein_resolved\")\n",
    "                         .reset_index())\n",
    "    else:\n",
    "        # no language -> aggregate total per day and allocate by language shares\n",
    "        ein_hist = (ein_df.groupby(\"Date\").size()\n",
    "                         .rename(\"einstein_resolved\")\n",
    "                         .reset_index())\n",
    "        alloc_rows_e = []\n",
    "        for _, row in ein_hist.iterrows():\n",
    "            alloc = allocate_by_language(int(row[\"einstein_resolved\"]), LANG_SHARES)\n",
    "            for lang, t in alloc.items():\n",
    "                alloc_rows_e.append({\"Date\": row[\"Date\"], \"language\": lang, \"einstein_resolved\": int(t)})\n",
    "        ein_hist = pd.DataFrame(alloc_rows_e)\n",
    "\n",
    "    ein_hist[\"Date\"] = pd.to_datetime(ein_hist[\"Date\"])\n",
    "    # Project per language for the required horizon\n",
    "    ein_fc_rows = []\n",
    "    for lang in incoming_lang[\"language\"].unique():\n",
    "        hist_lang = (ein_hist[ein_hist[\"language\"] == lang]\n",
    "                        .set_index(\"Date\")[\"einstein_resolved\"])\n",
    "        fc = project_einstein_series(hist_lang, pd.date_range(min_future, max_future, freq=\"D\"))\n",
    "        ein_fc_rows.append(pd.DataFrame({\"Date\": fc.index, \"language\": lang, \"einstein_resolved_forecast\": fc.values}))\n",
    "    ein_fc = pd.concat(ein_fc_rows, ignore_index=True)\n",
    "else:\n",
    "    # No Einstein data provided -> assume zero for all horizon\n",
    "    ein_fc = incoming_lang.copy()\n",
    "    ein_fc[\"einstein_resolved_forecast\"] = 0.0\n",
    "    ein_fc = ein_fc[[\"Date\",\"language\",\"einstein_resolved_forecast\"]]\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Merge incoming forecast & Einstein forecast → human tickets\n",
    "# ----------------------------\n",
    "dash = (incoming_lang\n",
    "        .merge(ein_fc, on=[\"Date\",\"language\"], how=\"left\"))\n",
    "dash[\"einstein_resolved_forecast\"] = dash[\"einstein_resolved_forecast\"].fillna(0).clip(lower=0)\n",
    "dash[\"tickets_human\"] = (dash[\"incoming_forecast\"] - dash[\"einstein_resolved_forecast\"]).clip(lower=0).astype(int)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Attach AHT per language\n",
    "#    Priority: AHT_updated (from your CP-2 blend) → fallback CONFIG['aht_ticket_lang_sec'] → 900s\n",
    "# ----------------------------\n",
    "if \"AHT_updated\" in globals():\n",
    "    AHT_MAP = {k:int(v) for k,v in AHT_updated.items()}\n",
    "else:\n",
    "    AHT_MAP = {k:int(v) for k,v in CONFIG.get(\"aht_ticket_lang_sec\", {}).items()}\n",
    "\n",
    "dash[\"aht_sec\"] = dash[\"language\"].map(AHT_MAP).fillna(900).astype(int)\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Compute agents_needed per day & language\n",
    "# ----------------------------\n",
    "dash[\"agents_needed\"] = dash.apply(\n",
    "    lambda r: agents_needed(r[\"tickets_human\"], r[\"aht_sec\"], WORK_H, OCC, SHR),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 6) SLA gap: if you provide df_planned_fte with columns [Date, language, fte_planned]\n",
    "#    we compute closable tickets and the gap in agents & tickets.\n",
    "# ----------------------------\n",
    "if \"df_planned_fte\" in globals():\n",
    "    plan = df_planned_fte.copy()\n",
    "    plan[\"Date\"] = pd.to_datetime(plan[\"Date\"], errors=\"coerce\")\n",
    "    plan = plan.dropna(subset=[\"Date\",\"language\",\"fte_planned\"])\n",
    "    dash = dash.merge(plan[[\"Date\",\"language\",\"fte_planned\"]], on=[\"Date\",\"language\"], how=\"left\")\n",
    "    dash[\"fte_planned\"] = dash[\"fte_planned\"].fillna(0.0)\n",
    "\n",
    "    # Closable tickets with planned FTE: supply = FTE * work_sec * occ / AHT * (1 - shrink)\n",
    "    work_sec = WORK_H * 3600.0\n",
    "    dash[\"closable_tickets\"] = ((dash[\"fte_planned\"] * work_sec * OCC) / dash[\"aht_sec\"]) * (1.0 - SHR)\n",
    "    dash[\"closable_tickets\"] = dash[\"closable_tickets\"].fillna(0).round().astype(int)\n",
    "\n",
    "    # Gaps\n",
    "    dash[\"gap_tickets\"] = (dash[\"tickets_human\"] - dash[\"closable_tickets\"]).clip(lower=0).astype(int)\n",
    "    dash[\"gap_agents\"]  = (dash[\"agents_needed\"] - dash[\"fte_planned\"]).clip(lower=0).round(0).astype(int)\n",
    "else:\n",
    "    dash[\"fte_planned\"] = np.nan\n",
    "    dash[\"closable_tickets\"] = np.nan\n",
    "    dash[\"gap_tickets\"] = np.nan\n",
    "    dash[\"gap_agents\"] = np.nan\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Nice ordering and export\n",
    "# ----------------------------\n",
    "dash = dash.sort_values([\"Date\",\"language\"]).reset_index(drop=True)\n",
    "cols = [\"Date\",\"language\",\"incoming_forecast\",\"einstein_resolved_forecast\",\"tickets_human\",\n",
    "        \"aht_sec\",\"agents_needed\",\"fte_planned\",\"closable_tickets\",\"gap_tickets\",\"gap_agents\"]\n",
    "dash = dash[cols]\n",
    "\n",
    "# Export CSV\n",
    "out_csv = OUTDIR / \"dashboard_daily_language.csv\"\n",
    "dash.to_csv(out_csv, index=False)\n",
    "print(\"Saved dashboard:\", out_csv)\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Optional quick plots (stacked human tickets by language + total agents)\n",
    "# ----------------------------\n",
    "try:\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objects as go\n",
    "\n",
    "    # Stacked bars: human tickets by language\n",
    "    fig1 = px.area(dash, x=\"Date\", y=\"tickets_human\", color=\"language\",\n",
    "                   title=\"Human tickets per day by language (stacked)\")\n",
    "    fig1.show()\n",
    "\n",
    "    # Total agents per day (sum across languages)\n",
    "    agents_daily = dash.groupby(\"Date\")[\"agents_needed\"].sum().reset_index()\n",
    "    fig2 = go.Figure()\n",
    "    fig2.add_trace(go.Scatter(x=agents_daily[\"Date\"], y=agents_daily[\"agents_needed\"],\n",
    "                              mode=\"lines+markers\", name=\"Agents needed (total)\"))\n",
    "    fig2.update_layout(title=\"Total agents needed per day\",\n",
    "                       xaxis_title=\"Date\", yaxis_title=\"Agents\")\n",
    "    fig2.show()\n",
    "except Exception as e:\n",
    "    print(\"Plotly not available or rendering blocked. Skipping plots. Reason:\", e)\n",
    "\n",
    "# Display head\n",
    "dash.head(20)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
