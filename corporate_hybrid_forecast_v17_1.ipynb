{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55b06fc9",
   "metadata": {},
   "source": [
    "# corporate_hybrid_forecast_v17_1\n",
    "\n",
    "This notebook replaces the baseline forecaster per department using the **recommended model** mapping.\n",
    "\n",
    "Pipeline: daily forecast → monthly (sum) → Einstein deduction → calibration → capacity projection → boards.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd90e653",
   "metadata": {},
   "source": [
    "## 1) Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93026fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT_XLSX → C:\\Users\\pt3canro\\Desktop\\CAPACITY\\outputs\\capacity_forecast_v17_1.xlsx\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "BASE_DIR = r'C:\\Users\\pt3canro\\Desktop\\CAPACITY'\n",
    "BASE_DIR = str(Path(BASE_DIR).expanduser().resolve())\n",
    "\n",
    "INPUT_DIR  = str(Path(BASE_DIR) / 'input_model')\n",
    "OUTPUT_DIR = str(Path(BASE_DIR) / 'outputs')\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "INCOMING_SOURCE_PATH = os.path.join(INPUT_DIR, 'Incoming_new.xlsx')\n",
    "INCOMING_SHEET       = 'Main'\n",
    "DEPT_MAP_PATH  = os.path.join(INPUT_DIR, 'department.xlsx')\n",
    "DEPT_MAP_SHEET = 'map'\n",
    "AGENT_CAPACITY_PATH = os.path.join(INPUT_DIR, 'agent_language_n_target.xlsx')\n",
    "EINSTEIN_PATH       = os.path.join(INPUT_DIR, 'einstein.xlsx')\n",
    "INVENTORY_PATH      = os.path.join(INPUT_DIR, 'inventory_month.xlsx')\n",
    "\n",
    "OUTPUT_XLSX = os.path.join(OUTPUT_DIR, 'capacity_forecast_v17_1.xlsx')\n",
    "\n",
    "HORIZON_MONTHS = 12\n",
    "VERTICALS_TARGET = ['Payments','Partners','Hospitality']\n",
    "TARGET_TPH = 6.0\n",
    "EXCLUDE_DEPARTMENT_NAME_TOKENS = ['PROJ','DIST','KEY','PROXIMIS']\n",
    "\n",
    "print('OUTPUT_XLSX →', OUTPUT_XLSX)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b272c4",
   "metadata": {},
   "source": [
    "## 2) Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1ba8ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from calendar import monthrange\n",
    "\n",
    "def pick_col(df, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    lower = {c.lower(): c for c in df.columns}\n",
    "    for c in candidates:\n",
    "        if c.lower() in lower:\n",
    "            return lower[c.lower()]\n",
    "    return None\n",
    "\n",
    "def std_cols(df):\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def to_month_start(dt_series):\n",
    "    s = pd.to_datetime(dt_series, errors='coerce')\n",
    "    return s.dt.to_period('M').dt.to_timestamp(how='start')\n",
    "\n",
    "def working_days_in_month(year:int, month:int):\n",
    "    start = datetime(year, month, 1)\n",
    "    end = datetime(year+1,1,1)-timedelta(days=1) if month==12 else datetime(year,month+1,1)-timedelta(days=1)\n",
    "    days = pd.date_range(start, end, freq='D')\n",
    "    return int(np.sum(days.dayofweek < 5))\n",
    "\n",
    "def validate_quantiles(dfm):\n",
    "    viol = dfm[(dfm['forecast_p05_dept']>dfm['forecast_monthly_dept']) | (dfm['forecast_monthly_dept']>dfm['forecast_p95_dept'])]\n",
    "    if not viol.empty:\n",
    "        raise ValueError('Quantile order violation in monthly aggregation.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8699c853",
   "metadata": {},
   "source": [
    "## 3) Forecast engines (Baseline + SARIMAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8856fdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_daily_baseline(y: pd.Series, horizon_days: int = 365):\n",
    "    y = y.asfreq('D').fillna(0)\n",
    "    if len(y) < 56:\n",
    "        idx = pd.date_range(y.index[-1] + pd.Timedelta(days=1), periods=horizon_days, freq='D')\n",
    "        last_week = y[-7:].to_numpy()\n",
    "        p50 = np.resize(last_week, horizon_days)\n",
    "        resid = (y[7:] - y.shift(7)[7:]).dropna()\n",
    "        std = resid.std() if len(resid)>0 else max(1.0, np.sqrt(max(y.mean(),1)))\n",
    "        p05 = np.clip(p50 - 1.645*std, 0, None)\n",
    "        p95 = p50 + 1.645*std\n",
    "        return pd.DataFrame({'date': idx, 'p50': p50, 'p05': p05, 'p95': p95})\n",
    "    try:\n",
    "        from statsmodels.tsa.seasonal import STL\n",
    "        y_box = np.log1p(y)\n",
    "        stl = STL(y_box, period=7, robust=True)\n",
    "        res = stl.fit()\n",
    "        trend, seas, resid = res.trend, res.seasonal, res.resid\n",
    "        last_trend = trend.iloc[-1]\n",
    "        std = resid.std() if resid.std()>0 else 0.5\n",
    "        idx = pd.date_range(y.index[-1] + pd.Timedelta(days=1), periods=horizon_days, freq='D')\n",
    "        seas_fut = np.resize(seas[-7:].to_numpy(), horizon_days)\n",
    "        mu_log = last_trend + seas_fut\n",
    "        p50 = np.expm1(mu_log); p50 = np.clip(p50, 0, None)\n",
    "        p05 = np.expm1(mu_log - 1.645*std); p05 = np.clip(p05, 0, None)\n",
    "        p95 = np.expm1(mu_log + 1.645*std)\n",
    "        return pd.DataFrame({'date': idx, 'p50': p50, 'p05': p05, 'p95': p95})\n",
    "    except Exception:\n",
    "        idx = pd.date_range(y.index[-1] + pd.Timedelta(days=1), periods=horizon_days, freq='D')\n",
    "        last_week = y[-7:].to_numpy()\n",
    "        p50 = np.resize(last_week, horizon_days)\n",
    "        resid = (y[7:] - y.shift(7)[7:]).dropna()\n",
    "        std = resid.std() if len(resid)>0 else max(1.0, np.sqrt(max(y.mean(),1)))\n",
    "        p05 = np.clip(p50 - 1.645*std, 0, None)\n",
    "        p95 = p50 + 1.645*std\n",
    "        return pd.DataFrame({'date': idx, 'p50': p50, 'p05': p05, 'p95': p95})\n",
    "\n",
    "\n",
    "def forecast_daily_sarimax(y: pd.Series, horizon_days: int = 365):\n",
    "    try:\n",
    "        from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "    except Exception:\n",
    "        return None\n",
    "    y = y.asfreq('D').fillna(0)\n",
    "    y_log = np.log1p(y)\n",
    "    try:\n",
    "        model = SARIMAX(y_log, order=(1,0,1), seasonal_order=(1,0,1,7), enforce_stationarity=False, enforce_invertibility=False)\n",
    "        res = model.fit(disp=False)\n",
    "        pred = res.get_forecast(steps=horizon_days)\n",
    "        mean = np.expm1(pred.predicted_mean).clip(lower=0)\n",
    "        resid = (y[7:] - y.shift(7)[7:]).dropna()\n",
    "        std = resid.std() if len(resid)>0 else 1.0\n",
    "        p05 = np.clip(mean - 1.645*std, 0, None)\n",
    "        p95 = mean + 1.645*std\n",
    "        idx = pd.date_range(y.index[-1] + pd.Timedelta(days=1), periods=horizon_days, freq='D')\n",
    "        return pd.DataFrame({'date': idx, 'p50': mean.values, 'p05': p05.values, 'p95': p95.values})\n",
    "    except Exception:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa80668",
   "metadata": {},
   "source": [
    "## 4) Load inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "964d3cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "incoming_raw = pd.read_excel(INCOMING_SOURCE_PATH, sheet_name=INCOMING_SHEET, engine='openpyxl')\n",
    "std_cols(incoming_raw)\n",
    "\n",
    "c_date = pick_col(incoming_raw, ['Date','date'])\n",
    "c_dept = pick_col(incoming_raw, ['department_id','dept_id','Department_ID'])\n",
    "c_cnt  = pick_col(incoming_raw, ['ticket_total','Ticket_Total','count','tickets','qty'])\n",
    "if c_date is None or c_dept is None:\n",
    "    raise KeyError('Incoming_new.xlsx must include Date and department_id columns.')\n",
    "\n",
    "incoming = incoming_raw[[c_date, c_dept] + ([c_cnt] if c_cnt else [])].copy()\n",
    "incoming.columns = ['date','department_id'] + (['ticket_total'] if c_cnt else [])\n",
    "if 'ticket_total' not in incoming.columns:\n",
    "    incoming['ticket_total'] = 1\n",
    "\n",
    "incoming['date']  = pd.to_datetime(incoming['date'], errors='coerce')\n",
    "incoming['month'] = to_month_start(incoming['date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13ff575b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded incoming rows = 458599\n"
     ]
    }
   ],
   "source": [
    "dept_map = pd.read_excel(DEPT_MAP_PATH, sheet_name=DEPT_MAP_SHEET, engine='openpyxl')\n",
    "std_cols(dept_map)\n",
    "dm_id   = pick_col(dept_map, ['department_id','dept_id','Department_ID'])\n",
    "dm_name = pick_col(dept_map, ['department_name','Department','dept_name','Department_Name'])\n",
    "dm_vert = pick_col(dept_map, ['vertical','Vertical'])\n",
    "if dm_id is None or dm_name is None or dm_vert is None:\n",
    "    raise KeyError('department.xlsx must contain department_id, department_name and vertical (sheet map).')\n",
    "\n",
    "dept_map = dept_map[[dm_id, dm_name, dm_vert]].drop_duplicates()\n",
    "dept_map.columns = ['department_id','department_name','vertical']\n",
    "\n",
    "incoming['department_id'] = pd.to_numeric(incoming['department_id'], errors='coerce').astype('Int64')\n",
    "dept_map['department_id'] = pd.to_numeric(dept_map['department_id'], errors='coerce').astype('Int64')\n",
    "\n",
    "incoming = incoming.merge(dept_map, on='department_id', how='left')\n",
    "\n",
    "incoming = incoming[incoming['vertical'].isin(VERTICALS_TARGET)].copy()\n",
    "mask_excl = pd.Series(False, index=incoming.index)\n",
    "for tok in EXCLUDE_DEPARTMENT_NAME_TOKENS:\n",
    "    mask_excl |= incoming['department_name'].astype(str).str.upper().str.contains(tok.upper(), na=False)\n",
    "\n",
    "incoming = incoming.loc[~mask_excl].copy()\n",
    "\n",
    "monthly_actuals = (incoming\n",
    "                   .groupby(['vertical','department_id','department_name','month'], as_index=False)\n",
    "                   .agg(actual_volume=('ticket_total','sum')))\n",
    "print('Loaded incoming rows =', len(incoming))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88c964b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "if Path(EINSTEIN_PATH).exists():\n",
    "    ein = pd.read_excel(EINSTEIN_PATH, engine='openpyxl'); std_cols(ein)\n",
    "    e_date = pick_col(ein, ['Date','date']); e_dept = pick_col(ein, ['department_id','dept_id','Department_ID'])\n",
    "    if e_date and e_dept:\n",
    "        ein_use = ein[[e_date, e_dept]].copy(); ein_use.columns=['date','department_id']\n",
    "        ein_use['date'] = pd.to_datetime(ein_use['date'], errors='coerce'); ein_use['month'] = to_month_start(ein_use['date'])\n",
    "        ein_month = ein_use.groupby(['department_id','month'], as_index=False).size().rename(columns={'size':'einstein_solved'})\n",
    "    else:\n",
    "        ein_month = pd.DataFrame(columns=['department_id','month','einstein_solved'])\n",
    "else:\n",
    "    ein_month = pd.DataFrame(columns=['department_id','month','einstein_solved'])\n",
    "\n",
    "if Path(AGENT_CAPACITY_PATH).exists():\n",
    "    cap = pd.read_excel(AGENT_CAPACITY_PATH, engine='openpyxl'); std_cols(cap)\n",
    "    c_year  = pick_col(cap, ['Year','year'])\n",
    "    c_mnum  = pick_col(cap, ['Month_number','month_number','month'])\n",
    "    c_mstart= pick_col(cap, ['MonthStartDate','monthstartdate'])\n",
    "    c_deptc = pick_col(cap, ['department_id','dept_id','Department_ID'])\n",
    "    c_prod_total = pick_col(cap, ['productivity_total','prod_total_model'])\n",
    "    c_avgpd      = pick_col(cap, ['avg_per_day'])\n",
    "    c_hours      = pick_col(cap, ['productive'])\n",
    "    if c_mstart is None and (c_year is not None and c_mnum is not None):\n",
    "        cap['MonthStartDate'] = pd.to_datetime(cap[c_year].astype(str)+'-'+cap[c_mnum].astype(str)+'-01'); c_mstart='MonthStartDate'\n",
    "    if c_mstart is None or c_deptc is None:\n",
    "        monthly_capacity_hist = pd.DataFrame(columns=['department_id','month','capacity'])\n",
    "    else:\n",
    "        cols=[c_mstart,c_deptc]; ren={c_mstart:'month', c_deptc:'department_id'}\n",
    "        if c_prod_total: cols.append(c_prod_total); ren[c_prod_total]='tickets_capacity'\n",
    "        if c_avgpd:      cols.append(c_avgpd);      ren[c_avgpd]='avg_per_day'\n",
    "        if c_hours:      cols.append(c_hours);      ren[c_hours]='productive_hours'\n",
    "        cap_use = cap[cols].rename(columns=ren)\n",
    "        cap_use['month'] = pd.to_datetime(cap_use['month'], errors='coerce').dt.to_period('M').dt.to_timestamp(how='start')\n",
    "        if 'tickets_capacity' in cap_use.columns:\n",
    "            monthly_capacity_hist = cap_use.groupby(['department_id','month'], as_index=False).agg(capacity=('tickets_capacity','sum'))\n",
    "        elif 'avg_per_day' in cap_use.columns:\n",
    "            cap_use['wdays'] = cap_use['month'].apply(lambda d: working_days_in_month(d.year, d.month))\n",
    "            cap_use['capacity'] = cap_use['avg_per_day'] * cap_use['wdays']\n",
    "            monthly_capacity_hist = cap_use.groupby(['department_id','month'], as_index=False).agg(capacity=('capacity','sum'))\n",
    "        elif 'productive_hours' in cap_use.columns:\n",
    "            cap_use['capacity'] = cap_use['productive_hours'] * 6.0\n",
    "            monthly_capacity_hist = cap_use.groupby(['department_id','month'], as_index=False).agg(capacity=('capacity','sum'))\n",
    "        else:\n",
    "            monthly_capacity_hist = pd.DataFrame(columns=['department_id','month','capacity'])\n",
    "else:\n",
    "    monthly_capacity_hist = pd.DataFrame(columns=['department_id','month','capacity'])\n",
    "\n",
    "if Path(INVENTORY_PATH).exists():\n",
    "    inv = pd.read_excel(INVENTORY_PATH, engine='openpyxl'); std_cols(inv)\n",
    "    i_date = pick_col(inv, ['Date','date']); i_dept = pick_col(inv, ['department_id','dept_id','Department_ID'])\n",
    "    if i_date and i_dept:\n",
    "        inv_use = inv[[i_date,i_dept]].copy(); inv_use.columns=['date','department_id']\n",
    "        inv_use['date']=pd.to_datetime(inv_use['date'], errors='coerce'); inv_use['month']=to_month_start(inv_use['date'])\n",
    "        inv_daily = inv_use.groupby(['department_id','date'], as_index=False).size().rename(columns={'size':'open_count'})\n",
    "        inv_daily['month']=to_month_start(inv_daily['date'])\n",
    "        monthly_inventory = inv_daily.groupby(['department_id','month'], as_index=False).agg(inventory_mean=('open_count','mean'))\n",
    "    else:\n",
    "        monthly_inventory = pd.DataFrame(columns=['department_id','month','inventory_mean'])\n",
    "else:\n",
    "    monthly_inventory = pd.DataFrame(columns=['department_id','month','inventory_mean'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a572cbc0",
   "metadata": {},
   "source": [
    "## 5) Recommended model mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f1752a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: no recommended models found. Using Baseline for all.\n"
     ]
    }
   ],
   "source": [
    "recommended = {}\n",
    "try:\n",
    "    prev = pd.read_excel(os.path.join(OUTPUT_DIR, 'capacity_forecast_v17.xlsx'), sheet_name='Remediation_Report')\n",
    "    if {'department_id','Recommended_Model'}.issubset(prev.columns):\n",
    "        for _,r in prev.iterrows():\n",
    "            if pd.notna(r['department_id']) and pd.notna(r['Recommended_Model']):\n",
    "                recommended[int(r['department_id'])] = str(r['Recommended_Model'])\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    rm_csv = os.path.join(INPUT_DIR,'recommended_models.csv')\n",
    "    if Path(rm_csv).exists():\n",
    "        dfm = pd.read_csv(rm_csv)\n",
    "        if {'department_id','model'}.issubset(dfm.columns):\n",
    "            for _,r in dfm.iterrows():\n",
    "                recommended[int(r['department_id'])] = str(r['model'])\n",
    "except Exception:\n",
    "    pass\n",
    "if not recommended:\n",
    "    print('WARNING: no recommended models found. Using Baseline for all.')\n",
    "else:\n",
    "    print('Recommended models loaded:', len(recommended))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f13fd2c",
   "metadata": {},
   "source": [
    "## 6) Daily forecast using per-department engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c68ba1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily forecast rows: 5840\n"
     ]
    }
   ],
   "source": [
    "incoming_daily = (incoming.groupby(['department_id','date'], as_index=False)\n",
    "                  .agg(tickets=('ticket_total','sum')))\n",
    "\n",
    "engine_funcs = {\n",
    "    'Baseline(STL)': forecast_daily_baseline,\n",
    "    'STL': forecast_daily_baseline,\n",
    "    'SARIMAX-7': forecast_daily_sarimax\n",
    "}\n",
    "\n",
    "daily_forecasts = []\n",
    "HORIZON_DAYS = 365\n",
    "for dpt_id, grp in incoming_daily.groupby('department_id'):\n",
    "    y = grp.set_index('date')['tickets'].sort_index().asfreq('D').fillna(0)\n",
    "    engine = recommended.get(int(dpt_id), 'Baseline(STL)')\n",
    "    func = engine_funcs.get(engine, forecast_daily_baseline)\n",
    "    try:\n",
    "        fc = func(y, horizon_days=HORIZON_DAYS)\n",
    "        if fc is None:\n",
    "            raise RuntimeError('Engine returned None')\n",
    "    except Exception:\n",
    "        fc = forecast_daily_baseline(y, horizon_days=HORIZON_DAYS)\n",
    "        engine = 'Baseline(STL)'\n",
    "    fc.insert(0, 'department_id', dpt_id)\n",
    "    fc['engine_used'] = engine\n",
    "    daily_forecasts.append(fc)\n",
    "\n",
    "fc_daily_built = pd.concat(daily_forecasts, ignore_index=True) if daily_forecasts else pd.DataFrame()\n",
    "fc_daily_built = fc_daily_built.merge(dept_map, on='department_id', how='left')\n",
    "fc_daily_built = fc_daily_built[fc_daily_built['vertical'].isin(VERTICALS_TARGET)]\n",
    "for tok in EXCLUDE_DEPARTMENT_NAME_TOKENS:\n",
    "    fc_daily_built = fc_daily_built[~fc_daily_built['department_name'].astype(str).str.upper().str.contains(tok.upper(), na=False)]\n",
    "\n",
    "fc_daily_built.rename(columns={'p50':'forecast_daily_dept','p05':'p05_daily_dept','p95':'p95_daily_dept'}, inplace=True)\n",
    "print('Daily forecast rows:', len(fc_daily_built))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7ed113",
   "metadata": {},
   "source": [
    "## 7) Monthly aggregation + Einstein + calibration + capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651d71e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pt3canro\\AppData\\Local\\Temp\\ipykernel_38408\\1793162477.py:14: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: g.set_index('month')['einstein_rate'].tail(3).mean())\n"
     ]
    }
   ],
   "source": [
    "monthly_fc_raw = (fc_daily_built.assign(month=to_month_start(fc_daily_built['date']))\n",
    "                  .groupby(['vertical','department_id','department_name','month'], as_index=False)\n",
    "                  .agg(forecast_monthly_dept=('forecast_daily_dept','sum'),\n",
    "                       forecast_p05_dept=('p05_daily_dept','sum'),\n",
    "                       forecast_p95_dept=('p95_daily_dept','sum')))\n",
    "validate_quantiles(monthly_fc_raw)\n",
    "\n",
    "if not ein_month.empty:\n",
    "    hist_rates = monthly_actuals.merge(ein_month, on=['department_id','month'], how='left')\n",
    "    hist_rates['einstein_solved'] = hist_rates['einstein_solved'].fillna(0)\n",
    "    hist_rates['einstein_rate'] = (hist_rates['einstein_solved'] / hist_rates['actual_volume']).replace([np.inf,-np.inf], 0).fillna(0)\n",
    "    recent = (hist_rates.sort_values('month')\n",
    "              .groupby('department_id')\n",
    "              .apply(lambda g: g.set_index('month')['einstein_rate'].tail(3).mean())\n",
    "              .rename('einstein_rate_recent').reset_index())\n",
    "else:\n",
    "    recent = pd.DataFrame(columns=['department_id','einstein_rate_recent'])\n",
    "\n",
    "monthly_adj = monthly_fc_raw.merge(recent, on='department_id', how='left')\n",
    "monthly_adj['einstein_rate_recent'] = monthly_adj['einstein_rate_recent'].fillna(0).clip(0, 0.9)\n",
    "for c in ['forecast_monthly_dept','forecast_p05_dept','forecast_p95_dept']:\n",
    "    monthly_adj[c + '_post_einstein'] = monthly_adj[c] * (1 - monthly_adj['einstein_rate_recent'])\n",
    "\n",
    "# no external calibration table here → identity\n",
    "monthly_adj['calib_factor'] = 1.0\n",
    "for c in ['forecast_monthly_dept_post_einstein','forecast_p05_dept_post_einstein','forecast_p95_dept_post_einstein']:\n",
    "    monthly_adj[c + '_cal'] = monthly_adj[c] * monthly_adj['calib_factor']\n",
    "\n",
    "last_hist_month = (monthly_actuals['month'].max() if not monthly_actuals.empty else pd.Timestamp.today().to_period('M').to_timestamp(how='start'))\n",
    "first_future_month = last_hist_month + pd.offsets.MonthBegin(1)\n",
    "future_months = pd.date_range(first_future_month, periods=HORIZON_MONTHS, freq='MS')\n",
    "\n",
    "dep_cap_proj = []\n",
    "if not monthly_capacity_hist.empty:\n",
    "    for dpt_id, g in monthly_capacity_hist.groupby('department_id'):\n",
    "        avg_cap = g.sort_values('month').tail(3)['capacity'].mean() if not g.empty else np.nan\n",
    "        for m in future_months:\n",
    "            dep_cap_proj.append({'department_id': dpt_id, 'month': m, 'capacity': avg_cap})\n",
    "cap_proj = pd.DataFrame(dep_cap_proj) if dep_cap_proj else pd.DataFrame(columns=['department_id','month','capacity'])\n",
    "monthly_capacity_all = pd.concat([monthly_capacity_hist, cap_proj], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea40c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# A) Apply calibration per department using bias_pct (from your table)\n",
    "#    - calib_factor = 1 - bias_pct/100, clipped to [0.70, 1.30]\n",
    "# =========================================\n",
    "\n",
    "# -- Build the Model_Used_and_Error dataframe from your provided table\n",
    "model_used_error_df = pd.DataFrame([\n",
    "    {\"vertical\":\"Hospitality\",\"department_id\":7,\"department_name\":\"CS_PMSH_L1\",\"model_used\":\"STL\",\"backtest_months\":12,\"mape_pct\":29.35,\"wape_pct\":26.00,\"bias_pct\":-0.40},\n",
    "    {\"vertical\":\"Hospitality\",\"department_id\":8,\"department_name\":\"CS_PMSP_CLOUD_L1\",\"model_used\":\"STL\",\"backtest_months\":12,\"mape_pct\":43.80,\"wape_pct\":39.12,\"bias_pct\":12.64},\n",
    "    {\"vertical\":\"Hospitality\",\"department_id\":11,\"department_name\":\"CS_PMSP_CLOUD_L2\",\"model_used\":\"STL\",\"backtest_months\":12,\"mape_pct\":27.76,\"wape_pct\":22.27,\"bias_pct\":-4.06},\n",
    "    {\"vertical\":\"Hospitality\",\"department_id\":23,\"department_name\":\"CS_PMSP_FRANCE\",\"model_used\":\"STL\",\"backtest_months\":12,\"mape_pct\":31.44,\"wape_pct\":25.79,\"bias_pct\":-1.79},\n",
    "    {\"vertical\":\"Hospitality\",\"department_id\":5,\"department_name\":\"CS_PMSP_INTEG\",\"model_used\":\"STL\",\"backtest_months\":12,\"mape_pct\":20.88,\"wape_pct\":17.68,\"bias_pct\":-2.70},\n",
    "    {\"vertical\":\"Hospitality\",\"department_id\":9,\"department_name\":\"CS_PMSP_PREM_L1\",\"model_used\":\"STL\",\"backtest_months\":12,\"mape_pct\":17.29,\"wape_pct\":14.44,\"bias_pct\":-3.07},\n",
    "    {\"vertical\":\"Hospitality\",\"department_id\":10,\"department_name\":\"CS_PMSP_PREM_L2\",\"model_used\":\"STL\",\"backtest_months\":12,\"mape_pct\":28.77,\"wape_pct\":23.23,\"bias_pct\":-1.12},\n",
    "    {\"vertical\":\"Partners\",\"department_id\":12,\"department_name\":\"CS_PART_APAC\",\"model_used\":\"STL\",\"backtest_months\":12,\"mape_pct\":20.88,\"wape_pct\":20.99,\"bias_pct\":-5.14},\n",
    "    {\"vertical\":\"Partners\",\"department_id\":13,\"department_name\":\"CS_PART_EMEA\",\"model_used\":\"STL\",\"backtest_months\":12,\"mape_pct\":27.76,\"wape_pct\":25.91,\"bias_pct\":7.36},\n",
    "    {\"vertical\":\"Partners\",\"department_id\":14,\"department_name\":\"CS_PART_LATAM\",\"model_used\":\"STL\",\"backtest_months\":12,\"mape_pct\":26.60,\"wape_pct\":21.38,\"bias_pct\":5.29},\n",
    "    {\"vertical\":\"Partners\",\"department_id\":15,\"department_name\":\"CS_PART_US\",\"model_used\":\"STL\",\"backtest_months\":12,\"mape_pct\":40.03,\"wape_pct\":30.97,\"bias_pct\":-9.47},\n",
    "    {\"vertical\":\"Payments\",\"department_id\":3,\"department_name\":\"CA_PYAC\",\"model_used\":\"STL\",\"backtest_months\":12,\"mape_pct\":18.63,\"wape_pct\":18.70,\"bias_pct\":1.28},\n",
    "    {\"vertical\":\"Payments\",\"department_id\":1,\"department_name\":\"CS_GT3C_EU\",\"model_used\":\"STL\",\"backtest_months\":12,\"mape_pct\":24.80,\"wape_pct\":22.19,\"bias_pct\":0.58},\n",
    "    {\"vertical\":\"Payments\",\"department_id\":18,\"department_name\":\"Datatrans L2 Customer Support\",\"model_used\":\"STL\",\"backtest_months\":12,\"mape_pct\":54.89,\"wape_pct\":38.55,\"bias_pct\":-7.28},\n",
    "    {\"vertical\":\"Payments\",\"department_id\":2,\"department_name\":\"L2 Customer Support\",\"model_used\":\"STL\",\"backtest_months\":12,\"mape_pct\":28.56,\"wape_pct\":24.46,\"bias_pct\":-1.36},\n",
    "    {\"vertical\":\"Payments\",\"department_id\":21,\"department_name\":\"Specialist - L2 Customer Support\",\"model_used\":\"STL\",\"backtest_months\":12,\"mape_pct\":42.25,\"wape_pct\":41.25,\"bias_pct\":17.26},\n",
    "])\n",
    "\n",
    "# -- Compute calibration factor from bias\n",
    "calib_from_bias = model_used_error_df[['department_id','bias_pct']].copy()\n",
    "calib_from_bias['department_id'] = pd.to_numeric(calib_from_bias['department_id'], errors='coerce')\n",
    "calib_from_bias['calib_factor'] = (1 - calib_from_bias['bias_pct']/100.0).clip(0.70, 1.30)\n",
    "\n",
    "# -- Merge into monthly_adj and rebuild *_cal using the factor\n",
    "monthly_adj = monthly_adj.drop(columns=['calib_factor'], errors='ignore')\n",
    "monthly_adj = monthly_adj.merge(calib_from_bias[['department_id','calib_factor']], on='department_id', how='left')\n",
    "monthly_adj['calib_factor'] = monthly_adj['calib_factor'].fillna(1.0)\n",
    "\n",
    "for c in ['forecast_monthly_dept_post_einstein','forecast_p05_dept_post_einstein','forecast_p95_dept_post_einstein']:\n",
    "    monthly_adj[c + '_cal'] = monthly_adj[c] * monthly_adj['calib_factor']\n",
    "\n",
    "print(\"Calibration applied from bias_pct. Factor coverage:\",\n",
    "      monthly_adj['calib_factor'].ne(1.0).mean().round(2), \"of rows have factor != 1.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82362bd1",
   "metadata": {},
   "source": [
    "## 8) Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "234f7015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Users\\pt3canro\\Desktop\\CAPACITY\\outputs\\capacity_forecast_v17_1.xlsx\n"
     ]
    }
   ],
   "source": [
    "with pd.ExcelWriter(OUTPUT_XLSX, engine='openpyxl') as writer:\n",
    "    monthly_actuals.to_excel(writer, sheet_name='Monthly_Actuals', index=False)\n",
    "    monthly_fc_raw.to_excel(writer, sheet_name='Monthly_Forecast_RAW', index=False)\n",
    "    monthly_adj.to_excel(writer, sheet_name='Monthly_Forecast_CAL', index=False)\n",
    "    if not monthly_capacity_hist.empty:\n",
    "        monthly_capacity_hist.to_excel(writer, sheet_name='Monthly_Capacity_Hist', index=False)\n",
    "    if 'monthly_capacity_all' in globals() and not monthly_capacity_all.empty:\n",
    "        monthly_capacity_all.to_excel(writer, sheet_name='Monthly_Capacity_All', index=False)\n",
    "    if not monthly_inventory.empty:\n",
    "        monthly_inventory.to_excel(writer, sheet_name='Monthly_Inventory', index=False)\n",
    "\n",
    "print('Saved:', OUTPUT_XLSX)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
