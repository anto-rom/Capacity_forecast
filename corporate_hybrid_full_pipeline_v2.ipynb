{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e709ee08",
   "metadata": {},
   "source": [
    "# Corporate Hybrid Forecast Notebook (Prophet + ARIMA) – v2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f323b68",
   "metadata": {},
   "source": [
    "## 01 - Imports & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22c4f3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from prophet import Prophet\n",
    "import pmdarima as pm\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from typing import Optional, List, Dict\n",
    "\n",
    "# Pandas engines\n",
    "PD_READ_XLSX_ENGINE = 'openpyxl'\n",
    "PD_WRITE_XLSX_ENGINE = 'openpyxl'\n",
    "\n",
    "# === Project paths (adapt to your local if needed) ===\n",
    "INPUT_FOLDER  = r\"C:\\Users\\pt3canro\\Desktop\\CAPACITY\\input_model\"\n",
    "OUTPUT_FOLDER = r\"C:\\Users\\pt3canro\\Desktop\\CAPACITY\\outputs\"\n",
    "\n",
    "# Source files (as in your original pipeline)\n",
    "INCOMING_PATH   = os.path.join(INPUT_FOLDER, \"Incoming_new.xlsx\")\n",
    "CALL_PATH       = os.path.join(INPUT_FOLDER, \"call_performance.xlsx\")\n",
    "AGENTS_PATH     = os.path.join(INPUT_FOLDER, \"agent_language_n_target.xlsx\")\n",
    "PROD_PATH       = os.path.join(INPUT_FOLDER, \"productivity_agents.xlsx\")\n",
    "EINSTEIN_PATH   = os.path.join(INPUT_FOLDER, \"einstein.xlsx\")\n",
    "INVENTORY_PATH  = os.path.join(INPUT_FOLDER, \"inventory_month.xlsx\")\n",
    "DEPT_PATH  = os.path.join(INPUT_FOLDER, \"department.xlsx\")   # official mapping source\n",
    "DEPT_SHEET = None\n",
    "\n",
    "# Output files\n",
    "OUT_XLSX = \"capacity_forecast_hybrid.xlsx\"\n",
    "OUT_MAPE = \"mape_by_department.xlsx\"\n",
    "\n",
    "# Forecast horizon\n",
    "HORIZON_MONTHS = 6\n",
    "\n",
    "# Language shares (fixed)\n",
    "LANG_SHARE = {\n",
    "    'English': 64.35,\n",
    "    'French' : 7.41,\n",
    "    'German' : 8.60,\n",
    "    'Italian': 6.67,\n",
    "    'Portuguese': 1.62,\n",
    "    'Spanish': 11.35,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64578b79",
   "metadata": {},
   "source": [
    "## 02 - Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31e59737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_first(df: pd.DataFrame, candidates: List[str]) -> Optional[str]:\n",
    "    \"\"\"Return the first column in df that matches any candidate (case-insensitive, normalized).\"\"\"\n",
    "    low = {c.lower(): c for c in df.columns}\n",
    "    for cand in candidates:\n",
    "        if cand.lower() in low:\n",
    "            return low[cand.lower()]\n",
    "    for c in df.columns:\n",
    "        cl = c.lower().replace(' ', '').replace('_', '')\n",
    "        for cand in candidates:\n",
    "            if cand.lower().replace(' ', '').replace('_', '') == cl:\n",
    "                return c\n",
    "    return None\n",
    "\n",
    "def mape_safe(y_true: pd.Series, y_pred: pd.Series) -> Optional[float]:\n",
    "    \"\"\"Safe MAPE (%) ignoring zeros in denominator.\"\"\"\n",
    "    y_true = y_true.astype(float)\n",
    "    y_pred = y_pred.astype(float)\n",
    "    mask = y_true > 0\n",
    "    if mask.sum() == 0:\n",
    "        return None\n",
    "    return float(mean_absolute_percentage_error(y_true[mask], y_pred[mask]) * 100.0)\n",
    "\n",
    "def month_start(dt: pd.Timestamp) -> pd.Timestamp:\n",
    "    return pd.Timestamp(dt.year, dt.month, 1)\n",
    "\n",
    "# === Calendar regressors for monthly data ===\n",
    "def month_calendar_features(months: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"Build monthly exogenous features: business days and weekend days for each month.\"\"\"\n",
    "    uniq = pd.Series(pd.to_datetime(months).values.astype('datetime64[M]')).drop_duplicates().sort_values()\n",
    "    rows = []\n",
    "    for m in uniq:\n",
    "        m = pd.Timestamp(m)\n",
    "        rng = pd.date_range(m, m + pd.offsets.MonthEnd(0), freq='D')\n",
    "        biz = int(np.sum(rng.weekday < 5))\n",
    "        wknd = int(len(rng) - biz)\n",
    "        rows.append({'Month': m, 'biz_days': biz, 'weekend_days': wknd})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def build_exog_for_months(month_series: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"Wrapper to obtain the exogenous dataframe for all relevant months (history + future).\"\"\"\n",
    "    return month_calendar_features(month_series)\n",
    "\n",
    "# === Rolling-origin backtest for Prophet/ARIMA (1-step ahead) ===\n",
    "def rolling_mape_one_dep(dep_df: pd.DataFrame, model: str,\n",
    "                         exog_cols=('biz_days','weekend_days'),\n",
    "                         min_train=12, horizon=1) -> float:\n",
    "    \"\"\"\n",
    "    dep_df columns: ['Month','tickets'] + exog_cols\n",
    "    model: 'prophet' or 'arima'\n",
    "    Returns average MAPE (%) over rolling windows. NaN if not enough data.\n",
    "    \"\"\"\n",
    "    df = dep_df.sort_values('Month').copy()\n",
    "    df['y'] = df['tickets'].astype(float)\n",
    "    df['ds'] = df['Month']\n",
    "\n",
    "    n = len(df)\n",
    "    if n < (min_train + horizon):\n",
    "        return np.nan\n",
    "\n",
    "    errors = []\n",
    "    for end in range(min_train, n - horizon + 1):\n",
    "        train = df.iloc[:end]\n",
    "        test  = df.iloc[end:end+horizon]\n",
    "        if model == 'prophet':\n",
    "            m = Prophet(interval_width=0.90, yearly_seasonality=False, seasonality_mode='multiplicative',\n",
    "                        changepoint_prior_scale=0.2)  # slightly elastic\n",
    "            for c in exog_cols:\n",
    "                m.add_regressor(c)\n",
    "            m.fit(train[['ds','y'] + list(exog_cols)])\n",
    "            future = test[['ds']].copy()\n",
    "            future = future.merge(test[['ds'] + list(exog_cols)], on='ds', how='left')\n",
    "            pred = m.predict(future)['yhat'].values\n",
    "        else:\n",
    "            X_train = train[list(exog_cols)].values\n",
    "            X_test  = test[list(exog_cols)].values\n",
    "            mdl = pm.auto_arima(train['y'].values, X=X_train, seasonal=False,\n",
    "                                error_action='ignore', suppress_warnings=True,\n",
    "                                n_jobs=1, stepwise=True, max_p=5, max_q=5, max_d=2)\n",
    "            pred = mdl.predict(horizon, X=X_test)\n",
    "\n",
    "        y_true = test['y'].values\n",
    "        mask = y_true > 0\n",
    "        if mask.any():\n",
    "            errors.append(mean_absolute_percentage_error(y_true[mask], pred[mask]) * 100.0)\n",
    "\n",
    "    return float(np.mean(errors)) if errors else np.nan\n",
    "\n",
    "# === Forecast functions with exogenous regressors ===\n",
    "def forecast_prophet_exog(train_m: pd.DataFrame, exog: pd.DataFrame,\n",
    "                          dep_col=\"Department\", date_col=\"Month\", y_col=\"tickets\",\n",
    "                          horizon=6, for_verticals=None, dep_to_vertical=None) -> pd.DataFrame:\n",
    "    \"\"\"Prophet with calendar regressors; tunable CPS for target verticals.\"\"\"\n",
    "    rows = []\n",
    "    for dep, g in train_m.groupby(dep_col):\n",
    "        g = g[[date_col, y_col]].dropna().sort_values(date_col)\n",
    "        g = g.rename(columns={date_col:'ds', y_col:'y'})\n",
    "        g = g.merge(exog.rename(columns={'Month':'ds'}), on='ds', how='left')\n",
    "\n",
    "        # More elastic CPS for the target verticals only\n",
    "        is_target = (for_verticals is not None and dep_to_vertical is not None\n",
    "                     and dep_to_vertical.get(dep) in for_verticals)\n",
    "        cps = 0.20 if is_target else 0.05\n",
    "\n",
    "        try:\n",
    "            m = Prophet(interval_width=0.90, yearly_seasonality=False, seasonality_mode='multiplicative',\n",
    "                        changepoint_prior_scale=cps)\n",
    "            for c in ('biz_days','weekend_days'):\n",
    "                m.add_regressor(c)\n",
    "            m.fit(g[['ds','y','biz_days','weekend_days']])\n",
    "\n",
    "            start_fc = month_start(pd.Timestamp.today())\n",
    "            future = pd.date_range(start_fc, periods=horizon, freq='MS').to_frame(index=False, name='ds')\n",
    "            future = future.merge(exog.rename(columns={'Month':'ds'}), on='ds', how='left')\n",
    "            pred = m.predict(future).tail(horizon)['yhat']\n",
    "            fc_vals = np.maximum(0, np.round(pred)).astype(int).tolist()\n",
    "        except Exception:\n",
    "            fc_vals = [int(g['y'].iloc[-1])] * horizon if len(g) else [0] * horizon\n",
    "\n",
    "        for d, v in zip(pd.to_datetime(future['ds']), fc_vals):\n",
    "            rows.append({dep_col: dep, date_col: d, 'Forecast_Prophet': int(v)})\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def forecast_arima_exog(train_m: pd.DataFrame, exog: pd.DataFrame,\n",
    "                        dep_col=\"Department\", date_col=\"Month\", y_col=\"tickets\",\n",
    "                        horizon=6, for_verticals=None, dep_to_vertical=None) -> pd.DataFrame:\n",
    "    \"\"\"Auto-ARIMA with exogenous regressors; wider search for target verticals.\"\"\"\n",
    "    rows = []\n",
    "    for dep, g in train_m.groupby(dep_col):\n",
    "        g = g[[date_col, y_col]].dropna().sort_values(date_col)\n",
    "        X = exog.merge(g[[date_col]], left_on='Month', right_on=date_col, how='right')[['biz_days','weekend_days']].values\n",
    "        y = g[y_col].astype(float).values\n",
    "\n",
    "        if len(y) >= 6:\n",
    "            try:\n",
    "                is_target = (for_verticals is not None and dep_to_vertical is not None\n",
    "                             and dep_to_vertical.get(dep) in for_verticals)\n",
    "                stepwise = not is_target  # allow broader search only for target verticals\n",
    "                mdl = pm.auto_arima(y, X=X, seasonal=False, error_action='ignore', suppress_warnings=True,\n",
    "                                    stepwise=stepwise, max_p=5, max_q=5, max_d=2)\n",
    "                start_fc = month_start(pd.Timestamp.today())\n",
    "                future = pd.date_range(start_fc, periods=horizon, freq='MS')\n",
    "                Xf = exog[exog['Month'].isin(future)][['biz_days','weekend_days']].values\n",
    "                fc = mdl.predict(horizon, X=Xf)\n",
    "                fc_vals = np.maximum(0, np.round(fc)).astype(int).tolist()\n",
    "            except Exception:\n",
    "                fc_vals = [int(y[-1])] * horizon\n",
    "        else:\n",
    "            fc_vals = [int(y[-1])] * horizon if len(y) else [0] * horizon\n",
    "\n",
    "        for d, v in zip(pd.date_range(month_start(pd.Timestamp.today()), periods=horizon, freq='MS'), fc_vals):\n",
    "            rows.append({dep_col: dep, date_col: d, 'Forecast_ARIMA': int(v)})\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# === Hybrid selection with CV (realistic accuracy) ===\n",
    "def build_hybrid_table_cv(train_m: pd.DataFrame, exog: pd.DataFrame,\n",
    "                          dep_col='Department', date_col='Month', y_col='tickets',\n",
    "                          for_verticals=None, dep_to_vertical=None, min_train=12) -> pd.DataFrame:\n",
    "    out = []\n",
    "    t = train_m.merge(exog, on=date_col, how='left')\n",
    "    for dep, g in t.groupby(dep_col):\n",
    "        m_p = rolling_mape_one_dep(g[[date_col, y_col, 'biz_days','weekend_days']], 'prophet', min_train=min_train)\n",
    "        m_a = rolling_mape_one_dep(g[[date_col, y_col, 'biz_days','weekend_days']], 'arima',   min_train=min_train)\n",
    "        if np.isnan(m_p) and np.isnan(m_a):\n",
    "            best = 'Prophet'\n",
    "        elif np.isnan(m_p):\n",
    "            best = 'ARIMA'\n",
    "        elif np.isnan(m_a):\n",
    "            best = 'Prophet'\n",
    "        else:\n",
    "            best = 'Prophet' if m_p <= m_a else 'ARIMA'\n",
    "        out.append({dep_col: dep, 'MAPE_Prophet_CV': m_p, 'MAPE_ARIMA_CV': m_a, 'Best_Model': best})\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "def apply_hybrid(kpi: pd.DataFrame, table: pd.DataFrame, dep_col='Department') -> pd.DataFrame:\n",
    "    kpi = kpi.copy()\n",
    "    kpi['Forecast_Hybrid'] = 0\n",
    "    best_map = table.set_index(dep_col)['Best_Model'].to_dict()\n",
    "    for dep, best in best_map.items():\n",
    "        src = 'Forecast_Prophet' if best == 'Prophet' else 'Forecast_ARIMA'\n",
    "        kpi.loc[kpi[dep_col] == dep, 'Forecast_Hybrid'] = kpi.loc[kpi[dep_col] == dep, src].values\n",
    "    return kpi\n",
    "\n",
    "# === Language split (fixed shares) ===\n",
    "def language_split(df_fc: pd.DataFrame, dep_col='Department', date_col='Month',\n",
    "                   fc_col='Forecast_Hybrid', lang_share: Dict[str, float] = LANG_SHARE) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    shares = {k: float(v)/100.0 for k, v in lang_share.items()}\n",
    "    for _, r in df_fc[[dep_col, date_col, fc_col]].iterrows():\n",
    "        for lang, s in shares.items():\n",
    "            rows.append({\n",
    "                dep_col: r[dep_col],\n",
    "                date_col: r[date_col],\n",
    "                'Language': lang,\n",
    "                'Forecast_Hybrid_Lang': int(round(r[fc_col]*s))\n",
    "            })\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b89251",
   "metadata": {},
   "source": [
    "## 03 - Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63106fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files loaded: 13689 38177 2402 26508 440578 76 (+ dept map rows: 109 )\n"
     ]
    }
   ],
   "source": [
    "# === Load input dataframes (patched for department.xlsx) ===\n",
    "incoming     = pd.read_excel(INCOMING_PATH,  engine=PD_READ_XLSX_ENGINE)\n",
    "call         = pd.read_excel(CALL_PATH,      engine=PD_READ_XLSX_ENGINE)\n",
    "agents       = pd.read_excel(AGENTS_PATH,    engine=PD_READ_XLSX_ENGINE)\n",
    "productivity = pd.read_excel(PROD_PATH,      engine=PD_READ_XLSX_ENGINE)\n",
    "einstein     = pd.read_excel(EINSTEIN_PATH,  engine=PD_READ_XLSX_ENGINE)\n",
    "inventory    = pd.read_excel(INVENTORY_PATH, engine=PD_READ_XLSX_ENGINE)\n",
    "\n",
    "# Official department mapping (primary source)\n",
    "try:\n",
    "    dept_sheets = pd.read_excel(DEPT_PATH, engine=PD_READ_XLSX_ENGINE, sheet_name=None)  # may return dict\n",
    "    # Pick preferred sheet if present; otherwise first sheet\n",
    "    preferred = \"CustomerService department\"\n",
    "    if isinstance(dept_sheets, dict):\n",
    "        if preferred in dept_sheets:\n",
    "            dept_map_df = dept_sheets[preferred]\n",
    "        else:\n",
    "            # take the first sheet\n",
    "            first_name = next(iter(dept_sheets.keys()))\n",
    "            dept_map_df = dept_sheets[first_name]\n",
    "    else:\n",
    "        dept_map_df = dept_sheets  # already a DataFrame\n",
    "except Exception as e:\n",
    "    print(f\"[WARN] Could not read department.xlsx: {e}\")\n",
    "    dept_map_df = pd.DataFrame()\n",
    "\n",
    "print('Files loaded:',\n",
    "      len(incoming), len(call), len(agents), len(productivity), len(einstein), len(inventory),\n",
    "      \"(+ dept map rows:\", len(dept_map_df), \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94666aba",
   "metadata": {},
   "source": [
    "## 04 - Preprocessing – Build train_m (Department, Month, tickets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aac4ba83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_m built: (688, 3)\n"
     ]
    }
   ],
   "source": [
    "# === Preprocessing — Build monthly training table (Department, Month, tickets) ===\n",
    "cand_date = ['Date', 'date', 'Created Date', 'Ticket Date']\n",
    "cand_dep  = ['Department', 'department', 'Department Name', 'dept_name', 'department_id']\n",
    "cand_val  = ['total_incoming', 'ticket_total', 'volume', 'count', 'Tickets', 'tickets']\n",
    "\n",
    "col_date = find_first(incoming, cand_date)\n",
    "col_dep  = find_first(incoming, cand_dep)\n",
    "col_val  = find_first(incoming, cand_val)\n",
    "\n",
    "if col_date is None or col_dep is None or col_val is None:\n",
    "    raise ValueError(f\"Incoming file must contain date/department/tickets-like columns. Found: {col_date}, {col_dep}, {col_val}\")\n",
    "\n",
    "inc = incoming[[col_date, col_dep, col_val]].copy()\n",
    "inc.columns = ['Date', 'Department', 'tickets']\n",
    "inc['Date']  = pd.to_datetime(inc['Date'], errors='coerce')\n",
    "inc['Month'] = inc['Date'].values.astype('datetime64[M]')\n",
    "\n",
    "train_m = (\n",
    "    inc.groupby(['Department', 'Month'], as_index=False)['tickets']\n",
    "      .sum()\n",
    "      .sort_values(['Department', 'Month'])\n",
    ")\n",
    "\n",
    "print('train_m built:', train_m.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bba0378",
   "metadata": {},
   "source": [
    "## 05 - KPI skeleton – join actuals and future months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80a40cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === KPI skeleton — history + future months + inventory merge ===\n",
    "actuals = train_m.rename(columns={'tickets': 'Actual Volume'})[['Department','Month','Actual Volume']]\n",
    "start_fc = pd.Timestamp.today().to_period('M').to_timestamp()\n",
    "future_months = pd.date_range(start_fc, periods=HORIZON_MONTHS, freq='MS')\n",
    "\n",
    "# Keep last 18 months history for context (configurable)\n",
    "hist_keep = actuals.groupby('Department').tail(18)\n",
    "\n",
    "kpi = pd.concat(\n",
    "    [\n",
    "        hist_keep,\n",
    "        pd.MultiIndex.from_product(\n",
    "            [actuals['Department'].unique(), future_months],\n",
    "            names=['Department','Month']\n",
    "        ).to_frame(index=False)\n",
    "    ],\n",
    "    ignore_index=True\n",
    ").drop_duplicates(['Department','Month'])\n",
    "\n",
    "kpi = kpi.merge(actuals, on=['Department','Month'], how='left')\n",
    "\n",
    "# Merge inventory if present\n",
    "if not inventory.empty:\n",
    "    inv_dep  = find_first(inventory, ['Department','department'])\n",
    "    inv_mon  = find_first(inventory, ['Month','month','Date'])\n",
    "    inv_val  = find_first(inventory, ['Inventory','inventory','Backlog'])\n",
    "    if inv_dep and inv_mon and inv_val:\n",
    "        inv = inventory[[inv_dep, inv_mon, inv_val]].copy()\n",
    "        inv.columns = ['Department','Month','Inventory']\n",
    "        inv['Month'] = pd.to_datetime(inv['Month'], errors='coerce').values.astype('datetime64[M]')\n",
    "        kpi = kpi.merge(inv, on=['Department','Month'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc42d2c",
   "metadata": {},
   "source": [
    "## 06 - Forecasts – Prophet and AutoARIMA with exogenous regressors + CV-based hybrid selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56c3c9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Department->Vertical entries from department.xlsx: 101\n",
      "[INFO] Department->Vertical entries from einstein: 101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:00:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:41 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:43 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:43 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:44 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:00:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:00:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:23 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:23 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:41 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:41 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:44 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:44 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:01:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:01:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:02:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:02:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:02:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:02:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:02:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:02:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:02:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:02:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:02:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:02:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:02:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:02:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:02:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:02:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:02:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:02:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:02:04 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    }
   ],
   "source": [
    "# 6.1 Build exogenous features for all months involved (history + future)\n",
    "exog_all_months = build_exog_for_months(pd.concat([train_m['Month'], kpi['Month']]))\n",
    "\n",
    "\n",
    "# 6.2 Load Department -> Vertical mapping (official Excel first, then Einstein)\n",
    "def load_department_vertical_map_from_df(dept_df: pd.DataFrame) -> dict:\n",
    "    \"\"\"Build a {department_name -> vertical} dictionary from a DataFrame.\"\"\"\n",
    "    if dept_df is None or len(dept_df) == 0:\n",
    "        return {}\n",
    "\n",
    "    def pick(df, candidates):\n",
    "        low = {c.lower(): c for c in df.columns}\n",
    "        for cand in candidates:\n",
    "            if cand.lower() in low:\n",
    "                return low[cand.lower()]\n",
    "        # normalized search\n",
    "        for c in df.columns:\n",
    "            cl = c.lower().replace(' ', '').replace('_', '')\n",
    "            for cand in candidates:\n",
    "                if cand.lower().replace(' ', '').replace('_', '') == cl:\n",
    "                    return c\n",
    "        return None\n",
    "\n",
    "    col_dep  = pick(dept_df, ['department_name', 'Department', 'dept_name'])\n",
    "    col_vert = pick(dept_df, ['vertical', 'Vertical', 'business unit', 'bu'])\n",
    "\n",
    "    if not col_dep or not col_vert:\n",
    "        print(\"[WARN] department.xlsx missing required columns (department_name/vertical).\")\n",
    "        return {}\n",
    "\n",
    "    m = (dept_df[[col_dep, col_vert]]\n",
    "         .dropna().drop_duplicates()\n",
    "         .rename(columns={col_dep: 'Department_name', col_vert: 'Vertical'}))\n",
    "\n",
    "    return dict(zip(m['Department_name'], m['Vertical']))\n",
    "\n",
    "\n",
    "def build_dep_to_vertical_map_from_einstein(einstein_df: pd.DataFrame) -> dict:\n",
    "    dep_col  = find_first(einstein_df, ['Department','department','Department Name','dept_name'])\n",
    "    vert_col = find_first(einstein_df, ['Vertical','vertical','Business Unit','BU'])\n",
    "\n",
    "    if dep_col and vert_col:\n",
    "        m = (einstein_df[[dep_col, vert_col]]\n",
    "             .dropna().drop_duplicates()\n",
    "             .rename(columns={dep_col:'Department', vert_col:'Vertical'}))\n",
    "        return dict(zip(m['Department'], m['Vertical']))\n",
    "\n",
    "    return {}\n",
    "\n",
    "\n",
    "# Build mapping dicts\n",
    "dep_to_vertical_excel = load_department_vertical_map_from_df(dept_map_df)\n",
    "dep_to_vertical_ein   = build_dep_to_vertical_map_from_einstein(einstein)\n",
    "\n",
    "# Merge (Excel is authoritative, overrides Einstein)\n",
    "dep_to_vertical = {}\n",
    "dep_to_vertical.update(dep_to_vertical_ein)\n",
    "dep_to_vertical.update(dep_to_vertical_excel)\n",
    "\n",
    "print(f\"[INFO] Department->Vertical entries from department.xlsx: {len(dep_to_vertical_excel)}\")\n",
    "print(f\"[INFO] Department->Vertical entries from einstein: {len(dep_to_vertical_ein)}\")\n",
    "\n",
    "\n",
    "# 6.3 Compute base forecasts (Prophet / ARIMA) using exogenous regressors\n",
    "TARGET_VERTICALS = {'Payments','Hospitality'}\n",
    "\n",
    "fc_prophet = forecast_prophet_exog(\n",
    "    train_m, exog_all_months,\n",
    "    for_verticals=TARGET_VERTICALS,\n",
    "    dep_to_vertical=dep_to_vertical,\n",
    "    horizon=HORIZON_MONTHS\n",
    ")\n",
    "\n",
    "fc_arima = forecast_arima_exog(\n",
    "    train_m, exog_all_months,\n",
    "    for_verticals=TARGET_VERTICALS,\n",
    "    dep_to_vertical=dep_to_vertical,\n",
    "    horizon=HORIZON_MONTHS\n",
    ")\n",
    "\n",
    "\n",
    "kpi = (\n",
    "    kpi\n",
    "    .merge(fc_prophet, on=['Department','Month'], how='left')\n",
    "    .merge(fc_arima,   on=['Department','Month'], how='left')\n",
    ")\n",
    "\n",
    "for c in ['Forecast_Prophet','Forecast_ARIMA']:\n",
    "    if c in kpi.columns:\n",
    "        kpi[c] = kpi[c].fillna(0).astype(int)\n",
    "\n",
    "\n",
    "# 6.4 CV-based hybrid selection (honest accuracy)\n",
    "mape_table_cv = build_hybrid_table_cv(\n",
    "    train_m, exog_all_months,\n",
    "    for_verticals=TARGET_VERTICALS,\n",
    "    dep_to_vertical=dep_to_vertical\n",
    ")\n",
    "\n",
    "kpi = apply_hybrid(kpi, mape_table_cv, dep_col='Department')\n",
    "\n",
    "\n",
    "# Optional blending when MAPEs are almost equal (< 1 pp)\n",
    "if 'MAPE_Prophet_CV' in mape_table_cv.columns and 'MAPE_ARIMA_CV' in mape_table_cv.columns:\n",
    "    best_map = mape_table_cv.set_index('Department')[\n",
    "        ['MAPE_Prophet_CV','MAPE_ARIMA_CV','Best_Model']\n",
    "    ].to_dict('index')\n",
    "\n",
    "    for dep, info in best_map.items():\n",
    "        mpp, maa = info.get('MAPE_Prophet_CV'), info.get('MAPE_ARIMA_CV')\n",
    "        if pd.notna(mpp) and pd.notna(maa) and abs(mpp - maa) < 1.0:\n",
    "            wp = 1.0 / max(mpp, 1e-6)\n",
    "            wa = 1.0 / max(maa, 1e-6)\n",
    "            wsum = wp + wa\n",
    "            mask = kpi['Department'] == dep\n",
    "            blend = np.round(\n",
    "                (wp/wsum) * kpi.loc[mask,'Forecast_Prophet'] +\n",
    "                (wa/wsum) * kpi.loc[mask,'Forecast_ARIMA']\n",
    "            ).astype(int)\n",
    "            kpi.loc[mask, 'Forecast_Hybrid'] = blend.values\n",
    "\n",
    "\n",
    "kpi['Forecast_Hybrid'] = kpi['Forecast_Hybrid'].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac78509d",
   "metadata": {},
   "source": [
    "## 07. Capacity & Productivity enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6393833e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpi['Capacity'] = 0\n",
    "kpi['Productivity'] = 0\n",
    "\n",
    "# Productivity merge\n",
    "prod_dep = find_first(productivity, ['Department','department'])\n",
    "prod_mon = find_first(productivity, ['Month','month','Date'])\n",
    "prod_val = find_first(productivity, ['Productivity','productivity','Tickets per Agent','TPA'])\n",
    "\n",
    "if prod_dep and prod_mon and prod_val:\n",
    "    prod = productivity[[prod_dep, prod_mon, prod_val]].copy()\n",
    "    prod.columns = ['Department','Month','Productivity']\n",
    "    prod['Month'] = pd.to_datetime(prod['Month'], errors='coerce').values.astype('datetime64[M]')\n",
    "    kpi = kpi.drop(columns=['Productivity']).merge(prod, on=['Department','Month'], how='left').fillna({'Productivity':0})\n",
    "\n",
    "# Capacity merge (agents or call, whichever contains a capacity-like column)\n",
    "cap_dep = find_first(agents, ['Department','department']) or find_first(call, ['Department','department'])\n",
    "cap_mon = find_first(agents, ['Month','month','Date']) or find_first(call, ['Month','month','Date'])\n",
    "cap_val = find_first(agents, ['Capacity','capacity','HC Capacity']) or find_first(call, ['Capacity','capacity'])\n",
    "\n",
    "if cap_dep and cap_mon and cap_val:\n",
    "    cap_df = (agents if find_first(agents, ['Capacity','capacity','HC Capacity']) else call)\n",
    "    cap = cap_df[[cap_dep, cap_mon, cap_val]].copy()\n",
    "    cap.columns = ['Department','Month','Capacity']\n",
    "    cap['Month'] = pd.to_datetime(cap['Month'], errors='coerce').values.astype('datetime64[M]')\n",
    "    kpi = kpi.drop(columns=['Capacity']).merge(cap, on=['Department','Month'], how='left').fillna({'Capacity':0})\n",
    "\n",
    "# Basic derived gaps (absolute)\n",
    "kpi['Expected Forecast vs Capacity'] = kpi['Forecast_Hybrid'] - kpi['Capacity']\n",
    "kpi['Actual Volume vs Productivity'] = kpi.get('Actual Volume', pd.Series(0, index=kpi.index)).fillna(0) - kpi['Productivity']\n",
    "\n",
    "# If Inventory is missing, approximate as cumulative sum of monthly (Actual - Productivity)\n",
    "if 'Inventory' not in kpi.columns:\n",
    "    kpi = kpi.sort_values(['Department','Month'])\n",
    "    kpi['Inventory'] = kpi.groupby('Department')['Actual Volume vs Productivity'].cumsum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7b1a68",
   "metadata": {},
   "source": [
    "## 08. Build final report to upgrade to PowerBI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d66cd2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Vertical mapping coverage: 0/75 departments mapped.\n",
      "[WARN] Unmapped departments (sample up to 20): [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n"
     ]
    }
   ],
   "source": [
    "report = kpi.copy()\n",
    "\n",
    "# Attach Vertical using the official mapping dictionary\n",
    "report['Vertical'] = report['Department'].map(dep_to_vertical).fillna('Unknown')\n",
    "\n",
    "# Coverage logging (helps you fix unmapped departments in department.xlsx)\n",
    "total_deps  = kpi['Department'].nunique()\n",
    "mapped_deps = report.loc[report['Vertical']!='Unknown','Department'].nunique()\n",
    "print(f\"[INFO] Vertical mapping coverage: {mapped_deps}/{total_deps} departments mapped.\")\n",
    "if mapped_deps < total_deps:\n",
    "    missing = sorted(set(kpi['Department']) - set(report.loc[report['Vertical']!='Unknown','Department']))\n",
    "    print(\"[WARN] Unmapped departments (sample up to 20):\", missing[:20])\n",
    "\n",
    "# Rename Department to Department_name\n",
    "report = report.rename(columns={'Department':'Department_name'})\n",
    "\n",
    "# Core columns\n",
    "report['Forecast']       = report['Forecast_Hybrid'].astype(float)\n",
    "report['Actual Volume']  = report.get('Actual Volume', pd.Series(0, index=report.index)).astype(float)\n",
    "\n",
    "# Forecast Accuracy (as fraction; convert to percent later)\n",
    "report['Forecast Accuracy'] = np.where(\n",
    "    report['Actual Volume'] > 0,\n",
    "    1.0 - (np.abs(report['Forecast'] - report['Actual Volume']) / report['Actual Volume']),\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "# >>> Requested: \"Difference Capacity vs Productivity\" as PERCENT <<<\n",
    "# Percentage of productivity over capacity, expressed in percent\n",
    "report['Difference Capacity vs Productivity'] = np.where(\n",
    "    report['Capacity'] > 0,\n",
    "    (report['Productivity'] / report['Capacity']) * 100.0,\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "# Other numeric gaps as requested (keep for context)\n",
    "report['Expected Forecast vs Capacity'] = report['Forecast'] - report['Capacity']\n",
    "report['Actual Volume vs Productivity'] = report['Actual Volume'] - report['Productivity']\n",
    "\n",
    "# Column order EXACTLY as requested\n",
    "report_out = report[['Vertical',\n",
    "                     'Department_name',\n",
    "                     'Month',\n",
    "                     'Forecast',\n",
    "                     'Actual Volume',\n",
    "                     'Forecast Accuracy',\n",
    "                     'Capacity',\n",
    "                     'Productivity',\n",
    "                     'Difference Capacity vs Productivity',\n",
    "                     'Expected Forecast vs Capacity',\n",
    "                     'Actual Volume vs Productivity',\n",
    "                     'Inventory']].copy()\n",
    "\n",
    "# Friendly formatting\n",
    "report_out['Month'] = pd.to_datetime(report_out['Month']).dt.strftime('%b-%y').str.title()\n",
    "report_out['Forecast Accuracy'] = (report_out['Forecast Accuracy'] * 100).round(1)  # %\n",
    "report_out['Difference Capacity vs Productivity'] = report_out['Difference Capacity vs Productivity'].round(1)\n",
    "\n",
    "# (Optional) Round some integer-like fields for presentation\n",
    "for col in ['Forecast','Actual Volume','Capacity','Productivity','Expected Forecast vs Capacity',\n",
    "            'Actual Volume vs Productivity','Inventory']:\n",
    "    if col in report_out.columns:\n",
    "        report_out[col] = pd.to_numeric(report_out[col], errors='coerce').round(0).astype('Int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45118b09",
   "metadata": {},
   "source": [
    "## 09. Language split and export to excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6bb0730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported:\n",
      " - C:\\Users\\pt3canro\\Desktop\\CAPACITY\\outputs\\capacity_forecast_hybrid.xlsx\n",
      " - C:\\Users\\pt3canro\\Desktop\\CAPACITY\\outputs\\mape_by_department.xlsx\n",
      "Rows in KPI: 1102\n",
      "Departments: 75\n",
      "Actuals column present? False\n"
     ]
    }
   ],
   "source": [
    "lang_df   = language_split(kpi, fc_col='Forecast_Hybrid')\n",
    "lang_pivot= lang_df.pivot_table(index=['Department','Month'], columns='Language',\n",
    "                                values='Forecast_Hybrid_Lang', aggfunc='sum').reset_index()\n",
    "\n",
    "# Ensure output folder exists\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "out_xlsx = os.path.join(OUTPUT_FOLDER, OUT_XLSX)\n",
    "out_mape = os.path.join(OUTPUT_FOLDER, OUT_MAPE)\n",
    "\n",
    "# === Export all sheets ===\n",
    "with pd.ExcelWriter(out_xlsx, engine=PD_WRITE_XLSX_ENGINE) as writer:\n",
    "    # Full KPI table for traceability\n",
    "    kpi.to_excel(writer, sheet_name='kpi_final', index=False)\n",
    "    # CV table (model selection diagnostics)\n",
    "    mape_table_cv.to_excel(writer, sheet_name='mape_table_cv', index=False)\n",
    "    # Language detail\n",
    "    lang_df.to_excel(writer, sheet_name='forecast_by_language', index=False)\n",
    "    lang_pivot.to_excel(writer, sheet_name='language_pivot', index=False)\n",
    "    # Final requested report\n",
    "    report_out.to_excel(writer, sheet_name='capacity_report', index=False)\n",
    "\n",
    "# Separate MAPE file (compatibility with your original pipeline)\n",
    "mape_table_cv.to_excel(out_mape, index=False)\n",
    "\n",
    "print(\"Exported:\")\n",
    "print(\" -\", out_xlsx)\n",
    "print(\" -\", out_mape)\n",
    "print('Rows in KPI:', len(kpi))\n",
    "print('Departments:', kpi['Department_name'].nunique() if 'Department_name' in kpi.columns else kpi['Department'].nunique())\n",
    "print('Actuals column present?', 'Actual Volume' in kpi.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
