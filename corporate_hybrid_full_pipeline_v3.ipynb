{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e709ee08",
   "metadata": {},
   "source": [
    "# Corporate Hybrid Forecast Notebook ((Prophet vs ARIMA vs TBATS/ETS)) â€“ v3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f323b68",
   "metadata": {},
   "source": [
    "## 01 - Imports & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22c4f3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hybrid 3-Way (Prophet vs ARIMA vs TBATS/ETS) with 12-month horizon,\n",
    "capacity report, and daily capacity plan with language split.\n",
    "- Stable monthly forecasts (log-scale modeling + safety clipping)\n",
    "- Temporal reconciliation (monthly -> daily) so sheet2 sums match sheet1\n",
    "- CV sMAPE always computed (adaptive CV), Best_Model justified\n",
    "- capacity_error shows historical months (Actual_Volume) from REPORT_START_MONTH\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import warnings\n",
    "import traceback\n",
    "from typing import Optional, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Forecasting libs\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "try:\n",
    "    from prophet import Prophet\n",
    "except Exception:\n",
    "    Prophet = None\n",
    "\n",
    "try:\n",
    "    from tbats import TBATS\n",
    "except Exception:\n",
    "    TBATS = None\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---------- Configuration ----------\n",
    "# Inputs (aligned to your real files)\n",
    "INCOMING_SOURCE_PATH = r\"C:\\Users\\pt3canro\\Desktop\\CAPACITY\\input_model\\Incoming_new.xlsx\"  # Sheet 'Main'\n",
    "INCOMING_SHEET = \"Main\"\n",
    "DEPT_MAP_PATH = r\"C:\\Users\\pt3canro\\Desktop\\CAPACITY\\input_model\\department.xlsx\"\n",
    "DEPT_MAP_SHEET = \"map\"\n",
    "PRODUCTIVITY_PATH = r\"C:\\Users\\pt3canro\\Desktop\\CAPACITY\\input_model\\productivity_agents.xlsx\"\n",
    "\n",
    "# Outputs\n",
    "OUTPUT_XLSX = r\"C:\\Users\\pt3canro\\Desktop\\CAPACITY\\outputs\\capacity_forecast_hybrid.xlsx\"\n",
    "\n",
    "# Horizons and switches\n",
    "H_MONTHS = 12             # monthly forecast horizon\n",
    "DAILY_HORIZON_DAYS = 90   # daily plan horizon\n",
    "REPORT_START_MONTH = \"2025-01\"  # show historical months from this month onward in capacity_error\n",
    "\n",
    "# Temporal reconciliation: if True, build daily from monthly (top-down)\n",
    "USE_DAILY_FROM_MONTHLY = True\n",
    "\n",
    "# Safety clipping against extreme over-forecast\n",
    "ENABLE_FORECAST_CLIP = True\n",
    "FORECAST_CLIP_MULTIPLIER = 2.5  # cap to X * rolling 12m mean\n",
    "\n",
    "# Organization-specific week rule (kept for future use)\n",
    "WEEKLY_START_THU = True\n",
    "\n",
    "# Language shares (fixed)\n",
    "LANGUAGE_SHARES = {\n",
    "    'English': 0.6435,\n",
    "    'French': 0.0741,\n",
    "    'German': 0.0860,\n",
    "    'Italian': 0.0667,\n",
    "    'Portuguese': 0.0162,\n",
    "    'Spanish': 0.1135\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3c5b8a",
   "metadata": {},
   "source": [
    "## 02. Data loading and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f3aa7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_incoming(path: str, sheet_name: Optional[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load daily incoming volumes from Excel/CSV.\n",
    "    Expected columns: Date, department_id, ticket_total (or construct it).\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Incoming file not found:\\n{path}\\n\"\n",
    "            \"Please update INCOMING_SOURCE_PATH to the correct location.\"\n",
    "        )\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext in [\".xlsx\", \".xlsm\", \".xls\"]:\n",
    "        if not sheet_name:\n",
    "            raise ValueError(\"Excel file detected but no sheet_name provided (e.g., 'Main').\")\n",
    "        df = pd.read_excel(path, sheet_name=sheet_name, engine=\"openpyxl\")\n",
    "    elif ext == \".csv\":\n",
    "        df = pd.read_csv(path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported extension for incoming data: {ext}\")\n",
    "\n",
    "    # Basic columns\n",
    "    base_required = {'Date', 'department_id'}\n",
    "    missing_base = base_required - set(df.columns)\n",
    "    if missing_base:\n",
    "        raise ValueError(\n",
    "            f\"Incoming file must contain columns: {sorted(list(base_required))}. \"\n",
    "            f\"Found columns: {list(df.columns)}. Missing: {sorted(list(missing_base))}\"\n",
    "        )\n",
    "\n",
    "    # ticket_total creation\n",
    "    if 'ticket_total' not in df.columns:\n",
    "        if 'total_incoming' in df.columns:\n",
    "            df['ticket_total'] = pd.to_numeric(df['total_incoming'], errors='coerce').fillna(0)\n",
    "        elif {'incoming_from_customers', 'incoming_from_transfers'}.issubset(df.columns):\n",
    "            df['ticket_total'] = (\n",
    "                pd.to_numeric(df['incoming_from_customers'], errors='coerce').fillna(0) +\n",
    "                pd.to_numeric(df['incoming_from_transfers'], errors='coerce').fillna(0)\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Incoming file must contain 'ticket_total' or 'total_incoming' or \"\n",
    "                \"both 'incoming_from_customers' and 'incoming_from_transfers'. \"\n",
    "                f\"Found columns: {list(df.columns)}\"\n",
    "            )\n",
    "\n",
    "    # Dtypes\n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "    if df['Date'].isna().any():\n",
    "        bad = df.loc[df['Date'].isna()]\n",
    "        raise ValueError(f\"Some Date values could not be parsed. Example rows:\\n{bad.head(5)}\")\n",
    "    df['department_id'] = df['department_id'].astype(str).str.strip()\n",
    "    df['ticket_total'] = pd.to_numeric(df['ticket_total'], errors='coerce').fillna(0).astype(float)\n",
    "\n",
    "    # Optional columns\n",
    "    if 'department_name' in df.columns:\n",
    "        df['department_name'] = df['department_name'].astype(str).str.strip()\n",
    "    else:\n",
    "        df['department_name'] = None\n",
    "    if 'vertical' in df.columns:\n",
    "        df['vertical'] = df['vertical'].astype(str).str.strip()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_dept_map(path: str, sheet: Optional[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load dept mapping -> department_name, vertical.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        return pd.DataFrame(columns=['department_id', 'department_name', 'vertical'])\n",
    "\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext in (\".xlsx\", \".xlsm\", \".xls\"):\n",
    "        if sheet:\n",
    "            mp = pd.read_excel(path, sheet_name=sheet, engine=\"openpyxl\")\n",
    "        else:\n",
    "            xls = pd.ExcelFile(path, engine=\"openpyxl\")\n",
    "            mp = pd.read_excel(xls, sheet_name=xls.sheet_names[0])\n",
    "    else:\n",
    "        mp = pd.read_csv(path)\n",
    "\n",
    "    rename_map = {\n",
    "        'dept_id': 'department_id',\n",
    "        'dept_name': 'department_name',\n",
    "        'name': 'department_name',\n",
    "        'segment': 'vertical',\n",
    "        'vertical_name': 'vertical'\n",
    "    }\n",
    "    mp = mp.rename(columns={k: v for k, v in rename_map.items() if k in mp.columns})\n",
    "    if 'department_id' not in mp.columns:\n",
    "        raise ValueError(f\"Department map must contain 'department_id'. Found: {list(mp.columns)}\")\n",
    "\n",
    "    mp['department_id'] = mp['department_id'].astype(str).str.strip()\n",
    "    mp['department_name'] = (mp['department_name'].astype(str).str.strip()\n",
    "                             if 'department_name' in mp.columns else None)\n",
    "    mp['vertical'] = (mp['vertical'].astype(str).str.strip()\n",
    "                      if 'vertical' in mp.columns else None)\n",
    "\n",
    "    return mp[['department_id', 'department_name', 'vertical']].drop_duplicates('department_id')\n",
    "\n",
    "\n",
    "def load_productivity(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load agent productivity and compute dept-level mean tickets per agent-day.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Productivity file not found: {path}\")\n",
    "    df = pd.read_excel(path, engine=\"openpyxl\")\n",
    "    req = {'Date', 'agent_id', 'department_id', 'prod_total_model'}\n",
    "    missing = req - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"productivity_agents.xlsx missing columns: {sorted(list(missing))}. \"\n",
    "                         f\"Found: {list(df.columns)}\")\n",
    "    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "    df['department_id'] = df['department_id'].astype(str).str.strip()\n",
    "    df['prod_total_model'] = pd.to_numeric(df['prod_total_model'], errors='coerce')\n",
    "\n",
    "    prod_dept = (df.groupby('department_id', as_index=False)['prod_total_model']\n",
    "                 .mean()\n",
    "                 .rename(columns={'prod_total_model': 'avg_tickets_per_agent_day'}))\n",
    "    return prod_dept\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64578b79",
   "metadata": {},
   "source": [
    "## 03 - Utilities & helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "220f81ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_floor(dt: pd.Timestamp) -> pd.Timestamp:\n",
    "    return pd.Timestamp(year=dt.year, month=dt.month, day=1)\n",
    "\n",
    "\n",
    "def business_days_in_month(year: int, month: int) -> int:\n",
    "    \"\"\"Approximate Mon-Fri working days in a month.\"\"\"\n",
    "    rng = pd.date_range(start=pd.Timestamp(year=year, month=month, day=1),\n",
    "                        end=pd.Timestamp(year=year, month=month, day=1) + pd.offsets.MonthEnd(0),\n",
    "                        freq='D')\n",
    "    return int(np.sum(rng.weekday < 5))\n",
    "\n",
    "\n",
    "def smape(y_true, y_pred) -> float:\n",
    "    \"\"\"sMAPE robust for intermittent series.\"\"\"\n",
    "    y_true = np.array(y_true, dtype=float)\n",
    "    y_pred = np.array(y_pred, dtype=float)\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred))\n",
    "    denom[denom == 0] = 1.0\n",
    "    return float(np.mean(2.0 * np.abs(y_pred - y_true) / denom) * 100.0)\n",
    "\n",
    "\n",
    "def apply_mapping(incoming: pd.DataFrame, mapping: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge department_name / vertical using department_id.\n",
    "    Prefer incoming values over mapping values when available.\n",
    "    \"\"\"\n",
    "    merged = incoming.merge(mapping, on='department_id', how='left', suffixes=('', '_map'))\n",
    "\n",
    "    if 'department_name' not in merged.columns:\n",
    "        merged['department_name'] = None\n",
    "    if 'department_name_map' not in merged.columns:\n",
    "        merged['department_name_map'] = None\n",
    "    merged['department_name'] = merged['department_name'].fillna(merged['department_name_map']).fillna(\"Unknown\")\n",
    "\n",
    "    if 'vertical' not in merged.columns:\n",
    "        merged['vertical'] = None\n",
    "    if 'vertical_map' not in merged.columns:\n",
    "        merged['vertical_map'] = None\n",
    "    merged['vertical'] = merged['vertical'].fillna(merged['vertical_map']).fillna(\"Unmapped\")\n",
    "\n",
    "    drop_cols = [c for c in merged.columns if c.endswith('_map')]\n",
    "    merged.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac18c10",
   "metadata": {},
   "source": [
    "## 04. Modelling (Hybrid 3-Way Prophet / ARIMA / TBATS-ETS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5040d130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _clip_forecast_to_scale(ts_m: pd.Series, fc: pd.Series) -> pd.Series:\n",
    "    \"\"\"Optional safety clipping to avoid unreasonable explosions.\"\"\"\n",
    "    if not ENABLE_FORECAST_CLIP or ts_m.empty or fc.empty:\n",
    "        return fc\n",
    "    # rolling 12-month mean as a soft scale\n",
    "    r12 = float(ts_m.tail(12).mean()) if len(ts_m) >= 3 else float(ts_m.mean())\n",
    "    if not np.isfinite(r12) or r12 <= 0:\n",
    "        return fc.clip(lower=0)\n",
    "    upper = r12 * FORECAST_CLIP_MULTIPLIER\n",
    "    return fc.clip(lower=0, upper=upper)\n",
    "\n",
    "\n",
    "def fit_prophet_monthly_log(ts_m: pd.Series):\n",
    "    \"\"\"Fit Prophet on log1p(monthly).\"\"\"\n",
    "    if Prophet is None:\n",
    "        return None, None\n",
    "    y = np.log1p(ts_m.values)\n",
    "    dfp = pd.DataFrame({'ds': ts_m.index.to_timestamp(), 'y': y})\n",
    "    m = Prophet(weekly_seasonality=False, yearly_seasonality=True, daily_seasonality=False)\n",
    "    m.fit(dfp)\n",
    "\n",
    "    def fcast(h_months=H_MONTHS):\n",
    "        future = m.make_future_dataframe(periods=h_months, freq='MS')\n",
    "        pred = m.predict(future)\n",
    "        pred = pred.set_index(pd.PeriodIndex(pred['ds'], freq='M'))['yhat']\n",
    "        pred = np.expm1(pred.iloc[-h_months:])\n",
    "        return pd.Series(np.clip(pred, 0, None), index=pred.index)\n",
    "\n",
    "    return m, fcast\n",
    "\n",
    "\n",
    "def fit_arima_monthly_log(ts_m: pd.Series):\n",
    "    \"\"\"Search SARIMAX on log1p(monthly) with seasonal 12 when length permits.\"\"\"\n",
    "    y = np.log1p(ts_m)\n",
    "    best_aic, best_model = np.inf, None\n",
    "    best_order, best_seasonal = None, None\n",
    "    pqs = [0, 1, 2]\n",
    "    PsQs = [0, 1]\n",
    "    seasonal = len(ts_m) >= 12\n",
    "    for p in pqs:\n",
    "        for d in [1] if len(ts_m) < 24 else [0, 1]:\n",
    "            for q in pqs:\n",
    "                for P in ([0, 1] if seasonal else [0]):\n",
    "                    for D in ([0, 1] if seasonal else [0]):\n",
    "                        for Q in ([0, 1] if seasonal else [0]):\n",
    "                            try:\n",
    "                                model = SARIMAX(y, order=(p, d, q),\n",
    "                                                seasonal_order=(P, D, Q, 12 if seasonal else 0),\n",
    "                                                enforce_stationarity=False,\n",
    "                                                enforce_invertibility=False).fit(disp=False)\n",
    "                                if model.aic < best_aic:\n",
    "                                    best_aic = model.aic\n",
    "                                    best_model = model\n",
    "                                    best_order = (p, d, q)\n",
    "                                    best_seasonal = (P, D, Q, 12 if seasonal else 0)\n",
    "                            except Exception:\n",
    "                                continue\n",
    "\n",
    "    def fcast(h_months=H_MONTHS):\n",
    "        fc_log = best_model.get_forecast(h_months).predicted_mean\n",
    "        idx = pd.period_range(ts_m.index[-1] + 1, periods=h_months, freq='M')\n",
    "        fc = np.expm1(fc_log)\n",
    "        s = pd.Series(np.clip(fc, 0, None), index=idx)\n",
    "        return s\n",
    "\n",
    "    return best_model, fcast\n",
    "\n",
    "\n",
    "def fit_tbats_or_ets_monthly_log(ts_m: pd.Series):\n",
    "    \"\"\"Fit TBATS if available; else ETS (trend+seasonality) on log1p.\"\"\"\n",
    "    y = np.log1p(ts_m)\n",
    "    if TBATS is not None and len(ts_m) >= 12:\n",
    "        estimator = TBATS(use_arma_errors=False, seasonal_periods=[12])\n",
    "        model = estimator.fit(ts_m.to_timestamp())\n",
    "        def fcast(h_months=H_MONTHS):\n",
    "            vals_log = model.forecast(steps=h_months)\n",
    "            idx = pd.period_range(ts_m.index[-1] + 1, periods=h_months, freq='M')\n",
    "            vals = np.expm1(vals_log)\n",
    "            return pd.Series(np.clip(vals, 0, None), index=idx)\n",
    "        return model, fcast\n",
    "    else:\n",
    "        # ETS on log1p: additive trend + additive seasonality when possible\n",
    "        seasonal = 12 if len(ts_m) >= 24 else None\n",
    "        model = ExponentialSmoothing(y, trend='add', seasonal=('add' if seasonal else None),\n",
    "                                     seasonal_periods=seasonal).fit()\n",
    "        def fcast(h_months=H_MONTHS):\n",
    "            vals_log = model.forecast(h_months)\n",
    "            idx = pd.period_range(ts_m.index[-1] + 1, periods=h_months, freq='M')\n",
    "            vals = np.expm1(vals_log)\n",
    "            return pd.Series(np.clip(vals, 0, None), index=idx)\n",
    "        return model, fcast\n",
    "\n",
    "\n",
    "def rolling_cv_monthly_adaptive(ts_m: pd.Series) -> Optional[Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Adaptive rolling-origin CV:\n",
    "    - if n >= 15 -> h=3\n",
    "    - if 9 <= n < 15 -> h=1\n",
    "    Returns mean sMAPE per model on original scale.\n",
    "    \"\"\"\n",
    "    n = len(ts_m)\n",
    "    if n < 9:\n",
    "        return None\n",
    "    h = 3 if n >= 15 else 1\n",
    "    min_train = max(12, n - (h + 2))  # ensure at least one split\n",
    "    splits = []\n",
    "    for start in range(min_train, n - h + 1):\n",
    "        train = ts_m.iloc[:start]\n",
    "        test = ts_m.iloc[start:start + h]\n",
    "\n",
    "        metrics = {}\n",
    "\n",
    "        # Prophet\n",
    "        mp, fp = fit_prophet_monthly_log(train)\n",
    "        if fp is not None:\n",
    "            try:\n",
    "                pred = fp(h_months=h)\n",
    "                metrics['Prophet'] = smape(test.values, pred.values[:h])\n",
    "            except Exception:\n",
    "                metrics['Prophet'] = np.inf\n",
    "\n",
    "        # ARIMA\n",
    "        try:\n",
    "            ma, fa = fit_arima_monthly_log(train)\n",
    "            pred = fa(h_months=h)\n",
    "            metrics['ARIMA'] = smape(test.values, pred.values[:h])\n",
    "        except Exception:\n",
    "            metrics['ARIMA'] = np.inf\n",
    "\n",
    "        # TBATS/ETS\n",
    "        try:\n",
    "            mt, ft = fit_tbats_or_ets_monthly_log(train)\n",
    "            pred = ft(h_months=h)\n",
    "            metrics['TBATS/ETS'] = smape(test.values, pred.values[:h])\n",
    "        except Exception:\n",
    "            metrics['TBATS/ETS'] = np.inf\n",
    "\n",
    "        splits.append(metrics)\n",
    "\n",
    "    dfm = pd.DataFrame(splits)\n",
    "    return dfm.mean().to_dict()\n",
    "\n",
    "\n",
    "def select_or_blend_forecasts(fc_dict: Dict[str, pd.Series], cv_scores: Dict[str, float], blend: bool = True):\n",
    "    \"\"\"\n",
    "    Given forecasts per model and CV scores (lower better):\n",
    "    - select best model or\n",
    "    - blend with weights ~ 1/sMAPE.\n",
    "    \"\"\"\n",
    "    scores = {k: (v if v is not None and np.isfinite(v) else 1e6) for k, v in cv_scores.items()}\n",
    "    if not blend:\n",
    "        best = min(scores, key=scores.get)\n",
    "        return fc_dict[best], {'winner': best, 'weights': {best: 1.0}}\n",
    "\n",
    "    inv = {k: (1.0 / v if v > 0 else 0.0) for k, v in scores.items()}\n",
    "    total = sum(inv.values())\n",
    "    if total == 0:\n",
    "        best = min(scores, key=scores.get)\n",
    "        return fc_dict[best], {'winner': best, 'weights': {best: 1.0}}\n",
    "    w = {k: inv[k] / total for k in inv}\n",
    "\n",
    "    # Align indices\n",
    "    idx = None\n",
    "    for s in fc_dict.values():\n",
    "        idx = s.index if idx is None else idx.union(s.index)\n",
    "\n",
    "    blended = sum(w[k] * fc_dict[k].reindex(idx).fillna(0) for k in fc_dict)\n",
    "    return blended, {'winner': min(scores, key=scores.get), 'weights': w}\n",
    "\n",
    "\n",
    "def build_monthly_series(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Aggregate daily incoming to monthly by department.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['month'] = df['Date'].dt.to_period('M')\n",
    "    monthly = (df.groupby(['department_id', 'month'], as_index=False)['ticket_total']\n",
    "               .sum()\n",
    "               .rename(columns={'ticket_total': 'incoming_monthly'}))\n",
    "    return monthly\n",
    "\n",
    "\n",
    "def forecast_per_department_monthly(monthly: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Hybrid modelling per department (log-scale) + adaptive CV + optional clipping.\n",
    "    Returns columns:\n",
    "    department_id, month, forecast_monthly,\n",
    "    cv_prophet_smape, cv_arima_smape, cv_tbats_ets_smape,\n",
    "    winner_model, blend_prophet_w, blend_arima_w, blend_tbats_ets_w\n",
    "    \"\"\"\n",
    "    out_rows = []\n",
    "    dept_ids = monthly['department_id'].unique().tolist()\n",
    "\n",
    "    for dept in dept_ids:\n",
    "        ts = (monthly.loc[monthly['department_id'] == dept, ['month', 'incoming_monthly']]\n",
    "              .sort_values('month')\n",
    "              .set_index('month')['incoming_monthly'])\n",
    "        if not pd.api.types.is_period_dtype(ts.index):\n",
    "            ts.index = pd.PeriodIndex(ts.index, freq='M')\n",
    "        n = len(ts)\n",
    "        if n == 0:\n",
    "            continue\n",
    "\n",
    "        cv = {}\n",
    "        fc_dict: Dict[str, pd.Series] = {}\n",
    "\n",
    "        # PROPHET\n",
    "        if Prophet is not None and n >= 12:\n",
    "            try:\n",
    "                _, fp = fit_prophet_monthly_log(ts)\n",
    "                if fp is not None:\n",
    "                    fc_dict['Prophet'] = fp(H_MONTHS)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # ARIMA\n",
    "        try:\n",
    "            _, fa = fit_arima_monthly_log(ts)\n",
    "            fc_dict['ARIMA'] = fa(H_MONTHS)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # TBATS/ETS\n",
    "        try:\n",
    "            _, ft = fit_tbats_or_ets_monthly_log(ts)\n",
    "            fc_dict['TBATS/ETS'] = ft(H_MONTHS)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Fallback: naive mean\n",
    "        if not fc_dict:\n",
    "            idx = pd.period_range(ts.index[-1] + 1, periods=H_MONTHS, freq='M')\n",
    "            val = max(0.0, float(ts.mean())) if n > 0 else 0.0\n",
    "            fc_dict['NaiveMean'] = pd.Series([val] * H_MONTHS, index=idx)\n",
    "\n",
    "        # CV metrics\n",
    "        try:\n",
    "            cv = rolling_cv_monthly_adaptive(ts) or {}\n",
    "        except Exception:\n",
    "            cv = {}\n",
    "\n",
    "        # Blend or select\n",
    "        if not cv:\n",
    "            preferred = ['ARIMA', 'TBATS/ETS', 'Prophet', 'NaiveMean']\n",
    "            winner = next((k for k in preferred if k in fc_dict), list(fc_dict.keys())[0])\n",
    "            blended = fc_dict[winner]\n",
    "            meta = {'winner': winner, 'weights': {winner: 1.0}}\n",
    "        else:\n",
    "            blended, meta = select_or_blend_forecasts(fc_dict, cv_scores=cv, blend=True)\n",
    "\n",
    "        # Optional clipping\n",
    "        blended = _clip_forecast_to_scale(ts, blended)\n",
    "\n",
    "        for per, val in blended.items():\n",
    "            out_rows.append({\n",
    "                'department_id': dept,\n",
    "                'month': per,\n",
    "                'forecast_monthly': max(0.0, float(val)),\n",
    "                'cv_prophet_smape': cv.get('Prophet', np.nan),\n",
    "                'cv_arima_smape': cv.get('ARIMA', np.nan),\n",
    "                'cv_tbats_ets_smape': cv.get('TBATS/ETS', np.nan),\n",
    "                'winner_model': meta['winner'],\n",
    "                'blend_prophet_w': (meta['weights'].get('Prophet', np.nan) if 'weights' in meta else np.nan),\n",
    "                'blend_arima_w': (meta['weights'].get('ARIMA', np.nan) if 'weights' in meta else np.nan),\n",
    "                'blend_tbats_ets_w': (meta['weights'].get('TBATS/ETS', np.nan) if 'weights' in meta else np.nan),\n",
    "            })\n",
    "\n",
    "    df_out = pd.DataFrame(out_rows)\n",
    "    if not df_out.empty:\n",
    "        df_out['department_id'] = df_out['department_id'].astype(str)\n",
    "        if not pd.api.types.is_period_dtype(df_out['month']):\n",
    "            df_out['month'] = pd.PeriodIndex(df_out['month'], freq='M')\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def compute_monthly_accuracy_with_history(monthly: pd.DataFrame,\n",
    "                                          fc_monthly: pd.DataFrame,\n",
    "                                          report_start: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build capacity_error-like table:\n",
    "    - historical months from report_start .. last_actual: Actual_Volume (Forecast = NaN)\n",
    "    - future months: Forecast (Actual_Volume = NaN)\n",
    "    \"\"\"\n",
    "    monthly = monthly.copy()\n",
    "    monthly['department_id'] = monthly['department_id'].astype(str)\n",
    "    if not pd.api.types.is_period_dtype(monthly['month']):\n",
    "        monthly['month'] = pd.PeriodIndex(monthly['month'], freq='M')\n",
    "\n",
    "    fc = fc_monthly.copy()\n",
    "    fc['department_id'] = fc['department_id'].astype(str)\n",
    "    if not pd.api.types.is_period_dtype(fc['month']):\n",
    "        fc['month'] = pd.PeriodIndex(fc['month'], freq='M')\n",
    "\n",
    "    # Define reporting range\n",
    "    start_per = pd.Period(report_start, freq='M')\n",
    "    last_actual = monthly['month'].max()\n",
    "\n",
    "    # Historical frame\n",
    "    hist = (monthly.loc[monthly['month'] >= start_per, ['department_id', 'month', 'incoming_monthly']]\n",
    "            .rename(columns={'incoming_monthly': 'Actual_Volume'}))\n",
    "    hist['Forecast'] = np.nan\n",
    "\n",
    "    # Future frame\n",
    "    fut = fc[['department_id', 'month', 'forecast_monthly',\n",
    "              'cv_prophet_smape', 'cv_arima_smape', 'cv_tbats_ets_smape',\n",
    "              'winner_model', 'blend_prophet_w', 'blend_arima_w', 'blend_tbats_ets_w']].copy()\n",
    "    fut = fut.loc[fut['month'] > last_actual]\n",
    "    fut = fut.rename(columns={'forecast_monthly': 'Forecast'})\n",
    "    fut['Actual_Volume'] = np.nan\n",
    "\n",
    "    # Union\n",
    "    base = pd.concat([hist, fut], ignore_index=True, sort=False)\n",
    "\n",
    "    # Accuracy only when both available (mostly future months will be NaN)\n",
    "    base['Forecast_Accuracy'] = np.where(\n",
    "        (base['Actual_Volume'].notna()) & (base['Forecast'].notna()) & (base['Actual_Volume'] > 0),\n",
    "        (1 - (np.abs(base['Forecast'] - base['Actual_Volume']) / base['Actual_Volume'])) * 100.0,\n",
    "        np.nan\n",
    "    )\n",
    "    return base\n",
    "\n",
    "\n",
    "def compute_capacity_monthly(cap_df: pd.DataFrame, prod_dept: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Compute FTE/day needed per month = Forecast / (avg_tickets_per_agent_day * workdays_in_month).\"\"\"\n",
    "    out = cap_df.merge(prod_dept, on='department_id', how='left')\n",
    "    out['avg_tickets_per_agent_day'] = out['avg_tickets_per_agent_day'].replace(0, np.nan)\n",
    "    out['workdays_in_month'] = [business_days_in_month(m.start_time.year, m.start_time.month) for m in out['month']]\n",
    "    out['Capacity_FTE_per_day'] = np.where(\n",
    "        (out['avg_tickets_per_agent_day'] > 0) & (out['workdays_in_month'] > 0) & (out['Forecast'].notna()),\n",
    "        out['Forecast'] / (out['avg_tickets_per_agent_day'] * out['workdays_in_month']),\n",
    "        np.nan\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "def build_cv_table(fc_monthly: pd.DataFrame, mapping: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Build mape_table_cv with sMAPE, best model and weights per department.\"\"\"\n",
    "    if fc_monthly is None or fc_monthly.empty:\n",
    "        raise ValueError(\"fc_monthly is empty; cannot build CV table.\")\n",
    "    cols_keep = [\n",
    "        'department_id',\n",
    "        'cv_prophet_smape', 'cv_arima_smape', 'cv_tbats_ets_smape',\n",
    "        'winner_model',\n",
    "        'blend_prophet_w', 'blend_arima_w', 'blend_tbats_ets_w'\n",
    "    ]\n",
    "    df = (fc_monthly[cols_keep]\n",
    "          .drop_duplicates(subset=['department_id'])\n",
    "          .copy())\n",
    "    df = df.rename(columns={\n",
    "        'cv_prophet_smape': 'sMAPE_Prophet_CV',\n",
    "        'cv_arima_smape': 'sMAPE_ARIMA_CV',\n",
    "        'cv_tbats_ets_smape': 'sMAPE_TBATS_ETS_CV',\n",
    "        'winner_model': 'Best_Model',\n",
    "        'blend_prophet_w': 'Weight_Prophet',\n",
    "        'blend_arima_w': 'Weight_ARIMA',\n",
    "        'blend_tbats_ets_w': 'Weight_TBATS_ETS',\n",
    "    })\n",
    "    df['department_id'] = df['department_id'].astype(str)\n",
    "    df = apply_mapping(df, mapping)\n",
    "    ordered_cols = [\n",
    "        'department_id', 'department_name', 'vertical',\n",
    "        'sMAPE_Prophet_CV', 'sMAPE_ARIMA_CV', 'sMAPE_TBATS_ETS_CV',\n",
    "        'Best_Model',\n",
    "        'Weight_Prophet', 'Weight_ARIMA', 'Weight_TBATS_ETS'\n",
    "    ]\n",
    "    df = df[ordered_cols]\n",
    "    return df.sort_values(['vertical', 'department_id'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42b7257",
   "metadata": {},
   "source": [
    "## 05. Daily plan (reconciled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7263fb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dow_profile(g: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Build normalized day-of-week profile for a department, fallback to uniform.\"\"\"\n",
    "    prof = (g.assign(dow=g['Date'].dt.dayofweek)\n",
    "              .groupby('dow')['ticket_total']\n",
    "              .mean())\n",
    "    if prof.notna().sum() >= 3:\n",
    "        prof = prof / prof.mean()\n",
    "    else:\n",
    "        prof = pd.Series(1.0, index=range(7))\n",
    "    return prof\n",
    "\n",
    "\n",
    "def disaggregate_month_to_days(dept_df: pd.DataFrame,\n",
    "                               month_period: pd.Period,\n",
    "                               target_sum: float) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Allocate monthly forecast to each day in that month using recent DOW profile,\n",
    "    guaranteeing that daily sum equals the monthly target.\n",
    "    \"\"\"\n",
    "    # Date range for the target month\n",
    "    start = month_period.start_time\n",
    "    end = month_period.end_time\n",
    "    days = pd.date_range(start=start, end=end, freq='D')\n",
    "\n",
    "    # Build recent DOW profile from last 90 actual days\n",
    "    hist = dept_df.sort_values('Date').tail(90)\n",
    "    profile = dow_profile(hist)\n",
    "\n",
    "    weights = np.array([profile.get(d.dayofweek, 1.0) for d in days], dtype=float)\n",
    "    weights = np.maximum(weights, 1e-6)\n",
    "    weights = weights / weights.sum()\n",
    "\n",
    "    alloc = target_sum * weights\n",
    "    return pd.DataFrame({'Date': days, 'forecast_daily': alloc})\n",
    "\n",
    "\n",
    "def build_daily_from_monthly(incoming: pd.DataFrame,\n",
    "                             fc_monthly: pd.DataFrame,\n",
    "                             horizon_days: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Top-down daily plan:\n",
    "    - For each department and future month within horizon window,\n",
    "      disaggregate monthly forecast into daily using DOW profile.\n",
    "    \"\"\"\n",
    "    last_date = incoming['Date'].max()\n",
    "    start = last_date + pd.Timedelta(days=1)\n",
    "    end = start + pd.Timedelta(days=horizon_days - 1)\n",
    "    future_months = pd.period_range(start=start.to_period('M'),\n",
    "                                    end=end.to_period('M'), freq='M')\n",
    "\n",
    "    rows = []\n",
    "    for dept, g in incoming.groupby('department_id'):\n",
    "        for m in future_months:\n",
    "            fcm = fc_monthly[(fc_monthly['department_id'] == dept) & (fc_monthly['month'] == m)]\n",
    "            if fcm.empty:\n",
    "                continue\n",
    "            target = float(fcm['forecast_monthly'].iloc[0])\n",
    "            if target <= 0:\n",
    "                continue\n",
    "            alloc_df = disaggregate_month_to_days(g, m, target)\n",
    "            # Keep only days inside the [start, end] window\n",
    "            alloc_df = alloc_df[(alloc_df['Date'] >= start) & (alloc_df['Date'] <= end)]\n",
    "            alloc_df.insert(0, 'department_id', dept)\n",
    "            rows.append(alloc_df)\n",
    "\n",
    "    df = pd.concat(rows, ignore_index=True) if rows else pd.DataFrame(columns=['department_id', 'Date', 'forecast_daily'])\n",
    "    # Fill any missing dept-days (rare) with a small epsilon 0\n",
    "    return df\n",
    "\n",
    "\n",
    "def split_daily_by_language(df_daily_fc: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Split daily forecast by fixed language shares.\"\"\"\n",
    "    parts = []\n",
    "    for lang, w in LANGUAGE_SHARES.items():\n",
    "        tmp = df_daily_fc.copy()\n",
    "        tmp['language'] = lang\n",
    "        tmp['forecast_daily_language'] = tmp['forecast_daily'] * w\n",
    "        parts.append(tmp)\n",
    "    out = pd.concat(parts, ignore_index=True)\n",
    "    return out\n",
    "\n",
    "\n",
    "def build_daily_capacity_plan(incoming: pd.DataFrame,\n",
    "                              mapping: pd.DataFrame,\n",
    "                              prod_dept: pd.DataFrame,\n",
    "                              fc_monthly: pd.DataFrame,\n",
    "                              horizon_days: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    End-to-end daily plan:\n",
    "    - If USE_DAILY_FROM_MONTHLY: disaggregate monthly forecast (reconciled)\n",
    "    - Else: robust daily baseline (28D moving average)\n",
    "    - Split by languages\n",
    "    - Attach department_name / vertical\n",
    "    - Compute FTE per day per department/language\n",
    "    \"\"\"\n",
    "    if USE_DAILY_FROM_MONTHLY:\n",
    "        daily_fc = build_daily_from_monthly(incoming, fc_monthly, horizon_days)\n",
    "    else:\n",
    "        daily_fc = forecast_daily_baseline(incoming, horizon_days)\n",
    "\n",
    "    daily_fc_lang = split_daily_by_language(daily_fc)\n",
    "\n",
    "    # Attach names/vertical\n",
    "    daily_fc_lang = apply_mapping(daily_fc_lang, mapping)\n",
    "\n",
    "    # Merge productivity\n",
    "    daily_fc_lang = daily_fc_lang.merge(prod_dept, on='department_id', how='left')\n",
    "\n",
    "    # Compute FTE requirement per day\n",
    "    daily_fc_lang['avg_tickets_per_agent_day'] = pd.to_numeric(daily_fc_lang['avg_tickets_per_agent_day'], errors='coerce')\n",
    "    daily_fc_lang['FTE_per_day'] = np.where(\n",
    "        daily_fc_lang['avg_tickets_per_agent_day'] > 0,\n",
    "        daily_fc_lang['forecast_daily_language'] / daily_fc_lang['avg_tickets_per_agent_day'],\n",
    "        np.nan\n",
    "    )\n",
    "    cols = ['Date', 'department_id', 'department_name', 'vertical', 'language',\n",
    "            'forecast_daily_language', 'FTE_per_day']\n",
    "    daily_plan = daily_fc_lang[cols].sort_values(['Date', 'vertical', 'department_id', 'language'])\n",
    "    return daily_plan\n",
    "\n",
    "\n",
    "def forecast_daily_baseline(df_daily: pd.DataFrame, horizon_days: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Independent daily baseline (kept as optional):\n",
    "    - If >=28 days: 28-day moving average per department with DOW profile\n",
    "    - Else: historical average\n",
    "    \"\"\"\n",
    "    df = df_daily.copy()\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df['department_id'] = df['department_id'].astype(str).str.strip()\n",
    "    df = df.sort_values(['department_id', 'Date'])\n",
    "    last_date = df['Date'].max()\n",
    "    if pd.isna(last_date):\n",
    "        raise ValueError(\"forecast_daily_baseline: No valid dates in incoming.\")\n",
    "    start = last_date + pd.Timedelta(days=1)\n",
    "    idx_future = pd.date_range(start=start, periods=horizon_days, freq='D')\n",
    "\n",
    "    rows = []\n",
    "    for dept, g in df.groupby('department_id'):\n",
    "        g = g.sort_values('Date')\n",
    "        if len(g) >= 28:\n",
    "            roll_mean = (g.set_index('Date')['ticket_total']\n",
    "                         .rolling(window=28, min_periods=1)\n",
    "                         .mean()\n",
    "                         .iloc[-1])\n",
    "            base = float(roll_mean) if np.isfinite(roll_mean) else float(g['ticket_total'].mean())\n",
    "        else:\n",
    "            base = float(g['ticket_total'].mean())\n",
    "\n",
    "        prof = dow_profile(g)\n",
    "        vals = []\n",
    "        for d in idx_future:\n",
    "            w = prof[d.dayofweek] if d.dayofweek in prof.index else 1.0\n",
    "            vals.append(max(0.0, base * float(w)))\n",
    "        rows.append(pd.DataFrame({'department_id': dept, 'Date': idx_future, 'forecast_daily': vals}))\n",
    "\n",
    "    return pd.concat(rows, ignore_index=True) if rows else pd.DataFrame(columns=['department_id', 'Date', 'forecast_daily'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b39785",
   "metadata": {},
   "source": [
    "## 06. Main entry point and excel writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8345017",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:05:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:05:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:05:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:05:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:06:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:06:49 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:07:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:07:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:07:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:07:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:07:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:07:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:08:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:08:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:08:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:08:43 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:10:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:10:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:10:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:10:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:11:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:11:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:12:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:12:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:13:19 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:13:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:13:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:13:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:15:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:15:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:16:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:16:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:16:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:16:41 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:17:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:17:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:17:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:17:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:18:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:18:18 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:19:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:19:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:19:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:20:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:20:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:20:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:21:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:21:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:22:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:22:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:22:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:22:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:23:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:23:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:23:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:23:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:24:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:24:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:26:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:26:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:27:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:27:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:27:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:27:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:28:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:28:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:28:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:28:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:29:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:29:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:31:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:31:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:31:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:31:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:32:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:32:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:34:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:34:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:35:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:35:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:35:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:35:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:38:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:38:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:38:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:38:41 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:39:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:39:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:39:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:40:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:40:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:40:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:41:19 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:41:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:41:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:41:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:42:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:42:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:43:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:43:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:43:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:43:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:45:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:45:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:46:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:46:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:46:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:46:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:48:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:49:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:49:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:49:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:50:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:50:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "13:50:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:50:29 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel written: C:\\Users\\pt3canro\\Desktop\\CAPACITY\\outputs\\capacity_forecast_hybrid.xlsx\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # 1) Load inputs\n",
    "    incoming = load_incoming(INCOMING_SOURCE_PATH, sheet_name=INCOMING_SHEET)\n",
    "    mapping = load_dept_map(DEPT_MAP_PATH, DEPT_MAP_SHEET)\n",
    "    prod = load_productivity(PRODUCTIVITY_PATH)\n",
    "\n",
    "    # 2) Monthly forecast\n",
    "    monthly = build_monthly_series(incoming)\n",
    "    fc_monthly = forecast_per_department_monthly(monthly)\n",
    "\n",
    "    # 3) capacity_error (historicals + future forecast)\n",
    "    cap_err = compute_monthly_accuracy_with_history(monthly, fc_monthly, REPORT_START_MONTH)\n",
    "    cap_err = compute_capacity_monthly(cap_err, prod)\n",
    "    cap_err = apply_mapping(cap_err, mapping)\n",
    "\n",
    "    # 4) Daily plan (reconciled with monthly by default)\n",
    "    daily_capacity_plan = build_daily_capacity_plan(incoming, mapping, prod, fc_monthly, DAILY_HORIZON_DAYS)\n",
    "\n",
    "    # 5) CV table\n",
    "    cv_table = build_cv_table(fc_monthly, mapping)\n",
    "\n",
    "    # 6) Write Excel with required sheet names\n",
    "    with pd.ExcelWriter(OUTPUT_XLSX, engine=\"openpyxl\", mode=\"w\") as w:\n",
    "        # Rename monthly sheet exactly as requested\n",
    "        cap_err[['vertical', 'department_id', 'department_name', 'month',\n",
    "                 'Actual_Volume', 'Forecast', 'Forecast_Accuracy',\n",
    "                 'Capacity_FTE_per_day',\n",
    "                 'winner_model', 'cv_prophet_smape', 'cv_arima_smape', 'cv_tbats_ets_smape',\n",
    "                 'blend_prophet_w', 'blend_arima_w', 'blend_tbats_ets_w']]\\\n",
    "            .sort_values(['vertical', 'department_id', 'month']).to_excel(w, \"capacity_error\", index=False)\n",
    "\n",
    "        daily_capacity_plan.to_excel(w, \"daily_capacity_plan\", index=False)\n",
    "        cv_table.to_excel(w, \"mape_table_cv\", index=False)\n",
    "\n",
    "    print(\"Excel written:\", OUTPUT_XLSX)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
