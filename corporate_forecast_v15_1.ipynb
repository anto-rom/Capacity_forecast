{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42e849b4",
   "metadata": {},
   "source": [
    "# Planet | Capacity Forecast v15 (Stability‑First, Auditable)\n",
    "\n",
    "This notebook provides a **clean, minimal, and fully auditable** capacity forecasting pipeline.\n",
    "\n",
    "**Scope**\n",
    "- **Monthly forecast**: 12 months ahead (department-level, allocated from vertical forecasts)\n",
    "- **Daily plan**: 90 days ahead (department-level) using a historical **day-of-week (DOW) profile** (monthly forecast remains untouched)\n",
    "- **Languages**: Only `English, Spanish, Portuguese, French, German, Italian`. Any other language is mapped to **English**.\n",
    "\n",
    "**Modeling strategy (stability-first)**\n",
    "1. Aggregate incoming tickets to **monthly volumes per vertical**\n",
    "2. Forecast each vertical using **ETS (ExponentialSmoothing) only**\n",
    "3. Apply **vertical level recalibration** (last 3 months actual vs fitted)\n",
    "4. Allocate vertical forecast to departments using **EWMA shares** (renormalized per vertical-month)\n",
    "5. Compute **dept accuracy** via a clean rolling backtest (WAPE → Accuracy_staffing_%)\n",
    "\n",
    "Output Excel is saved under `outputs/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b84f898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR: C:\\Projects\\Capacity_forecast_2026\n",
      "INPUT_DIR: C:\\Projects\\Capacity_forecast_2026\\input_model\n",
      "OUTPUT_DIR: C:\\Projects\\Capacity_forecast_2026\\outputs\n",
      "Output file: C:\\Projects\\Capacity_forecast_2026\\outputs\\capacity_forecast_v15.xlsx\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 0) Setup (interactive folder selector)\n",
    "# -----------------------------\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = r\"C:\\Projects\\Capacity_forecast_2026\"\n",
    "BASE_DIR = str(Path(BASE_DIR).expanduser().resolve())\n",
    "\n",
    "INPUT_DIR  = str(Path(BASE_DIR) / \"input_model\")\n",
    "OUTPUT_DIR = str(Path(BASE_DIR) / \"outputs\")\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "INCOMING_SOURCE_PATH = os.path.join(INPUT_DIR, \"Incoming_new.xlsx\")  # Sheet 'Main'\n",
    "INCOMING_SHEET = \"Main\"\n",
    "\n",
    "DEPT_MAP_PATH = os.path.join(INPUT_DIR, \"department.xlsx\")\n",
    "DEPT_MAP_SHEET = \"map\"\n",
    "\n",
    "PRODUCTIVITY_PATH = os.path.join(INPUT_DIR, \"productivity_agents.xlsx\")  # optional\n",
    "\n",
    "OUTPUT_XLSX = os.path.join(OUTPUT_DIR, \"capacity_forecast_v15.xlsx\")\n",
    "\n",
    "# Horizons\n",
    "H_MONTHS = 12\n",
    "DAILY_HORIZON_DAYS = 90\n",
    "\n",
    "# Prediction interval\n",
    "PI_ALPHA = 0.05  # 95% PI\n",
    "\n",
    "# Backtest settings\n",
    "BT_MIN_TRAIN_MONTHS = 12\n",
    "BT_EVAL_MONTHS = 9\n",
    "BT_HORIZON_MONTHS = 1\n",
    "BT_MAX_SPLITS = 9\n",
    "\n",
    "# Governance\n",
    "SUPPORTED_LANGUAGES = [\"English\",\"Spanish\",\"Portuguese\",\"French\",\"German\",\"Italian\"]\n",
    "DEFAULT_LANGUAGE = \"English\"\n",
    "\n",
    "CRITICAL_VERTICALS = [\"Payments\",\"Hospitality\",\"Partners\"]  # adjust if needed\n",
    "\n",
    "VERTICAL_LEVEL_ADJ = {\n",
    "    \"enabled\": True,\n",
    "    \"lookback_months\": 3,\n",
    "    \"clip_min\": 0.70,\n",
    "    \"clip_max\": 1.10,\n",
    "}\n",
    "\n",
    "DEPT_SHARE_EWMA_ALPHA = 0.50\n",
    "\n",
    "# DOW profile\n",
    "DOW_LOOKBACK_DAYS = 180\n",
    "DOW_MIN_OBS = 30\n",
    "WEEKEND_OPEN_THRESHOLD = 0.05  # if weekend share < 5%, treat dept as closed weekends for daily plan\n",
    "\n",
    "print(\"BASE_DIR:\", BASE_DIR)\n",
    "print(\"INPUT_DIR:\", INPUT_DIR)\n",
    "print(\"OUTPUT_DIR:\", OUTPUT_DIR)\n",
    "print(\"Output file:\", OUTPUT_XLSX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcfa12ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1) Imports\n",
    "# -----------------------------\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d050d7e9",
   "metadata": {},
   "source": [
    "## 2) Data loaders & standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6f1a246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_incoming(path: str, sheet: str = \"Main\") -> pd.DataFrame:\n",
    "    df = pd.read_excel(path, sheet_name=sheet)\n",
    "\n",
    "    if \"Date\" not in df.columns:\n",
    "        for c in [\"date\", \"created_date\", \"created_at\"]:\n",
    "            if c in df.columns:\n",
    "                df = df.rename(columns={c: \"Date\"})\n",
    "                break\n",
    "    if \"Date\" not in df.columns:\n",
    "        raise ValueError(\"Incoming_new.xlsx must contain a 'Date' column (or a recognizable alternative).\")\n",
    "\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "\n",
    "    # Ticket weight: if absent, each row is 1 ticket\n",
    "    if \"ticket_total\" not in df.columns:\n",
    "        df[\"ticket_total\"] = 1.0\n",
    "    else:\n",
    "        df[\"ticket_total\"] = pd.to_numeric(df[\"ticket_total\"], errors=\"coerce\").fillna(1.0)\n",
    "\n",
    "    for col in [\"department_id\",\"department_name\",\"vertical\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = np.nan\n",
    "\n",
    "    if \"language\" not in df.columns:\n",
    "        df[\"language\"] = DEFAULT_LANGUAGE\n",
    "    df[\"language\"] = df[\"language\"].fillna(DEFAULT_LANGUAGE).astype(str)\n",
    "    df.loc[~df[\"language\"].isin(SUPPORTED_LANGUAGES), \"language\"] = DEFAULT_LANGUAGE\n",
    "\n",
    "    df[\"department_id\"] = df[\"department_id\"].astype(str)\n",
    "    return df\n",
    "\n",
    "def load_dept_map(path: str, sheet: str = \"map\") -> pd.DataFrame:\n",
    "    m = pd.read_excel(path, sheet_name=sheet)\n",
    "    if \"department_id\" not in m.columns:\n",
    "        raise ValueError(\"department.xlsx must contain 'department_id'\")\n",
    "    m[\"department_id\"] = m[\"department_id\"].astype(str)\n",
    "    if \"vertical\" not in m.columns and \"vertical_name\" in m.columns:\n",
    "        m = m.rename(columns={\"vertical_name\":\"vertical\"})\n",
    "    return m\n",
    "\n",
    "def load_productivity(path: str) -> pd.DataFrame:\n",
    "    p = pd.read_excel(path)\n",
    "    if \"Date\" not in p.columns:\n",
    "        raise ValueError(\"productivity_agents.xlsx must contain 'Date'\")\n",
    "    p[\"Date\"] = pd.to_datetime(p[\"Date\"])\n",
    "    if \"prod_total_model\" in p.columns:\n",
    "        p[\"prod_total_model\"] = pd.to_numeric(p[\"prod_total_model\"], errors=\"coerce\").fillna(0.0)\n",
    "    else:\n",
    "        p[\"prod_total_model\"] = 0.0\n",
    "    for col in [\"department_id\",\"department_name\"]:\n",
    "        if col not in p.columns:\n",
    "            p[col] = np.nan\n",
    "    p[\"department_id\"] = p[\"department_id\"].astype(str)\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8060f60",
   "metadata": {},
   "source": [
    "## 3) Monthly aggregates & shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "088a375b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monthly_vertical_series(incoming: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = incoming.copy()\n",
    "    d[\"month\"] = d[\"Date\"].dt.to_period(\"M\")\n",
    "    vm = d.groupby([\"vertical\",\"month\"], as_index=False)[\"ticket_total\"].sum()\n",
    "    return vm.sort_values([\"vertical\",\"month\"])\n",
    "\n",
    "def monthly_dept_series(incoming: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = incoming.copy()\n",
    "    d[\"month\"] = d[\"Date\"].dt.to_period(\"M\")\n",
    "    dm = d.groupby([\"vertical\",\"department_id\",\"month\"], as_index=False)[\"ticket_total\"].sum()\n",
    "    return dm.sort_values([\"vertical\",\"department_id\",\"month\"])\n",
    "\n",
    "def dept_share_ewma_within_vertical(incoming: pd.DataFrame, alpha: float = 0.5) -> pd.DataFrame:\n",
    "    dm = monthly_dept_series(incoming)\n",
    "    vm = monthly_vertical_series(incoming).rename(columns={\"ticket_total\":\"vertical_total\"})\n",
    "    x = dm.merge(vm, on=[\"vertical\",\"month\"], how=\"left\")\n",
    "    x[\"share\"] = np.where(x[\"vertical_total\"]>0, x[\"ticket_total\"]/x[\"vertical_total\"], 0.0)\n",
    "    x = x.sort_values([\"vertical\",\"department_id\",\"month\"])\n",
    "\n",
    "    x[\"share_ewma\"] = x.groupby([\"vertical\",\"department_id\"])[\"share\"].transform(\n",
    "        lambda s: s.ewm(alpha=alpha, adjust=False).mean()\n",
    "    )\n",
    "    denom = x.groupby([\"vertical\",\"month\"])[\"share_ewma\"].transform(\"sum\")\n",
    "    x[\"share_final\"] = np.where(denom>0, x[\"share_ewma\"]/denom, 0.0)\n",
    "\n",
    "    return x[[\"vertical\",\"department_id\",\"month\",\"share_final\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea367977",
   "metadata": {},
   "source": [
    "## 4) Vertical forecasting (ETS-only) + Level recalibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "397ddc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ets_monthly(y_ts: pd.Series):\n",
    "    y_ts = pd.Series(y_ts).dropna().astype(float).sort_index()\n",
    "    if len(y_ts) < 6:\n",
    "        return None, float(y_ts.std()) if len(y_ts) else 0.0\n",
    "\n",
    "    try:\n",
    "        seasonal = \"add\" if len(y_ts) >= 24 else None\n",
    "        sp = 12 if seasonal else None\n",
    "        model = ExponentialSmoothing(\n",
    "            y_ts,\n",
    "            trend=\"add\",\n",
    "            seasonal=seasonal,\n",
    "            seasonal_periods=sp,\n",
    "            initialization_method=\"estimated\"\n",
    "        )\n",
    "        fit = model.fit(optimized=True)\n",
    "        resid = (y_ts - fit.fittedvalues).dropna()\n",
    "        sigma = float(resid.std()) if len(resid) else float(y_ts.std())\n",
    "        return fit, sigma\n",
    "    except Exception:\n",
    "        return None, float(y_ts.std()) if len(y_ts) else 0.0\n",
    "\n",
    "def forecast_vertical_final(incoming: pd.DataFrame, periods: int = 12, alpha: float = 0.05) -> pd.DataFrame:\n",
    "    vm = monthly_vertical_series(incoming)\n",
    "    rows = []\n",
    "\n",
    "    lookback = int(VERTICAL_LEVEL_ADJ[\"lookback_months\"])\n",
    "    clip_min = float(VERTICAL_LEVEL_ADJ[\"clip_min\"])\n",
    "    clip_max = float(VERTICAL_LEVEL_ADJ[\"clip_max\"])\n",
    "    enabled = bool(VERTICAL_LEVEL_ADJ[\"enabled\"])\n",
    "    z = 1.96  # ~95%\n",
    "\n",
    "    for v, g in vm.groupby(\"vertical\"):\n",
    "        y = g.set_index(\"month\")[\"ticket_total\"].sort_index()\n",
    "        y_ts = y.copy()\n",
    "        y_ts.index = y_ts.index.to_timestamp()\n",
    "\n",
    "        fit, sigma = fit_ets_monthly(y_ts)\n",
    "\n",
    "        last_m = y.index.max()\n",
    "        future_m = pd.period_range(last_m + 1, periods=periods, freq=\"M\")\n",
    "        future_idx = future_m.to_timestamp()\n",
    "\n",
    "        if fit is None:\n",
    "            last = float(y.iloc[-1]) if len(y) else 0.0\n",
    "            fc = pd.Series([last]*periods, index=future_idx)\n",
    "            fitted = pd.Series([last]*len(y_ts), index=y_ts.index)\n",
    "        else:\n",
    "            fc_vals = fit.forecast(periods)\n",
    "            fc = pd.Series(fc_vals.values, index=future_idx)\n",
    "            fitted = fit.fittedvalues\n",
    "\n",
    "        fc = fc.clip(lower=0.0)\n",
    "        p05 = (fc - z*sigma).clip(lower=0.0)\n",
    "        p95 = (fc + z*sigma).clip(lower=0.0)\n",
    "\n",
    "        factor = 1.0\n",
    "        if enabled and (v in CRITICAL_VERTICALS) and len(y_ts) >= lookback:\n",
    "            actual_last = float(y_ts.tail(lookback).mean())\n",
    "            fitted_last = float(fitted.tail(lookback).mean()) if len(fitted) else actual_last\n",
    "            if fitted_last > 0:\n",
    "                factor = float(np.clip(actual_last / fitted_last, clip_min, clip_max))\n",
    "\n",
    "        rows.append(pd.DataFrame({\n",
    "            \"vertical\": v,\n",
    "            \"month\": future_m,\n",
    "            \"forecast_monthly_vertical\": (fc*factor).values,\n",
    "            \"forecast_p05_vertical\": (p05*factor).values,\n",
    "            \"forecast_p95_vertical\": (p95*factor).values,\n",
    "            \"vertical_level_factor\": factor,\n",
    "            \"model_used\": \"ETS\"\n",
    "        }))\n",
    "\n",
    "    return pd.concat(rows, ignore_index=True) if rows else pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea35c175",
   "metadata": {},
   "source": [
    "## 5) Allocation to department (EWMA shares, renormalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef08e9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def allocate_vertical_to_dept(fc_vertical: pd.DataFrame, shares_vd: pd.DataFrame) -> pd.DataFrame:\n",
    "    last_share = shares_vd.sort_values(\"month\").groupby([\"vertical\",\"department_id\"]).tail(1)\n",
    "    last_share = last_share[[\"vertical\",\"department_id\",\"share_final\"]].rename(columns={\"share_final\":\"share_cf\"})\n",
    "\n",
    "    fc = fc_vertical.copy()\n",
    "    fc = fc.merge(last_share, on=\"vertical\", how=\"left\")  # expands to depts per vertical\n",
    "    fc[\"share_cf\"] = fc[\"share_cf\"].fillna(0.0)\n",
    "\n",
    "    out = fc.assign(\n",
    "        forecast_monthly_dept = fc[\"forecast_monthly_vertical\"] * fc[\"share_cf\"],\n",
    "        forecast_p05_dept     = fc[\"forecast_p05_vertical\"]     * fc[\"share_cf\"],\n",
    "        forecast_p95_dept     = fc[\"forecast_p95_vertical\"]     * fc[\"share_cf\"],\n",
    "    )\n",
    "    return out[[\"vertical\",\"department_id\",\"month\",\n",
    "                \"forecast_monthly_dept\",\"forecast_p05_dept\",\"forecast_p95_dept\",\n",
    "                \"vertical_level_factor\",\"model_used\",\"share_cf\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b7b63b",
   "metadata": {},
   "source": [
    "## 6) Daily plan (90d) using DOW profile (monthly remains untouched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a19fb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dept_dow_profile(incoming: pd.DataFrame, lookback_days: int = 180, min_obs: int = 30) -> pd.DataFrame:\n",
    "    d = incoming.copy()\n",
    "    d = d[d[\"Date\"] >= (d[\"Date\"].max() - pd.Timedelta(days=lookback_days))]\n",
    "    daily = d.groupby([\"department_id\",\"Date\"], as_index=False)[\"ticket_total\"].sum()\n",
    "    daily[\"dow\"] = pd.to_datetime(daily[\"Date\"]).dt.dayofweek\n",
    "\n",
    "    prof = daily.groupby([\"department_id\",\"dow\"], as_index=False)[\"ticket_total\"].sum()\n",
    "    tot = prof.groupby(\"department_id\")[\"ticket_total\"].transform(\"sum\")\n",
    "    prof[\"dow_share\"] = np.where(tot>0, prof[\"ticket_total\"]/tot, 0.0)\n",
    "\n",
    "    idx = pd.MultiIndex.from_product([prof[\"department_id\"].unique(), range(7)], names=[\"department_id\",\"dow\"])\n",
    "    prof = prof.set_index([\"department_id\",\"dow\"]).reindex(idx).reset_index()\n",
    "    prof[\"dow_share\"] = prof[\"dow_share\"].fillna(0.0)\n",
    "\n",
    "    obs = daily.groupby(\"department_id\")[\"Date\"].nunique().rename(\"n_days\").reset_index()\n",
    "    prof = prof.merge(obs, on=\"department_id\", how=\"left\")\n",
    "\n",
    "    def _fix(g):\n",
    "        if float(g[\"n_days\"].iloc[0] or 0) < min_obs:\n",
    "            g[\"dow_share\"] = 0.0\n",
    "            g.loc[g[\"dow\"].isin([0,1,2,3,4]), \"dow_share\"] = 0.2\n",
    "        else:\n",
    "            s = g[\"dow_share\"].sum()\n",
    "            if s > 0:\n",
    "                g[\"dow_share\"] = g[\"dow_share\"] / s\n",
    "        return g\n",
    "\n",
    "    prof = prof.groupby(\"department_id\", group_keys=False).apply(_fix)\n",
    "    return prof[[\"department_id\",\"dow\",\"dow_share\"]]\n",
    "\n",
    "def daily_plan_from_monthly(fc_dept_monthly: pd.DataFrame, dow_profile: pd.DataFrame,\n",
    "                            start_date: Optional[pd.Timestamp] = None, horizon_days: int = 90) -> pd.DataFrame:\n",
    "    if start_date is None:\n",
    "        start_date = pd.Timestamp.today().normalize()\n",
    "\n",
    "    end_date = start_date + pd.Timedelta(days=horizon_days-1)\n",
    "    days = pd.date_range(start_date, end_date, freq=\"D\")\n",
    "\n",
    "    day_df = pd.DataFrame({\"Date\": days})\n",
    "    day_df[\"month\"] = day_df[\"Date\"].dt.to_period(\"M\")\n",
    "    day_df[\"dow\"] = day_df[\"Date\"].dt.dayofweek\n",
    "\n",
    "    m = fc_dept_monthly.copy()\n",
    "    out = m.merge(day_df, on=\"month\", how=\"right\")\n",
    "    out[\"department_id\"] = out[\"department_id\"].astype(str)\n",
    "\n",
    "    out = out.merge(dow_profile, on=[\"department_id\",\"dow\"], how=\"left\")\n",
    "    out[\"dow_share\"] = out[\"dow_share\"].fillna(0.0)\n",
    "\n",
    "    wk = dow_profile.copy()\n",
    "    wk[\"is_weekend\"] = wk[\"dow\"].isin([5,6])\n",
    "    wk_sum = wk.groupby(\"department_id\").apply(lambda g: float(g.loc[g[\"is_weekend\"],\"dow_share\"].sum())).rename(\"weekend_share\").reset_index()\n",
    "    out = out.merge(wk_sum, on=\"department_id\", how=\"left\")\n",
    "    out[\"weekend_share\"] = out[\"weekend_share\"].fillna(0.0)\n",
    "\n",
    "    closed = out[\"weekend_share\"] < WEEKEND_OPEN_THRESHOLD\n",
    "    out.loc[closed & out[\"dow\"].isin([5,6]), \"dow_share\"] = 0.0\n",
    "\n",
    "    denom = out.groupby([\"department_id\",\"month\"])[\"dow_share\"].transform(\"sum\")\n",
    "    out[\"dow_share_adj\"] = np.where(denom>0, out[\"dow_share\"]/denom, 0.0)\n",
    "\n",
    "    out[\"forecast_daily_dept\"] = out[\"forecast_monthly_dept\"] * out[\"dow_share_adj\"]\n",
    "    out[\"p05_daily_dept\"]      = out[\"forecast_p05_dept\"]     * out[\"dow_share_adj\"]\n",
    "    out[\"p95_daily_dept\"]      = out[\"forecast_p95_dept\"]     * out[\"dow_share_adj\"]\n",
    "\n",
    "    return out[[\"Date\",\"vertical\",\"department_id\",\"month\",\"dow\",\n",
    "                \"forecast_daily_dept\",\"p05_daily_dept\",\"p95_daily_dept\",\n",
    "                \"forecast_monthly_dept\",\"forecast_p05_dept\",\"forecast_p95_dept\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10bc674",
   "metadata": {},
   "source": [
    "## 7) Backtest (clean, same pipeline, WAPE → Accuracy_staffing_%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ecca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_wape(y_true: pd.Series, y_pred: pd.Series) -> float:\n",
    "    y_true = pd.Series(y_true).astype(float)\n",
    "    y_pred = pd.Series(y_pred).astype(float)\n",
    "    denom = float(y_true.sum())\n",
    "    if denom <= 0:\n",
    "        return np.nan\n",
    "    return float(np.abs(y_true - y_pred).sum() / denom)\n",
    "\n",
    "\n",
    "def backtest_dept_accuracy(\n",
    "    incoming: pd.DataFrame,\n",
    "    min_train_months: int = 12,\n",
    "    eval_months: int = 9,\n",
    "    horizon_months: int = 1,\n",
    "    max_splits: int = 9,\n",
    ") -> pd.DataFrame:\n",
    "    inc = incoming.copy()\n",
    "\n",
    "    # Asegura Date datetime\n",
    "    if \"Date\" not in inc.columns:\n",
    "        raise ValueError(\"incoming must contain 'Date' column\")\n",
    "    inc[\"Date\"] = pd.to_datetime(inc[\"Date\"], errors=\"coerce\")\n",
    "    inc = inc.dropna(subset=[\"Date\"])\n",
    "\n",
    "    # Month as Period[M]\n",
    "    inc[\"month\"] = inc[\"Date\"].dt.to_period(\"M\")\n",
    "\n",
    "    # Validaciones mínimas\n",
    "    for c in [\"department_id\", \"ticket_total\"]:\n",
    "        if c not in inc.columns:\n",
    "            raise ValueError(f\"incoming must contain '{c}' column\")\n",
    "\n",
    "    all_months = sorted(inc[\"month\"].unique())\n",
    "    if len(all_months) < (min_train_months + horizon_months + 1):\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    eval_targets = all_months[-eval_months:]\n",
    "    splits = eval_targets[-max_splits:]\n",
    "    results = []\n",
    "\n",
    "    # Cache dept->vertical por si falta vertical en test/fc\n",
    "    dept_vertical = None\n",
    "    if \"vertical\" in inc.columns:\n",
    "        dept_vertical = inc[[\"department_id\", \"vertical\"]].drop_duplicates(\"department_id\").copy()\n",
    "\n",
    "    for target_month in splits:\n",
    "        train_end = target_month - horizon_months\n",
    "        train = inc[inc[\"month\"] <= train_end].copy()\n",
    "        test  = inc[inc[\"month\"] == target_month].copy()\n",
    "\n",
    "        if train[\"month\"].nunique() < min_train_months:\n",
    "            continue\n",
    "\n",
    "        # Shares + forecast vertical\n",
    "        shares_vd = dept_share_ewma_within_vertical(train, alpha=DEPT_SHARE_EWMA_ALPHA)\n",
    "        fc_vert = forecast_vertical_final(train, periods=horizon_months, alpha=PI_ALPHA).copy()\n",
    "\n",
    "        # Normaliza month para comparar (Period[M])\n",
    "        if \"month\" in fc_vert.columns:\n",
    "            if not pd.api.types.is_period_dtype(fc_vert[\"month\"]):\n",
    "                fc_vert[\"month\"] = pd.to_datetime(fc_vert[\"month\"], errors=\"coerce\").dt.to_period(\"M\")\n",
    "\n",
    "        fc_vert = fc_vert[fc_vert[\"month\"] == target_month].copy()\n",
    "        if fc_vert.empty:\n",
    "            continue\n",
    "\n",
    "        # Allocation dept\n",
    "        fc_dept = allocate_vertical_to_dept(fc_vert, shares_vd)\n",
    "\n",
    "        # --- Guardrail: vertical en fc_dept ---\n",
    "        if \"vertical\" not in fc_dept.columns:\n",
    "            if \"vertical\" in shares_vd.columns:\n",
    "                vd = shares_vd[[\"department_id\", \"vertical\"]].drop_duplicates(\"department_id\")\n",
    "                fc_dept = fc_dept.merge(vd, on=\"department_id\", how=\"left\")\n",
    "\n",
    "        if \"vertical\" not in fc_dept.columns and dept_vertical is not None:\n",
    "            fc_dept = fc_dept.merge(dept_vertical, on=\"department_id\", how=\"left\")\n",
    "\n",
    "        if \"vertical\" not in fc_dept.columns:\n",
    "            fc_dept[\"vertical\"] = \"Unknown\"\n",
    "\n",
    "        fc_dept[\"vertical\"] = fc_dept[\"vertical\"].fillna(\"Unknown\").astype(str)\n",
    "\n",
    "        # --- Guardrail: vertical en test ---\n",
    "        if \"vertical\" not in test.columns:\n",
    "            if dept_vertical is not None:\n",
    "                test = test.merge(dept_vertical, on=\"department_id\", how=\"left\")\n",
    "\n",
    "        if \"vertical\" not in test.columns:\n",
    "            test[\"vertical\"] = \"Unknown\"\n",
    "\n",
    "        test[\"vertical\"] = test[\"vertical\"].fillna(\"Unknown\").astype(str)\n",
    "\n",
    "        # Aggregate actual vs forecast\n",
    "        actual = (\n",
    "            test.groupby([\"vertical\", \"department_id\"], as_index=False)[\"ticket_total\"]\n",
    "            .sum()\n",
    "            .rename(columns={\"ticket_total\": \"actual\"})\n",
    "        )\n",
    "\n",
    "        if \"forecast_monthly_dept\" not in fc_dept.columns:\n",
    "            raise ValueError(\n",
    "                \"fc_dept must contain 'forecast_monthly_dept'. \"\n",
    "                f\"Columns={list(fc_dept.columns)}\"\n",
    "            )\n",
    "\n",
    "        pred = (\n",
    "            fc_dept.groupby([\"vertical\", \"department_id\"], as_index=False)[\"forecast_monthly_dept\"]\n",
    "            .sum()\n",
    "            .rename(columns={\"forecast_monthly_dept\": \"forecast\"})\n",
    "        )\n",
    "\n",
    "        m = actual.merge(pred, on=[\"vertical\", \"department_id\"], how=\"outer\").fillna(0.0)\n",
    "        m[\"month\"] = target_month\n",
    "        results.append(m)\n",
    "\n",
    "    if not results:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    bt = pd.concat(results, ignore_index=True)\n",
    "\n",
    "    def _metrics(g: pd.DataFrame) -> pd.Series:\n",
    "        y = g[\"actual\"].values.astype(float)\n",
    "        yhat = g[\"forecast\"].values.astype(float)\n",
    "\n",
    "        mae = float(np.mean(np.abs(y - yhat))) if len(y) else np.nan\n",
    "        bias = float((yhat.sum() - y.sum()) / y.sum() * 100) if y.sum() > 0 else np.nan\n",
    "        wape = compute_wape(y, yhat) * 100 if y.sum() > 0 else np.nan\n",
    "        acc = max(0.0, 100.0 - wape) if np.isfinite(wape) else np.nan\n",
    "\n",
    "        return pd.Series(\n",
    "            {\"MAE\": mae, \"Bias_%\": bias, \"WAPE_%\": wape, \"Accuracy_staffing_%\": acc}\n",
    "        )\n",
    "\n",
    "    out = bt.groupby([\"vertical\", \"department_id\"]).apply(_metrics).reset_index()\n",
    "    out[\"Eval_Months\"] = bt.groupby([\"vertical\", \"department_id\"])[\"month\"].nunique().values\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44a68e7",
   "metadata": {},
   "source": [
    "## 8) Run end-to-end pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966ada6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "incoming = load_incoming(INCOMING_SOURCE_PATH, INCOMING_SHEET)\n",
    "mapping = load_dept_map(DEPT_MAP_PATH, DEPT_MAP_SHEET)\n",
    "\n",
    "incoming = incoming.merge(mapping, on=\"department_id\", how=\"left\", suffixes=(\"\",\"_map\"))\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# SAFE COALESCE: vertical + department_name (v15.1)\n",
    "# Guarantees columns exist and prevents KeyError: 'vertical'\n",
    "# -----------------------------\n",
    "if \"vertical\" not in incoming.columns:\n",
    "    incoming[\"vertical\"] = pd.NA\n",
    "if \"department_name\" not in incoming.columns:\n",
    "    incoming[\"department_name\"] = pd.NA\n",
    "\n",
    "if \"vertical_map\" in incoming.columns:\n",
    "    incoming[\"vertical\"] = incoming[\"vertical_map\"].combine_first(incoming[\"vertical\"])\n",
    "if \"department_name_map\" in incoming.columns:\n",
    "    incoming[\"department_name\"] = incoming[\"department_name_map\"].combine_first(incoming[\"department_name\"])\n",
    "\n",
    "incoming[\"vertical\"] = incoming[\"vertical\"].fillna(\"Unknown\").astype(str)\n",
    "incoming[\"department_name\"] = incoming[\"department_name\"].fillna(\"Unknown\").astype(str)\n",
    "\n",
    "if \"vertical_map\" in incoming.columns:\n",
    "    incoming[\"vertical\"] = incoming[\"vertical_map\"].combine_first(incoming[\"vertical\"])\n",
    "if \"department_name_map\" in incoming.columns:\n",
    "    incoming[\"department_name\"] = incoming[\"department_name_map\"].combine_first(incoming[\"department_name\"])\n",
    "\n",
    "incoming[\"vertical\"] = incoming[\"vertical\"].fillna(\"Unknown\").astype(str)\n",
    "incoming[\"department_name\"] = incoming[\"department_name\"].fillna(\"Unknown\").astype(str)\n",
    "\n",
    "print(\"Incoming rows:\", len(incoming))\n",
    "print(\"Verticals:\", sorted(incoming[\"vertical\"].unique().tolist()))\n",
    "print(\"Departments:\", incoming[\"department_id\"].nunique())\n",
    "\n",
    "# Shares\n",
    "shares_vd = dept_share_ewma_within_vertical(incoming, alpha=DEPT_SHARE_EWMA_ALPHA)\n",
    "san = shares_vd.groupby([\"vertical\",\"month\"])[\"share_final\"].sum().reset_index(name=\"share_sum\")\n",
    "print(\"Share sum (min/max):\", float(san[\"share_sum\"].min()), float(san[\"share_sum\"].max()))\n",
    "\n",
    "# Forecasts\n",
    "fc_vertical = forecast_vertical_final(incoming, periods=H_MONTHS, alpha=PI_ALPHA)\n",
    "fc_dept = allocate_vertical_to_dept(fc_vertical, shares_vd).merge(mapping, on=\"department_id\", how=\"left\")\n",
    "\n",
    "# -----------------------------\n",
    "# v15.1 GUARDAESPALDAS: fc_dept SIEMPRE con 'vertical'\n",
    "# -----------------------------\n",
    "if \"vertical\" not in fc_dept.columns:\n",
    "    # 1) Preferencia: traértelo desde shares_vd (vertical-dept real del share)\n",
    "    if \"vertical\" in shares_vd.columns:\n",
    "        vd = shares_vd[[\"department_id\", \"vertical\"]].drop_duplicates(\"department_id\")\n",
    "        fc_dept = fc_dept.merge(vd, on=\"department_id\", how=\"left\")\n",
    "\n",
    "# 2) Si aún no está, fallback a mapping\n",
    "if \"vertical\" not in fc_dept.columns:\n",
    "    if \"vertical\" in mapping.columns:\n",
    "        fc_dept = fc_dept.merge(\n",
    "            mapping[[\"department_id\", \"vertical\"]],\n",
    "            on=\"department_id\",\n",
    "            how=\"left\",\n",
    "            suffixes=(\"\", \"_map\")\n",
    "        )\n",
    "\n",
    "# 3) Normaliza nombre si quedó como vertical_map\n",
    "if \"vertical\" not in fc_dept.columns and \"vertical_map\" in fc_dept.columns:\n",
    "    fc_dept[\"vertical\"] = fc_dept[\"vertical_map\"]\n",
    "\n",
    "# 4) Último recurso\n",
    "if \"vertical\" not in fc_dept.columns:\n",
    "    fc_dept[\"vertical\"] = \"Unknown\"\n",
    "\n",
    "fc_dept[\"vertical\"] = fc_dept[\"vertical\"].fillna(\"Unknown\").astype(str)\n",
    "\n",
    "# Daily plan\n",
    "dow_profile = compute_dept_dow_profile(incoming, lookback_days=DOW_LOOKBACK_DAYS, min_obs=DOW_MIN_OBS)\n",
    "needed = [\"vertical\",\"department_id\",\"month\",\"forecast_monthly_dept\",\"forecast_p05_dept\",\"forecast_p95_dept\"]\n",
    "missing = [c for c in needed if c not in fc_dept.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"fc_dept missing columns for daily plan: {missing}. Columns={list(fc_dept.columns)}\")\n",
    "\n",
    "daily_plan = daily_plan_from_monthly(\n",
    "    fc_dept_monthly=fc_dept[needed],\n",
    "    dow_profile=dow_profile,\n",
    "    start_date=pd.Timestamp.today().normalize(),\n",
    "    horizon_days=DAILY_HORIZON_DAYS\n",
    ").merge(mapping, on=\"department_id\", how=\"left\")\n",
    "\n",
    "# Backtest\n",
    "acc_dept = backtest_dept_accuracy(\n",
    "    incoming,\n",
    "    min_train_months=BT_MIN_TRAIN_MONTHS,\n",
    "    eval_months=BT_EVAL_MONTHS,\n",
    "    horizon_months=BT_HORIZON_MONTHS,\n",
    "    max_splits=BT_MAX_SPLITS\n",
    ").merge(mapping, on=\"department_id\", how=\"left\")\n",
    "\n",
    "acc_dept = acc_dept.sort_values([\"vertical\",\"department_id\"]) if not acc_dept.empty else acc_dept\n",
    "\n",
    "print(\"\\nVertical factors (sanity):\")\n",
    "display(fc_vertical.groupby(\"vertical\", as_index=False)[\"vertical_level_factor\"].mean().sort_values(\"vertical_level_factor\"))\n",
    "\n",
    "print(\"\\nAccuracy table (top 15 rows):\")\n",
    "display(acc_dept.head(15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34b0939",
   "metadata": {},
   "source": [
    "## 9) Export to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20a56bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(OUTPUT_XLSX, engine=\"openpyxl\") as w:\n",
    "    fc_vertical.sort_values([\"vertical\",\"month\"]).to_excel(w, \"forecast_vertical_monthly\", index=False)\n",
    "    fc_dept.sort_values([\"vertical\",\"department_id\",\"month\"]).to_excel(w, \"forecast_dept_monthly\", index=False)\n",
    "    daily_plan.sort_values([\"vertical\",\"department_id\",\"Date\"]).to_excel(w, \"daily_plan_90d\", index=False)\n",
    "    shares_vd.sort_values([\"vertical\",\"department_id\",\"month\"]).to_excel(w, \"dept_share_ewma\", index=False)\n",
    "    dow_profile.sort_values([\"department_id\",\"dow\"]).to_excel(w, \"dept_dow_profile\", index=False)\n",
    "    acc_dept.sort_values([\"vertical\",\"department_id\"]).to_excel(w, \"accuracy_dept_monthly\", index=False)\n",
    "\n",
    "print(\"Saved:\", OUTPUT_XLSX)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
